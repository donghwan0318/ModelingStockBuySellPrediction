{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Yh0Pr4VUmZ93"
      ],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xWLxSEdmtwr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4h8hNxR-4YPR",
        "outputId": "1a19c368-8db2-4b36-be06-c6e0e4c55088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sk=pd.read_csv('/content/drive/MyDrive/2023-1학기-시계열팀주제분석/PreprocessedData/labeled/SK_label.csv')"
      ],
      "metadata": {
        "id": "Boihe_Kh4Wia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sk.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsG6akJpO_Wo",
        "outputId": "f3bc952b-9357-4c98-9ea7-e2b36d974968"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['날짜', 'year', 'month', 'day', '종가', '대비', '등락률', '거래량', '거래대금', '시가총액',\n",
              "       '외국인 보유수량', '외국인 지분율', '토론방', '순매수_기관', '순매수_기타법인', '순매수_개인', '순매수_외국인',\n",
              "       '검색어', '보도량', '기사감성점수', '뉴스심리지수', '비트코인종가', '비트코인거래량', '비트코인변동',\n",
              "       '코스피종가', '코스피대비', '코스피등락률', '코스피거래량', '코스피거래대금', '코스피시가총액', '한은금리',\n",
              "       '원/미국달러', '원/위안', '원/일본엔(100엔)', '원/유로', '경제심리지수(원계열)', '경제심리지수(순환변동치)',\n",
              "       '산업생산지수', '물가상승률', '소비자신뢰지수', '소비자심리지수', '경제활동참가율(%)', '실업률(%)',\n",
              "       '고용률(%)', 'day1_label', '3일 등락률', 'day3_label'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Full model\n",
        "SK_df = sk.drop(['날짜', 'year', 'month', 'day', '대비', '코스피대비', 'day1_label'], axis=1)"
      ],
      "metadata": {
        "id": "k5LVkT7Ga3M1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#KStest\n",
        "SK_df=sk[['종가', '등락률', '거래량', '거래대금', '시가총액', '토론방', '순매수_기관', '순매수_개인',\n",
        "          '순매수_외국인', '순매수_기타법인', '검색어', '보도량', '기사감성점수', '코스피대비',\n",
        "          '코스피거래대금', 'day3_label']]"
      ],
      "metadata": {
        "id": "ZVoOoYtO4-IH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#VIF 3일 등락률\n",
        "SK_df = sk[['경제심리지수(순환변동치)','시가총액','비트코인종가','원/미국달러','소비자심리지수','코스피거래대금','순매수_개인',\n",
        "         '순매수_외국인','산업생산지수','코스피거래량','뉴스심리지수','원/유로','토론방','실업률(%)','원/일본엔(100엔)',\n",
        "    '거래량','기사감성점수','외국인 보유수량','코스피등락률','경제활동참가율(%)','검색어','보도량','순매수_기관',\n",
        "    '비트코인거래량','비트코인변동', 'day3_label']]"
      ],
      "metadata": {
        "id": "ZPczzQnPVvjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SK_df.index = sk['날짜']"
      ],
      "metadata": {
        "id": "Le1934dP4V2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SK_df=SK_df.dropna(axis=0)"
      ],
      "metadata": {
        "id": "w03M2oh1-vEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SK_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 866
        },
        "id": "y0R3yw8MPWJx",
        "outputId": "883bb181-dab1-429c-bf8e-2320aa40a9ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            경제심리지수(순환변동치)          시가총액   비트코인종가  원/미국달러  소비자심리지수     코스피거래대금  \\\n",
              "날짜                                                                              \n",
              "2017-06-13           99.6  4.280654e+13   2713.0  1126.0    110.8   5665179.0   \n",
              "2017-06-14           99.6  4.317054e+13   2467.3  1128.9    110.8   6795165.0   \n",
              "2017-06-15           99.6  4.411694e+13   2442.5  1125.6    110.8   5882836.0   \n",
              "2017-06-16           99.6  4.404414e+13   2508.6  1122.8    110.8   5965450.0   \n",
              "2017-06-19           99.6  4.571855e+13   2616.8  1133.0    110.8   5421942.0   \n",
              "...                   ...           ...      ...     ...      ...         ...   \n",
              "2023-03-27           89.8  6.224420e+13  27129.8  1289.7     92.0   8534650.0   \n",
              "2023-03-28           89.8  6.435541e+13  27262.2  1300.3     92.0   8812618.0   \n",
              "2023-03-29           89.8  6.326341e+13  28350.4  1297.4     92.0   8829889.0   \n",
              "2023-03-30           89.8  6.464661e+13  28029.5  1299.3     92.0  10846640.0   \n",
              "2023-03-31           89.8  6.450101e+13  28473.7  1303.8     92.0  12903370.0   \n",
              "\n",
              "                  순매수_개인       순매수_외국인  산업생산지수    코스피거래량  ...    기사감성점수  \\\n",
              "날짜                                                        ...             \n",
              "2017-06-13 -1.325185e+10  8.168180e+09   103.9  322082.0  ...  0.590909   \n",
              "2017-06-14 -1.694931e+10 -1.667742e+10   103.9  392998.0  ...  0.352941   \n",
              "2017-06-15 -5.105465e+10  8.727358e+09   103.9  334886.0  ...  0.227273   \n",
              "2017-06-16  2.260853e+10 -5.685305e+10   103.9  329179.0  ...  0.588235   \n",
              "2017-06-19 -2.336965e+10 -2.064758e+10   103.9  290945.0  ...  0.461538   \n",
              "...                  ...           ...     ...       ...  ...       ...   \n",
              "2023-03-27  3.645140e+10 -3.181490e+10   112.5  472847.0  ...  0.145455   \n",
              "2023-03-28 -5.598959e+10 -8.886702e+08   112.5  524016.0  ...  0.290909   \n",
              "2023-03-29  3.466739e+10 -5.950933e+10   112.5  497218.0  ...  0.022989   \n",
              "2023-03-30 -3.928891e+10 -7.795352e+09   112.5  629052.0  ...  0.269231   \n",
              "2023-03-31  3.029416e+10  4.312767e+09   112.5  722612.0  ...  0.200000   \n",
              "\n",
              "               외국인 보유수량  코스피등락률  경제활동참가율(%)      검색어    보도량        순매수_기관  \\\n",
              "날짜                                                                          \n",
              "2017-06-13  373079472.0    0.71        63.9  5.65469   44.0  3.938152e+09   \n",
              "2017-06-14  373205666.0   -0.09        63.9  6.55951   51.0  3.378250e+10   \n",
              "2017-06-15  372839313.0   -0.46        63.9  7.20316   44.0  4.528110e+10   \n",
              "2017-06-16  372949640.0    0.01        63.9  6.22995   34.0  3.251843e+10   \n",
              "2017-06-19  372011865.0    0.38        63.9  8.09842   65.0  4.404935e+10   \n",
              "...                 ...     ...         ...      ...    ...           ...   \n",
              "2023-03-27  365466692.0   -0.24        64.1  4.70174   55.0 -2.786144e+09   \n",
              "2023-03-28  365123199.0    1.07        64.1  5.25521   55.0  5.638837e+10   \n",
              "2023-03-29  365150714.0    0.37        64.1  5.65259   87.0  2.506547e+10   \n",
              "2023-03-30  364441191.0    0.38        64.1  7.09025  104.0  4.623535e+10   \n",
              "2023-03-31  364366068.0    0.97        64.1  5.14803   45.0 -3.439400e+10   \n",
              "\n",
              "             비트코인거래량  비트코인변동  day3_label  \n",
              "날짜                                        \n",
              "2017-06-13  116740.0    2.12    maintain  \n",
              "2017-06-14  174190.0   -9.06    maintain  \n",
              "2017-06-15  262790.0   -1.01         buy  \n",
              "2017-06-16  117210.0    2.71    maintain  \n",
              "2017-06-19   81830.0    3.04         buy  \n",
              "...              ...     ...         ...  \n",
              "2023-03-27  107240.0   -3.02    maintain  \n",
              "2023-03-28   94160.0    0.49    maintain  \n",
              "2023-03-29  109320.0    3.99    maintain  \n",
              "2023-03-30  122510.0   -1.13    maintain  \n",
              "2023-03-31   98440.0    1.58    maintain  \n",
              "\n",
              "[1429 rows x 26 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-615e70a4-c868-4fee-be2c-79742f6e9ca1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>경제심리지수(순환변동치)</th>\n",
              "      <th>시가총액</th>\n",
              "      <th>비트코인종가</th>\n",
              "      <th>원/미국달러</th>\n",
              "      <th>소비자심리지수</th>\n",
              "      <th>코스피거래대금</th>\n",
              "      <th>순매수_개인</th>\n",
              "      <th>순매수_외국인</th>\n",
              "      <th>산업생산지수</th>\n",
              "      <th>코스피거래량</th>\n",
              "      <th>...</th>\n",
              "      <th>기사감성점수</th>\n",
              "      <th>외국인 보유수량</th>\n",
              "      <th>코스피등락률</th>\n",
              "      <th>경제활동참가율(%)</th>\n",
              "      <th>검색어</th>\n",
              "      <th>보도량</th>\n",
              "      <th>순매수_기관</th>\n",
              "      <th>비트코인거래량</th>\n",
              "      <th>비트코인변동</th>\n",
              "      <th>day3_label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>날짜</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2017-06-13</th>\n",
              "      <td>99.6</td>\n",
              "      <td>4.280654e+13</td>\n",
              "      <td>2713.0</td>\n",
              "      <td>1126.0</td>\n",
              "      <td>110.8</td>\n",
              "      <td>5665179.0</td>\n",
              "      <td>-1.325185e+10</td>\n",
              "      <td>8.168180e+09</td>\n",
              "      <td>103.9</td>\n",
              "      <td>322082.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.590909</td>\n",
              "      <td>373079472.0</td>\n",
              "      <td>0.71</td>\n",
              "      <td>63.9</td>\n",
              "      <td>5.65469</td>\n",
              "      <td>44.0</td>\n",
              "      <td>3.938152e+09</td>\n",
              "      <td>116740.0</td>\n",
              "      <td>2.12</td>\n",
              "      <td>maintain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-06-14</th>\n",
              "      <td>99.6</td>\n",
              "      <td>4.317054e+13</td>\n",
              "      <td>2467.3</td>\n",
              "      <td>1128.9</td>\n",
              "      <td>110.8</td>\n",
              "      <td>6795165.0</td>\n",
              "      <td>-1.694931e+10</td>\n",
              "      <td>-1.667742e+10</td>\n",
              "      <td>103.9</td>\n",
              "      <td>392998.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>373205666.0</td>\n",
              "      <td>-0.09</td>\n",
              "      <td>63.9</td>\n",
              "      <td>6.55951</td>\n",
              "      <td>51.0</td>\n",
              "      <td>3.378250e+10</td>\n",
              "      <td>174190.0</td>\n",
              "      <td>-9.06</td>\n",
              "      <td>maintain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-06-15</th>\n",
              "      <td>99.6</td>\n",
              "      <td>4.411694e+13</td>\n",
              "      <td>2442.5</td>\n",
              "      <td>1125.6</td>\n",
              "      <td>110.8</td>\n",
              "      <td>5882836.0</td>\n",
              "      <td>-5.105465e+10</td>\n",
              "      <td>8.727358e+09</td>\n",
              "      <td>103.9</td>\n",
              "      <td>334886.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.227273</td>\n",
              "      <td>372839313.0</td>\n",
              "      <td>-0.46</td>\n",
              "      <td>63.9</td>\n",
              "      <td>7.20316</td>\n",
              "      <td>44.0</td>\n",
              "      <td>4.528110e+10</td>\n",
              "      <td>262790.0</td>\n",
              "      <td>-1.01</td>\n",
              "      <td>buy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-06-16</th>\n",
              "      <td>99.6</td>\n",
              "      <td>4.404414e+13</td>\n",
              "      <td>2508.6</td>\n",
              "      <td>1122.8</td>\n",
              "      <td>110.8</td>\n",
              "      <td>5965450.0</td>\n",
              "      <td>2.260853e+10</td>\n",
              "      <td>-5.685305e+10</td>\n",
              "      <td>103.9</td>\n",
              "      <td>329179.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.588235</td>\n",
              "      <td>372949640.0</td>\n",
              "      <td>0.01</td>\n",
              "      <td>63.9</td>\n",
              "      <td>6.22995</td>\n",
              "      <td>34.0</td>\n",
              "      <td>3.251843e+10</td>\n",
              "      <td>117210.0</td>\n",
              "      <td>2.71</td>\n",
              "      <td>maintain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-06-19</th>\n",
              "      <td>99.6</td>\n",
              "      <td>4.571855e+13</td>\n",
              "      <td>2616.8</td>\n",
              "      <td>1133.0</td>\n",
              "      <td>110.8</td>\n",
              "      <td>5421942.0</td>\n",
              "      <td>-2.336965e+10</td>\n",
              "      <td>-2.064758e+10</td>\n",
              "      <td>103.9</td>\n",
              "      <td>290945.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>372011865.0</td>\n",
              "      <td>0.38</td>\n",
              "      <td>63.9</td>\n",
              "      <td>8.09842</td>\n",
              "      <td>65.0</td>\n",
              "      <td>4.404935e+10</td>\n",
              "      <td>81830.0</td>\n",
              "      <td>3.04</td>\n",
              "      <td>buy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-27</th>\n",
              "      <td>89.8</td>\n",
              "      <td>6.224420e+13</td>\n",
              "      <td>27129.8</td>\n",
              "      <td>1289.7</td>\n",
              "      <td>92.0</td>\n",
              "      <td>8534650.0</td>\n",
              "      <td>3.645140e+10</td>\n",
              "      <td>-3.181490e+10</td>\n",
              "      <td>112.5</td>\n",
              "      <td>472847.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.145455</td>\n",
              "      <td>365466692.0</td>\n",
              "      <td>-0.24</td>\n",
              "      <td>64.1</td>\n",
              "      <td>4.70174</td>\n",
              "      <td>55.0</td>\n",
              "      <td>-2.786144e+09</td>\n",
              "      <td>107240.0</td>\n",
              "      <td>-3.02</td>\n",
              "      <td>maintain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-28</th>\n",
              "      <td>89.8</td>\n",
              "      <td>6.435541e+13</td>\n",
              "      <td>27262.2</td>\n",
              "      <td>1300.3</td>\n",
              "      <td>92.0</td>\n",
              "      <td>8812618.0</td>\n",
              "      <td>-5.598959e+10</td>\n",
              "      <td>-8.886702e+08</td>\n",
              "      <td>112.5</td>\n",
              "      <td>524016.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.290909</td>\n",
              "      <td>365123199.0</td>\n",
              "      <td>1.07</td>\n",
              "      <td>64.1</td>\n",
              "      <td>5.25521</td>\n",
              "      <td>55.0</td>\n",
              "      <td>5.638837e+10</td>\n",
              "      <td>94160.0</td>\n",
              "      <td>0.49</td>\n",
              "      <td>maintain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-29</th>\n",
              "      <td>89.8</td>\n",
              "      <td>6.326341e+13</td>\n",
              "      <td>28350.4</td>\n",
              "      <td>1297.4</td>\n",
              "      <td>92.0</td>\n",
              "      <td>8829889.0</td>\n",
              "      <td>3.466739e+10</td>\n",
              "      <td>-5.950933e+10</td>\n",
              "      <td>112.5</td>\n",
              "      <td>497218.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.022989</td>\n",
              "      <td>365150714.0</td>\n",
              "      <td>0.37</td>\n",
              "      <td>64.1</td>\n",
              "      <td>5.65259</td>\n",
              "      <td>87.0</td>\n",
              "      <td>2.506547e+10</td>\n",
              "      <td>109320.0</td>\n",
              "      <td>3.99</td>\n",
              "      <td>maintain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-30</th>\n",
              "      <td>89.8</td>\n",
              "      <td>6.464661e+13</td>\n",
              "      <td>28029.5</td>\n",
              "      <td>1299.3</td>\n",
              "      <td>92.0</td>\n",
              "      <td>10846640.0</td>\n",
              "      <td>-3.928891e+10</td>\n",
              "      <td>-7.795352e+09</td>\n",
              "      <td>112.5</td>\n",
              "      <td>629052.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.269231</td>\n",
              "      <td>364441191.0</td>\n",
              "      <td>0.38</td>\n",
              "      <td>64.1</td>\n",
              "      <td>7.09025</td>\n",
              "      <td>104.0</td>\n",
              "      <td>4.623535e+10</td>\n",
              "      <td>122510.0</td>\n",
              "      <td>-1.13</td>\n",
              "      <td>maintain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-31</th>\n",
              "      <td>89.8</td>\n",
              "      <td>6.450101e+13</td>\n",
              "      <td>28473.7</td>\n",
              "      <td>1303.8</td>\n",
              "      <td>92.0</td>\n",
              "      <td>12903370.0</td>\n",
              "      <td>3.029416e+10</td>\n",
              "      <td>4.312767e+09</td>\n",
              "      <td>112.5</td>\n",
              "      <td>722612.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>364366068.0</td>\n",
              "      <td>0.97</td>\n",
              "      <td>64.1</td>\n",
              "      <td>5.14803</td>\n",
              "      <td>45.0</td>\n",
              "      <td>-3.439400e+10</td>\n",
              "      <td>98440.0</td>\n",
              "      <td>1.58</td>\n",
              "      <td>maintain</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1429 rows × 26 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-615e70a4-c868-4fee-be2c-79742f6e9ca1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-615e70a4-c868-4fee-be2c-79742f6e9ca1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-615e70a4-c868-4fee-be2c-79742f6e9ca1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#라벨인코딩"
      ],
      "metadata": {
        "id": "GuRTLfvsmWDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(SK_df['day3_label'].values)\n",
        "SK_df['day3_label'] = le.transform(SK_df['day3_label'].values)\n",
        "\n",
        "SK_df\n",
        "# buy = 0 (매수)\n",
        "# maintain = 1 (유지)\n",
        "# sell = 2 (매도)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 866
        },
        "id": "U_v13ReCPbqo",
        "outputId": "b527e89e-b982-4053-aada-7822151420a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            경제심리지수(순환변동치)          시가총액   비트코인종가  원/미국달러  소비자심리지수     코스피거래대금  \\\n",
              "날짜                                                                              \n",
              "2017-06-13           99.6  4.280654e+13   2713.0  1126.0    110.8   5665179.0   \n",
              "2017-06-14           99.6  4.317054e+13   2467.3  1128.9    110.8   6795165.0   \n",
              "2017-06-15           99.6  4.411694e+13   2442.5  1125.6    110.8   5882836.0   \n",
              "2017-06-16           99.6  4.404414e+13   2508.6  1122.8    110.8   5965450.0   \n",
              "2017-06-19           99.6  4.571855e+13   2616.8  1133.0    110.8   5421942.0   \n",
              "...                   ...           ...      ...     ...      ...         ...   \n",
              "2023-03-27           89.8  6.224420e+13  27129.8  1289.7     92.0   8534650.0   \n",
              "2023-03-28           89.8  6.435541e+13  27262.2  1300.3     92.0   8812618.0   \n",
              "2023-03-29           89.8  6.326341e+13  28350.4  1297.4     92.0   8829889.0   \n",
              "2023-03-30           89.8  6.464661e+13  28029.5  1299.3     92.0  10846640.0   \n",
              "2023-03-31           89.8  6.450101e+13  28473.7  1303.8     92.0  12903370.0   \n",
              "\n",
              "                  순매수_개인       순매수_외국인  산업생산지수    코스피거래량  ...    기사감성점수  \\\n",
              "날짜                                                        ...             \n",
              "2017-06-13 -1.325185e+10  8.168180e+09   103.9  322082.0  ...  0.590909   \n",
              "2017-06-14 -1.694931e+10 -1.667742e+10   103.9  392998.0  ...  0.352941   \n",
              "2017-06-15 -5.105465e+10  8.727358e+09   103.9  334886.0  ...  0.227273   \n",
              "2017-06-16  2.260853e+10 -5.685305e+10   103.9  329179.0  ...  0.588235   \n",
              "2017-06-19 -2.336965e+10 -2.064758e+10   103.9  290945.0  ...  0.461538   \n",
              "...                  ...           ...     ...       ...  ...       ...   \n",
              "2023-03-27  3.645140e+10 -3.181490e+10   112.5  472847.0  ...  0.145455   \n",
              "2023-03-28 -5.598959e+10 -8.886702e+08   112.5  524016.0  ...  0.290909   \n",
              "2023-03-29  3.466739e+10 -5.950933e+10   112.5  497218.0  ...  0.022989   \n",
              "2023-03-30 -3.928891e+10 -7.795352e+09   112.5  629052.0  ...  0.269231   \n",
              "2023-03-31  3.029416e+10  4.312767e+09   112.5  722612.0  ...  0.200000   \n",
              "\n",
              "               외국인 보유수량  코스피등락률  경제활동참가율(%)      검색어    보도량        순매수_기관  \\\n",
              "날짜                                                                          \n",
              "2017-06-13  373079472.0    0.71        63.9  5.65469   44.0  3.938152e+09   \n",
              "2017-06-14  373205666.0   -0.09        63.9  6.55951   51.0  3.378250e+10   \n",
              "2017-06-15  372839313.0   -0.46        63.9  7.20316   44.0  4.528110e+10   \n",
              "2017-06-16  372949640.0    0.01        63.9  6.22995   34.0  3.251843e+10   \n",
              "2017-06-19  372011865.0    0.38        63.9  8.09842   65.0  4.404935e+10   \n",
              "...                 ...     ...         ...      ...    ...           ...   \n",
              "2023-03-27  365466692.0   -0.24        64.1  4.70174   55.0 -2.786144e+09   \n",
              "2023-03-28  365123199.0    1.07        64.1  5.25521   55.0  5.638837e+10   \n",
              "2023-03-29  365150714.0    0.37        64.1  5.65259   87.0  2.506547e+10   \n",
              "2023-03-30  364441191.0    0.38        64.1  7.09025  104.0  4.623535e+10   \n",
              "2023-03-31  364366068.0    0.97        64.1  5.14803   45.0 -3.439400e+10   \n",
              "\n",
              "             비트코인거래량  비트코인변동  day3_label  \n",
              "날짜                                        \n",
              "2017-06-13  116740.0    2.12           1  \n",
              "2017-06-14  174190.0   -9.06           1  \n",
              "2017-06-15  262790.0   -1.01           0  \n",
              "2017-06-16  117210.0    2.71           1  \n",
              "2017-06-19   81830.0    3.04           0  \n",
              "...              ...     ...         ...  \n",
              "2023-03-27  107240.0   -3.02           1  \n",
              "2023-03-28   94160.0    0.49           1  \n",
              "2023-03-29  109320.0    3.99           1  \n",
              "2023-03-30  122510.0   -1.13           1  \n",
              "2023-03-31   98440.0    1.58           1  \n",
              "\n",
              "[1429 rows x 26 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-58e5ede3-58d0-417b-abda-1d57d1150602\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>경제심리지수(순환변동치)</th>\n",
              "      <th>시가총액</th>\n",
              "      <th>비트코인종가</th>\n",
              "      <th>원/미국달러</th>\n",
              "      <th>소비자심리지수</th>\n",
              "      <th>코스피거래대금</th>\n",
              "      <th>순매수_개인</th>\n",
              "      <th>순매수_외국인</th>\n",
              "      <th>산업생산지수</th>\n",
              "      <th>코스피거래량</th>\n",
              "      <th>...</th>\n",
              "      <th>기사감성점수</th>\n",
              "      <th>외국인 보유수량</th>\n",
              "      <th>코스피등락률</th>\n",
              "      <th>경제활동참가율(%)</th>\n",
              "      <th>검색어</th>\n",
              "      <th>보도량</th>\n",
              "      <th>순매수_기관</th>\n",
              "      <th>비트코인거래량</th>\n",
              "      <th>비트코인변동</th>\n",
              "      <th>day3_label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>날짜</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2017-06-13</th>\n",
              "      <td>99.6</td>\n",
              "      <td>4.280654e+13</td>\n",
              "      <td>2713.0</td>\n",
              "      <td>1126.0</td>\n",
              "      <td>110.8</td>\n",
              "      <td>5665179.0</td>\n",
              "      <td>-1.325185e+10</td>\n",
              "      <td>8.168180e+09</td>\n",
              "      <td>103.9</td>\n",
              "      <td>322082.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.590909</td>\n",
              "      <td>373079472.0</td>\n",
              "      <td>0.71</td>\n",
              "      <td>63.9</td>\n",
              "      <td>5.65469</td>\n",
              "      <td>44.0</td>\n",
              "      <td>3.938152e+09</td>\n",
              "      <td>116740.0</td>\n",
              "      <td>2.12</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-06-14</th>\n",
              "      <td>99.6</td>\n",
              "      <td>4.317054e+13</td>\n",
              "      <td>2467.3</td>\n",
              "      <td>1128.9</td>\n",
              "      <td>110.8</td>\n",
              "      <td>6795165.0</td>\n",
              "      <td>-1.694931e+10</td>\n",
              "      <td>-1.667742e+10</td>\n",
              "      <td>103.9</td>\n",
              "      <td>392998.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>373205666.0</td>\n",
              "      <td>-0.09</td>\n",
              "      <td>63.9</td>\n",
              "      <td>6.55951</td>\n",
              "      <td>51.0</td>\n",
              "      <td>3.378250e+10</td>\n",
              "      <td>174190.0</td>\n",
              "      <td>-9.06</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-06-15</th>\n",
              "      <td>99.6</td>\n",
              "      <td>4.411694e+13</td>\n",
              "      <td>2442.5</td>\n",
              "      <td>1125.6</td>\n",
              "      <td>110.8</td>\n",
              "      <td>5882836.0</td>\n",
              "      <td>-5.105465e+10</td>\n",
              "      <td>8.727358e+09</td>\n",
              "      <td>103.9</td>\n",
              "      <td>334886.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.227273</td>\n",
              "      <td>372839313.0</td>\n",
              "      <td>-0.46</td>\n",
              "      <td>63.9</td>\n",
              "      <td>7.20316</td>\n",
              "      <td>44.0</td>\n",
              "      <td>4.528110e+10</td>\n",
              "      <td>262790.0</td>\n",
              "      <td>-1.01</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-06-16</th>\n",
              "      <td>99.6</td>\n",
              "      <td>4.404414e+13</td>\n",
              "      <td>2508.6</td>\n",
              "      <td>1122.8</td>\n",
              "      <td>110.8</td>\n",
              "      <td>5965450.0</td>\n",
              "      <td>2.260853e+10</td>\n",
              "      <td>-5.685305e+10</td>\n",
              "      <td>103.9</td>\n",
              "      <td>329179.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.588235</td>\n",
              "      <td>372949640.0</td>\n",
              "      <td>0.01</td>\n",
              "      <td>63.9</td>\n",
              "      <td>6.22995</td>\n",
              "      <td>34.0</td>\n",
              "      <td>3.251843e+10</td>\n",
              "      <td>117210.0</td>\n",
              "      <td>2.71</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-06-19</th>\n",
              "      <td>99.6</td>\n",
              "      <td>4.571855e+13</td>\n",
              "      <td>2616.8</td>\n",
              "      <td>1133.0</td>\n",
              "      <td>110.8</td>\n",
              "      <td>5421942.0</td>\n",
              "      <td>-2.336965e+10</td>\n",
              "      <td>-2.064758e+10</td>\n",
              "      <td>103.9</td>\n",
              "      <td>290945.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>372011865.0</td>\n",
              "      <td>0.38</td>\n",
              "      <td>63.9</td>\n",
              "      <td>8.09842</td>\n",
              "      <td>65.0</td>\n",
              "      <td>4.404935e+10</td>\n",
              "      <td>81830.0</td>\n",
              "      <td>3.04</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-27</th>\n",
              "      <td>89.8</td>\n",
              "      <td>6.224420e+13</td>\n",
              "      <td>27129.8</td>\n",
              "      <td>1289.7</td>\n",
              "      <td>92.0</td>\n",
              "      <td>8534650.0</td>\n",
              "      <td>3.645140e+10</td>\n",
              "      <td>-3.181490e+10</td>\n",
              "      <td>112.5</td>\n",
              "      <td>472847.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.145455</td>\n",
              "      <td>365466692.0</td>\n",
              "      <td>-0.24</td>\n",
              "      <td>64.1</td>\n",
              "      <td>4.70174</td>\n",
              "      <td>55.0</td>\n",
              "      <td>-2.786144e+09</td>\n",
              "      <td>107240.0</td>\n",
              "      <td>-3.02</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-28</th>\n",
              "      <td>89.8</td>\n",
              "      <td>6.435541e+13</td>\n",
              "      <td>27262.2</td>\n",
              "      <td>1300.3</td>\n",
              "      <td>92.0</td>\n",
              "      <td>8812618.0</td>\n",
              "      <td>-5.598959e+10</td>\n",
              "      <td>-8.886702e+08</td>\n",
              "      <td>112.5</td>\n",
              "      <td>524016.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.290909</td>\n",
              "      <td>365123199.0</td>\n",
              "      <td>1.07</td>\n",
              "      <td>64.1</td>\n",
              "      <td>5.25521</td>\n",
              "      <td>55.0</td>\n",
              "      <td>5.638837e+10</td>\n",
              "      <td>94160.0</td>\n",
              "      <td>0.49</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-29</th>\n",
              "      <td>89.8</td>\n",
              "      <td>6.326341e+13</td>\n",
              "      <td>28350.4</td>\n",
              "      <td>1297.4</td>\n",
              "      <td>92.0</td>\n",
              "      <td>8829889.0</td>\n",
              "      <td>3.466739e+10</td>\n",
              "      <td>-5.950933e+10</td>\n",
              "      <td>112.5</td>\n",
              "      <td>497218.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.022989</td>\n",
              "      <td>365150714.0</td>\n",
              "      <td>0.37</td>\n",
              "      <td>64.1</td>\n",
              "      <td>5.65259</td>\n",
              "      <td>87.0</td>\n",
              "      <td>2.506547e+10</td>\n",
              "      <td>109320.0</td>\n",
              "      <td>3.99</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-30</th>\n",
              "      <td>89.8</td>\n",
              "      <td>6.464661e+13</td>\n",
              "      <td>28029.5</td>\n",
              "      <td>1299.3</td>\n",
              "      <td>92.0</td>\n",
              "      <td>10846640.0</td>\n",
              "      <td>-3.928891e+10</td>\n",
              "      <td>-7.795352e+09</td>\n",
              "      <td>112.5</td>\n",
              "      <td>629052.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.269231</td>\n",
              "      <td>364441191.0</td>\n",
              "      <td>0.38</td>\n",
              "      <td>64.1</td>\n",
              "      <td>7.09025</td>\n",
              "      <td>104.0</td>\n",
              "      <td>4.623535e+10</td>\n",
              "      <td>122510.0</td>\n",
              "      <td>-1.13</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-31</th>\n",
              "      <td>89.8</td>\n",
              "      <td>6.450101e+13</td>\n",
              "      <td>28473.7</td>\n",
              "      <td>1303.8</td>\n",
              "      <td>92.0</td>\n",
              "      <td>12903370.0</td>\n",
              "      <td>3.029416e+10</td>\n",
              "      <td>4.312767e+09</td>\n",
              "      <td>112.5</td>\n",
              "      <td>722612.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>364366068.0</td>\n",
              "      <td>0.97</td>\n",
              "      <td>64.1</td>\n",
              "      <td>5.14803</td>\n",
              "      <td>45.0</td>\n",
              "      <td>-3.439400e+10</td>\n",
              "      <td>98440.0</td>\n",
              "      <td>1.58</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1429 rows × 26 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-58e5ede3-58d0-417b-abda-1d57d1150602')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-58e5ede3-58d0-417b-abda-1d57d1150602 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-58e5ede3-58d0-417b-abda-1d57d1150602');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SK_df['day3_label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvtkuwnyWK8G",
        "outputId": "73d9dddf-0e25-41ef-a1f7-206e985b7d15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    1167\n",
              "0     155\n",
              "2     107\n",
              "Name: day3_label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_data(dataset): #X, y 모두 정규화\n",
        "  cols = dataset.columns.tolist()\n",
        "  col_name = [0]*len(cols)\n",
        "  for i in range(len(cols)):\n",
        "    col_name[i]=i\n",
        "  dataset.columns = col_name\n",
        "  dtypes = dataset.dtypes.tolist()\n",
        "  minmax = list()\n",
        "  for column in dataset:\n",
        "    dataset = dataset.astype({column: 'float32'})\n",
        "  for i in range(len(cols)):\n",
        "    col_values = dataset[col_name[i]]\n",
        "    value_min = min(col_values)\n",
        "    value_max = max(col_values)\n",
        "    minmax.append([value_min,value_max])\n",
        "  for column in dataset:\n",
        "    values = dataset[column].values\n",
        "    for i in range(len(values)):\n",
        "      values[i]=(values[i]-minmax[column][0])/(minmax[column][1]-minmax[column][0])\n",
        "    dataset[column] = values\n",
        "  dataset[column]=values\n",
        "  return dataset, minmax"
      ],
      "metadata": {
        "id": "oiIJca7XnMvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset, minmax = normalize_data(SK_df)\n",
        "\n",
        "#y label은 원래대로\n",
        "dataset.loc[dataset[25]==1, 25] = 2\n",
        "dataset.loc[dataset[25]==0.5, 25] = 1\n",
        "dataset = dataset.astype({25:'int'})\n",
        "\n",
        "values = dataset.values\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        },
        "id": "SJGeIs4BoDRA",
        "outputId": "42e80209-e488-4400-bb22-ca4b914976cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  0         1         2         3         4         5   \\\n",
              "날짜                                                                       \n",
              "2017-06-13  0.787234  0.012115  0.007345  0.180475  0.968992  0.066032   \n",
              "2017-06-14  0.787234  0.017621  0.003582  0.188127  0.968992  0.093254   \n",
              "2017-06-15  0.787234  0.031938  0.003202  0.179420  0.968992  0.071275   \n",
              "2017-06-16  0.787234  0.030837  0.004215  0.172032  0.968992  0.073266   \n",
              "2017-06-19  0.787234  0.056167  0.005872  0.198945  0.968992  0.060172   \n",
              "...              ...       ...       ...       ...       ...       ...   \n",
              "2023-03-27  0.489362  0.306167  0.381294  0.612401  0.483204  0.135160   \n",
              "2023-03-28  0.489362  0.338106  0.383322  0.640370  0.483204  0.141856   \n",
              "2023-03-29  0.489362  0.321586  0.399988  0.632718  0.483204  0.142272   \n",
              "2023-03-30  0.489362  0.342511  0.395073  0.637731  0.483204  0.190858   \n",
              "2023-03-31  0.489362  0.340308  0.401876  0.649604  0.483204  0.240406   \n",
              "\n",
              "                  6         7         8         9   ...        16        17  \\\n",
              "날짜                                                  ...                       \n",
              "2017-06-13  0.348929  0.716928  0.272031  0.042152  ...  0.808744  0.869827   \n",
              "2017-06-14  0.345932  0.695973  0.272031  0.063830  ...  0.656892  0.872557   \n",
              "2017-06-15  0.318290  0.717399  0.272031  0.046066  ...  0.576700  0.864631   \n",
              "2017-06-16  0.377993  0.662088  0.272031  0.044321  ...  0.807038  0.867018   \n",
              "2017-06-19  0.340728  0.692624  0.272031  0.032633  ...  0.726190  0.846727   \n",
              "...              ...       ...       ...       ...  ...       ...       ...   \n",
              "2023-03-27  0.389213  0.683206  0.601532  0.088239  ...  0.524489  0.705111   \n",
              "2023-03-28  0.314290  0.709289  0.601532  0.103880  ...  0.617307  0.697679   \n",
              "2023-03-29  0.387767  0.659848  0.601532  0.095688  ...  0.446341  0.698275   \n",
              "2023-03-30  0.327826  0.703464  0.601532  0.135988  ...  0.603474  0.682922   \n",
              "2023-03-31  0.384222  0.713676  0.601532  0.164589  ...  0.559296  0.681297   \n",
              "\n",
              "                  18        19        20        21        22        23  \\\n",
              "날짜                                                                       \n",
              "2017-06-13  0.535609  0.750000  0.034670  0.179775  0.545081  0.000026   \n",
              "2017-06-14  0.488523  0.750000  0.043928  0.219101  0.608513  0.000039   \n",
              "2017-06-15  0.466745  0.750000  0.050514  0.179775  0.632953  0.000059   \n",
              "2017-06-16  0.494408  0.750000  0.040556  0.123596  0.605827  0.000026   \n",
              "2017-06-19  0.516186  0.750000  0.059674  0.297753  0.630335  0.000018   \n",
              "...              ...       ...       ...       ...       ...       ...   \n",
              "2023-03-27  0.479694  0.799999  0.024920  0.241573  0.530788  0.000024   \n",
              "2023-03-28  0.556798  0.799999  0.030583  0.241573  0.656561  0.000021   \n",
              "2023-03-29  0.515597  0.799999  0.034649  0.421348  0.589986  0.000024   \n",
              "2023-03-30  0.516186  0.799999  0.049359  0.516854  0.634981  0.000027   \n",
              "2023-03-31  0.550912  0.799999  0.029486  0.185393  0.463607  0.000022   \n",
              "\n",
              "                  24  25  \n",
              "날짜                        \n",
              "2017-06-13  0.637936   1  \n",
              "2017-06-14  0.465246   1  \n",
              "2017-06-15  0.589589   0  \n",
              "2017-06-16  0.647050   1  \n",
              "2017-06-19  0.652147   0  \n",
              "...              ...  ..  \n",
              "2023-03-27  0.558542   1  \n",
              "2023-03-28  0.612759   1  \n",
              "2023-03-29  0.666821   1  \n",
              "2023-03-30  0.587736   1  \n",
              "2023-03-31  0.629595   1  \n",
              "\n",
              "[1429 rows x 26 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b650a1ac-3e22-4cd8-b25e-5dc18477f666\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>날짜</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2017-06-13</th>\n",
              "      <td>0.787234</td>\n",
              "      <td>0.012115</td>\n",
              "      <td>0.007345</td>\n",
              "      <td>0.180475</td>\n",
              "      <td>0.968992</td>\n",
              "      <td>0.066032</td>\n",
              "      <td>0.348929</td>\n",
              "      <td>0.716928</td>\n",
              "      <td>0.272031</td>\n",
              "      <td>0.042152</td>\n",
              "      <td>...</td>\n",
              "      <td>0.808744</td>\n",
              "      <td>0.869827</td>\n",
              "      <td>0.535609</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.034670</td>\n",
              "      <td>0.179775</td>\n",
              "      <td>0.545081</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>0.637936</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-06-14</th>\n",
              "      <td>0.787234</td>\n",
              "      <td>0.017621</td>\n",
              "      <td>0.003582</td>\n",
              "      <td>0.188127</td>\n",
              "      <td>0.968992</td>\n",
              "      <td>0.093254</td>\n",
              "      <td>0.345932</td>\n",
              "      <td>0.695973</td>\n",
              "      <td>0.272031</td>\n",
              "      <td>0.063830</td>\n",
              "      <td>...</td>\n",
              "      <td>0.656892</td>\n",
              "      <td>0.872557</td>\n",
              "      <td>0.488523</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.043928</td>\n",
              "      <td>0.219101</td>\n",
              "      <td>0.608513</td>\n",
              "      <td>0.000039</td>\n",
              "      <td>0.465246</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-06-15</th>\n",
              "      <td>0.787234</td>\n",
              "      <td>0.031938</td>\n",
              "      <td>0.003202</td>\n",
              "      <td>0.179420</td>\n",
              "      <td>0.968992</td>\n",
              "      <td>0.071275</td>\n",
              "      <td>0.318290</td>\n",
              "      <td>0.717399</td>\n",
              "      <td>0.272031</td>\n",
              "      <td>0.046066</td>\n",
              "      <td>...</td>\n",
              "      <td>0.576700</td>\n",
              "      <td>0.864631</td>\n",
              "      <td>0.466745</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.050514</td>\n",
              "      <td>0.179775</td>\n",
              "      <td>0.632953</td>\n",
              "      <td>0.000059</td>\n",
              "      <td>0.589589</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-06-16</th>\n",
              "      <td>0.787234</td>\n",
              "      <td>0.030837</td>\n",
              "      <td>0.004215</td>\n",
              "      <td>0.172032</td>\n",
              "      <td>0.968992</td>\n",
              "      <td>0.073266</td>\n",
              "      <td>0.377993</td>\n",
              "      <td>0.662088</td>\n",
              "      <td>0.272031</td>\n",
              "      <td>0.044321</td>\n",
              "      <td>...</td>\n",
              "      <td>0.807038</td>\n",
              "      <td>0.867018</td>\n",
              "      <td>0.494408</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.040556</td>\n",
              "      <td>0.123596</td>\n",
              "      <td>0.605827</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>0.647050</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-06-19</th>\n",
              "      <td>0.787234</td>\n",
              "      <td>0.056167</td>\n",
              "      <td>0.005872</td>\n",
              "      <td>0.198945</td>\n",
              "      <td>0.968992</td>\n",
              "      <td>0.060172</td>\n",
              "      <td>0.340728</td>\n",
              "      <td>0.692624</td>\n",
              "      <td>0.272031</td>\n",
              "      <td>0.032633</td>\n",
              "      <td>...</td>\n",
              "      <td>0.726190</td>\n",
              "      <td>0.846727</td>\n",
              "      <td>0.516186</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.059674</td>\n",
              "      <td>0.297753</td>\n",
              "      <td>0.630335</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>0.652147</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-27</th>\n",
              "      <td>0.489362</td>\n",
              "      <td>0.306167</td>\n",
              "      <td>0.381294</td>\n",
              "      <td>0.612401</td>\n",
              "      <td>0.483204</td>\n",
              "      <td>0.135160</td>\n",
              "      <td>0.389213</td>\n",
              "      <td>0.683206</td>\n",
              "      <td>0.601532</td>\n",
              "      <td>0.088239</td>\n",
              "      <td>...</td>\n",
              "      <td>0.524489</td>\n",
              "      <td>0.705111</td>\n",
              "      <td>0.479694</td>\n",
              "      <td>0.799999</td>\n",
              "      <td>0.024920</td>\n",
              "      <td>0.241573</td>\n",
              "      <td>0.530788</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.558542</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-28</th>\n",
              "      <td>0.489362</td>\n",
              "      <td>0.338106</td>\n",
              "      <td>0.383322</td>\n",
              "      <td>0.640370</td>\n",
              "      <td>0.483204</td>\n",
              "      <td>0.141856</td>\n",
              "      <td>0.314290</td>\n",
              "      <td>0.709289</td>\n",
              "      <td>0.601532</td>\n",
              "      <td>0.103880</td>\n",
              "      <td>...</td>\n",
              "      <td>0.617307</td>\n",
              "      <td>0.697679</td>\n",
              "      <td>0.556798</td>\n",
              "      <td>0.799999</td>\n",
              "      <td>0.030583</td>\n",
              "      <td>0.241573</td>\n",
              "      <td>0.656561</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.612759</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-29</th>\n",
              "      <td>0.489362</td>\n",
              "      <td>0.321586</td>\n",
              "      <td>0.399988</td>\n",
              "      <td>0.632718</td>\n",
              "      <td>0.483204</td>\n",
              "      <td>0.142272</td>\n",
              "      <td>0.387767</td>\n",
              "      <td>0.659848</td>\n",
              "      <td>0.601532</td>\n",
              "      <td>0.095688</td>\n",
              "      <td>...</td>\n",
              "      <td>0.446341</td>\n",
              "      <td>0.698275</td>\n",
              "      <td>0.515597</td>\n",
              "      <td>0.799999</td>\n",
              "      <td>0.034649</td>\n",
              "      <td>0.421348</td>\n",
              "      <td>0.589986</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.666821</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-30</th>\n",
              "      <td>0.489362</td>\n",
              "      <td>0.342511</td>\n",
              "      <td>0.395073</td>\n",
              "      <td>0.637731</td>\n",
              "      <td>0.483204</td>\n",
              "      <td>0.190858</td>\n",
              "      <td>0.327826</td>\n",
              "      <td>0.703464</td>\n",
              "      <td>0.601532</td>\n",
              "      <td>0.135988</td>\n",
              "      <td>...</td>\n",
              "      <td>0.603474</td>\n",
              "      <td>0.682922</td>\n",
              "      <td>0.516186</td>\n",
              "      <td>0.799999</td>\n",
              "      <td>0.049359</td>\n",
              "      <td>0.516854</td>\n",
              "      <td>0.634981</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.587736</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-31</th>\n",
              "      <td>0.489362</td>\n",
              "      <td>0.340308</td>\n",
              "      <td>0.401876</td>\n",
              "      <td>0.649604</td>\n",
              "      <td>0.483204</td>\n",
              "      <td>0.240406</td>\n",
              "      <td>0.384222</td>\n",
              "      <td>0.713676</td>\n",
              "      <td>0.601532</td>\n",
              "      <td>0.164589</td>\n",
              "      <td>...</td>\n",
              "      <td>0.559296</td>\n",
              "      <td>0.681297</td>\n",
              "      <td>0.550912</td>\n",
              "      <td>0.799999</td>\n",
              "      <td>0.029486</td>\n",
              "      <td>0.185393</td>\n",
              "      <td>0.463607</td>\n",
              "      <td>0.000022</td>\n",
              "      <td>0.629595</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1429 rows × 26 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b650a1ac-3e22-4cd8-b25e-5dc18477f666')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b650a1ac-3e22-4cd8-b25e-5dc18477f666 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b650a1ac-3e22-4cd8-b25e-5dc18477f666');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_sequences(sequence, n_steps): #windowing 함수\n",
        "  X, y = list(), list()\n",
        "  for i in range(len(sequence)):\n",
        "    end_ix = i + n_steps\n",
        "    if end_ix > len(sequence)-1:\n",
        "      break\n",
        "    seq_x, seq_y = sequence[i:end_ix], sequence[end_ix] #X는 2차원인 듯\n",
        "    X.append(seq_x)\n",
        "    y.append(seq_y)\n",
        "  return np.array(X), np.array(y)\n",
        "\n",
        "def data_setup(n_steps, sequence):\n",
        "  X, y = split_sequences(sequence, n_steps) #windowing 내부적으로 진행\n",
        "  n_features = X.shape[2]\n",
        "  X = X.reshape((len(X), n_steps, n_features))\n",
        "  new_y=[]\n",
        "  for term in y:\n",
        "    new_term = term[-1]\n",
        "    new_y.append(new_term)\n",
        "  return X, np.array(new_y), n_features\n",
        "\n",
        "n_steps = 3 # windowing 시점\n",
        "#n_seq=10000 \n",
        "rel_test_len = 0.2 #test set 비율\n",
        "X,y,n_features = data_setup(n_steps, values) #n_seq\n",
        "\n",
        "# X = X[:-1]\n",
        "# y = y[1:]\n",
        "#train test split\n",
        "X_test,y_test = X[:int(len(X)*rel_test_len)],y[:int(len(X)*rel_test_len)]\n",
        "X_train,y_train=X[int(len(X)*rel_test_len):],y[int(len(X)*rel_test_len):]\n",
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrCKdX2qoCUT",
        "outputId": "6aa0d193-a5c6-4ce8-a652-e35bc8401a94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1426, 3, 26)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#더미인코딩"
      ],
      "metadata": {
        "id": "Yh0Pr4VUmZ93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SK_df['buy']=SK_df['등락률'].apply(lambda x : 1 if x >3 else 0)\n",
        "SK_df['sell']=SK_df['등락률'].apply(lambda x : 1 if x <-3 else 0)"
      ],
      "metadata": {
        "id": "wbzareG2mdTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SK_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        },
        "id": "rK3f1F4Vmfjn",
        "outputId": "2fc13756-70f5-4d49-e693-40728a853769"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 종가   등락률        거래량          거래대금          시가총액  토론방  \\\n",
              "날짜                                                                      \n",
              "2017-06-13  58800.0  2.26  3431811.0  1.987173e+11  4.280654e+13  233   \n",
              "2017-06-14  59300.0  0.85  4441904.0  2.638198e+11  4.317054e+13  104   \n",
              "2017-06-15  60600.0  2.19  4549233.0  2.736022e+11  4.411694e+13  222   \n",
              "2017-06-16  60500.0 -0.17  3932203.0  2.386388e+11  4.404414e+13  131   \n",
              "2017-06-19  62800.0  3.80  5315543.0  3.319266e+11  4.571855e+13  168   \n",
              "...             ...   ...        ...           ...           ...  ...   \n",
              "2023-03-27  85500.0 -2.06  3211190.0  2.751969e+11  6.224420e+13   84   \n",
              "2023-03-28  88400.0  3.39  3180431.0  2.779770e+11  6.435541e+13   69   \n",
              "2023-03-29  86900.0 -1.70  3070422.0  2.675525e+11  6.326341e+13   90   \n",
              "2023-03-30  88800.0  2.19  4264354.0  3.799672e+11  6.464661e+13  141   \n",
              "2023-03-31  88600.0 -0.23  2676327.0  2.366869e+11  6.450101e+13   87   \n",
              "\n",
              "                  순매수_기관        순매수_개인       순매수_외국인      순매수_기타법인      검색어  \\\n",
              "날짜                                                                            \n",
              "2017-06-13  3.938152e+09 -1.325185e+10  8.168180e+09  1.145514e+09  5.65469   \n",
              "2017-06-14  3.378250e+10 -1.694931e+10 -1.667742e+10 -1.557680e+08  6.55951   \n",
              "2017-06-15  4.528110e+10 -5.105465e+10  8.727358e+09 -2.953806e+09  7.20316   \n",
              "2017-06-16  3.251843e+10  2.260853e+10 -5.685305e+10  1.726089e+09  6.22995   \n",
              "2017-06-19  4.404935e+10 -2.336965e+10 -2.064758e+10 -3.212170e+07  8.09842   \n",
              "...                  ...           ...           ...           ...      ...   \n",
              "2023-03-27 -2.786144e+09  3.645140e+10 -3.181490e+10 -1.850354e+09  4.70174   \n",
              "2023-03-28  5.638837e+10 -5.598959e+10 -8.886702e+08  4.898941e+08  5.25521   \n",
              "2023-03-29  2.506547e+10  3.466739e+10 -5.950933e+10 -2.235300e+08  5.65259   \n",
              "2023-03-30  4.623535e+10 -3.928891e+10 -7.795352e+09  8.489159e+08  7.09025   \n",
              "2023-03-31 -3.439400e+10  3.029416e+10  4.312767e+09 -2.129345e+08  5.14803   \n",
              "\n",
              "              보도량    기사감성점수  코스피대비     코스피거래대금 day3_label  buy  sell  \n",
              "날짜                                                                    \n",
              "2017-06-13   44.0  0.590909  16.83   5665179.0   maintain    0     0  \n",
              "2017-06-14   51.0  0.352941  -2.06   6795165.0   maintain    0     0  \n",
              "2017-06-15   44.0  0.227273 -10.99   5882836.0        buy    0     0  \n",
              "2017-06-16   34.0  0.588235   0.18   5965450.0   maintain    0     0  \n",
              "2017-06-19   65.0  0.461538   9.07   5421942.0        buy    1     0  \n",
              "...           ...       ...    ...         ...        ...  ...   ...  \n",
              "2023-03-27   55.0  0.145455  -5.74   8534650.0   maintain    0     0  \n",
              "2023-03-28   55.0  0.290909  25.72   8812618.0   maintain    1     0  \n",
              "2023-03-29   87.0  0.022989   8.98   8829889.0   maintain    0     0  \n",
              "2023-03-30  104.0  0.269231   9.24  10846640.0   maintain    0     0  \n",
              "2023-03-31   45.0  0.200000  23.70  12903370.0   maintain    0     0  \n",
              "\n",
              "[1429 rows x 18 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-438849b3-5cc9-4b3e-8cd1-48e104e6935a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>종가</th>\n",
              "      <th>등락률</th>\n",
              "      <th>거래량</th>\n",
              "      <th>거래대금</th>\n",
              "      <th>시가총액</th>\n",
              "      <th>토론방</th>\n",
              "      <th>순매수_기관</th>\n",
              "      <th>순매수_개인</th>\n",
              "      <th>순매수_외국인</th>\n",
              "      <th>순매수_기타법인</th>\n",
              "      <th>검색어</th>\n",
              "      <th>보도량</th>\n",
              "      <th>기사감성점수</th>\n",
              "      <th>코스피대비</th>\n",
              "      <th>코스피거래대금</th>\n",
              "      <th>day3_label</th>\n",
              "      <th>buy</th>\n",
              "      <th>sell</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>날짜</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2017-06-13</th>\n",
              "      <td>58800.0</td>\n",
              "      <td>2.26</td>\n",
              "      <td>3431811.0</td>\n",
              "      <td>1.987173e+11</td>\n",
              "      <td>4.280654e+13</td>\n",
              "      <td>233</td>\n",
              "      <td>3.938152e+09</td>\n",
              "      <td>-1.325185e+10</td>\n",
              "      <td>8.168180e+09</td>\n",
              "      <td>1.145514e+09</td>\n",
              "      <td>5.65469</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.590909</td>\n",
              "      <td>16.83</td>\n",
              "      <td>5665179.0</td>\n",
              "      <td>maintain</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-06-14</th>\n",
              "      <td>59300.0</td>\n",
              "      <td>0.85</td>\n",
              "      <td>4441904.0</td>\n",
              "      <td>2.638198e+11</td>\n",
              "      <td>4.317054e+13</td>\n",
              "      <td>104</td>\n",
              "      <td>3.378250e+10</td>\n",
              "      <td>-1.694931e+10</td>\n",
              "      <td>-1.667742e+10</td>\n",
              "      <td>-1.557680e+08</td>\n",
              "      <td>6.55951</td>\n",
              "      <td>51.0</td>\n",
              "      <td>0.352941</td>\n",
              "      <td>-2.06</td>\n",
              "      <td>6795165.0</td>\n",
              "      <td>maintain</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-06-15</th>\n",
              "      <td>60600.0</td>\n",
              "      <td>2.19</td>\n",
              "      <td>4549233.0</td>\n",
              "      <td>2.736022e+11</td>\n",
              "      <td>4.411694e+13</td>\n",
              "      <td>222</td>\n",
              "      <td>4.528110e+10</td>\n",
              "      <td>-5.105465e+10</td>\n",
              "      <td>8.727358e+09</td>\n",
              "      <td>-2.953806e+09</td>\n",
              "      <td>7.20316</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.227273</td>\n",
              "      <td>-10.99</td>\n",
              "      <td>5882836.0</td>\n",
              "      <td>buy</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-06-16</th>\n",
              "      <td>60500.0</td>\n",
              "      <td>-0.17</td>\n",
              "      <td>3932203.0</td>\n",
              "      <td>2.386388e+11</td>\n",
              "      <td>4.404414e+13</td>\n",
              "      <td>131</td>\n",
              "      <td>3.251843e+10</td>\n",
              "      <td>2.260853e+10</td>\n",
              "      <td>-5.685305e+10</td>\n",
              "      <td>1.726089e+09</td>\n",
              "      <td>6.22995</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.588235</td>\n",
              "      <td>0.18</td>\n",
              "      <td>5965450.0</td>\n",
              "      <td>maintain</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-06-19</th>\n",
              "      <td>62800.0</td>\n",
              "      <td>3.80</td>\n",
              "      <td>5315543.0</td>\n",
              "      <td>3.319266e+11</td>\n",
              "      <td>4.571855e+13</td>\n",
              "      <td>168</td>\n",
              "      <td>4.404935e+10</td>\n",
              "      <td>-2.336965e+10</td>\n",
              "      <td>-2.064758e+10</td>\n",
              "      <td>-3.212170e+07</td>\n",
              "      <td>8.09842</td>\n",
              "      <td>65.0</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>9.07</td>\n",
              "      <td>5421942.0</td>\n",
              "      <td>buy</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-27</th>\n",
              "      <td>85500.0</td>\n",
              "      <td>-2.06</td>\n",
              "      <td>3211190.0</td>\n",
              "      <td>2.751969e+11</td>\n",
              "      <td>6.224420e+13</td>\n",
              "      <td>84</td>\n",
              "      <td>-2.786144e+09</td>\n",
              "      <td>3.645140e+10</td>\n",
              "      <td>-3.181490e+10</td>\n",
              "      <td>-1.850354e+09</td>\n",
              "      <td>4.70174</td>\n",
              "      <td>55.0</td>\n",
              "      <td>0.145455</td>\n",
              "      <td>-5.74</td>\n",
              "      <td>8534650.0</td>\n",
              "      <td>maintain</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-28</th>\n",
              "      <td>88400.0</td>\n",
              "      <td>3.39</td>\n",
              "      <td>3180431.0</td>\n",
              "      <td>2.779770e+11</td>\n",
              "      <td>6.435541e+13</td>\n",
              "      <td>69</td>\n",
              "      <td>5.638837e+10</td>\n",
              "      <td>-5.598959e+10</td>\n",
              "      <td>-8.886702e+08</td>\n",
              "      <td>4.898941e+08</td>\n",
              "      <td>5.25521</td>\n",
              "      <td>55.0</td>\n",
              "      <td>0.290909</td>\n",
              "      <td>25.72</td>\n",
              "      <td>8812618.0</td>\n",
              "      <td>maintain</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-29</th>\n",
              "      <td>86900.0</td>\n",
              "      <td>-1.70</td>\n",
              "      <td>3070422.0</td>\n",
              "      <td>2.675525e+11</td>\n",
              "      <td>6.326341e+13</td>\n",
              "      <td>90</td>\n",
              "      <td>2.506547e+10</td>\n",
              "      <td>3.466739e+10</td>\n",
              "      <td>-5.950933e+10</td>\n",
              "      <td>-2.235300e+08</td>\n",
              "      <td>5.65259</td>\n",
              "      <td>87.0</td>\n",
              "      <td>0.022989</td>\n",
              "      <td>8.98</td>\n",
              "      <td>8829889.0</td>\n",
              "      <td>maintain</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-30</th>\n",
              "      <td>88800.0</td>\n",
              "      <td>2.19</td>\n",
              "      <td>4264354.0</td>\n",
              "      <td>3.799672e+11</td>\n",
              "      <td>6.464661e+13</td>\n",
              "      <td>141</td>\n",
              "      <td>4.623535e+10</td>\n",
              "      <td>-3.928891e+10</td>\n",
              "      <td>-7.795352e+09</td>\n",
              "      <td>8.489159e+08</td>\n",
              "      <td>7.09025</td>\n",
              "      <td>104.0</td>\n",
              "      <td>0.269231</td>\n",
              "      <td>9.24</td>\n",
              "      <td>10846640.0</td>\n",
              "      <td>maintain</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-31</th>\n",
              "      <td>88600.0</td>\n",
              "      <td>-0.23</td>\n",
              "      <td>2676327.0</td>\n",
              "      <td>2.366869e+11</td>\n",
              "      <td>6.450101e+13</td>\n",
              "      <td>87</td>\n",
              "      <td>-3.439400e+10</td>\n",
              "      <td>3.029416e+10</td>\n",
              "      <td>4.312767e+09</td>\n",
              "      <td>-2.129345e+08</td>\n",
              "      <td>5.14803</td>\n",
              "      <td>45.0</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>23.70</td>\n",
              "      <td>12903370.0</td>\n",
              "      <td>maintain</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1429 rows × 18 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-438849b3-5cc9-4b3e-8cd1-48e104e6935a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-438849b3-5cc9-4b3e-8cd1-48e104e6935a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-438849b3-5cc9-4b3e-8cd1-48e104e6935a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_data(dataset): #X, y 모두 정규화\n",
        "  cols = dataset.columns.tolist()\n",
        "  col_name = [0]*len(cols)\n",
        "  for i in range(len(cols)):\n",
        "    col_name[i]=i\n",
        "  dataset.columns = col_name\n",
        "  dtypes = dataset.dtypes.tolist()\n",
        "  minmax = list()\n",
        "  for column in dataset:\n",
        "    dataset = dataset.astype({column: 'float32'})\n",
        "  for i in range(len(cols)):\n",
        "    col_values = dataset[col_name[i]]\n",
        "    value_min = min(col_values)\n",
        "    value_max = max(col_values)\n",
        "    minmax.append([value_min,value_max])\n",
        "  for column in dataset:\n",
        "    values = dataset[column].values\n",
        "    for i in range(len(values)):\n",
        "      values[i]=(values[i]-minmax[column][0])/(minmax[column][1]-minmax[column][0])\n",
        "    dataset[column] = values\n",
        "  dataset[column]=values\n",
        "  return dataset, minmax"
      ],
      "metadata": {
        "id": "sQ7idjnOmgaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset, minmax = normalize_data(SK_df.drop(['day3_label', 'buy', 'sell'], axis=1))\n",
        "\n",
        "dataset['buy'] = SK_df['buy']\n",
        "dataset['sell'] = SK_df['sell']\n",
        "\n",
        "values = dataset.values\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        },
        "id": "zaqAsCcynwU2",
        "outputId": "142cd07f-ac2d-4cda-81a8-7d0883487e34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   0         1         2         3         4         5  \\\n",
              "날짜                                                                       \n",
              "2017-06-13  0.012115  0.504448  0.124810  0.057120  0.012115  0.047985   \n",
              "2017-06-14  0.017621  0.441726  0.181050  0.092440  0.017621  0.017936   \n",
              "2017-06-15  0.031938  0.501335  0.187026  0.097748  0.031938  0.045423   \n",
              "2017-06-16  0.030837  0.396352  0.152671  0.078779  0.030837  0.024225   \n",
              "2017-06-19  0.056167  0.572954  0.229692  0.129390  0.056167  0.032844   \n",
              "...              ...       ...       ...       ...       ...       ...   \n",
              "2023-03-27  0.306167  0.312278  0.112526  0.098613  0.306167  0.013277   \n",
              "2023-03-28  0.338106  0.554715  0.110814  0.100121  0.338106  0.009783   \n",
              "2023-03-29  0.321586  0.328292  0.104689  0.094465  0.321586  0.014675   \n",
              "2023-03-30  0.342511  0.501335  0.171164  0.155454  0.342511  0.026555   \n",
              "2023-03-31  0.340308  0.393683  0.082746  0.077720  0.340308  0.013976   \n",
              "\n",
              "                   6         7         8         9        10        11  \\\n",
              "날짜                                                                       \n",
              "2017-06-13  0.545081  0.348929  0.716928  0.235175  0.034670  0.179775   \n",
              "2017-06-14  0.608513  0.345932  0.695973  0.221014  0.043928  0.219101   \n",
              "2017-06-15  0.632953  0.318290  0.717399  0.190564  0.050514  0.179775   \n",
              "2017-06-16  0.605827  0.377993  0.662088  0.241494  0.040556  0.123596   \n",
              "2017-06-19  0.630335  0.340728  0.692624  0.222360  0.059674  0.297753   \n",
              "...              ...       ...       ...       ...       ...       ...   \n",
              "2023-03-27  0.530788  0.389213  0.683206  0.202573  0.024920  0.241573   \n",
              "2023-03-28  0.656561  0.314290  0.709289  0.228041  0.030583  0.241573   \n",
              "2023-03-29  0.589986  0.387767  0.659848  0.220277  0.034649  0.421348   \n",
              "2023-03-30  0.634981  0.327826  0.703464  0.231948  0.049359  0.516854   \n",
              "2023-03-31  0.463607  0.384222  0.713676  0.220392  0.029486  0.185393   \n",
              "\n",
              "                  12        13        14  buy  sell  \n",
              "날짜                                                   \n",
              "2017-06-13  0.808744  0.576052  0.066032    0     0  \n",
              "2017-06-14  0.656892  0.503696  0.093254    0     0  \n",
              "2017-06-15  0.576700  0.469491  0.071275    0     0  \n",
              "2017-06-16  0.807038  0.512276  0.073266    0     0  \n",
              "2017-06-19  0.726190  0.546329  0.060172    1     0  \n",
              "...              ...       ...       ...  ...   ...  \n",
              "2023-03-27  0.524489  0.489600  0.135160    0     0  \n",
              "2023-03-28  0.617307  0.610105  0.141856    1     0  \n",
              "2023-03-29  0.446341  0.545984  0.142272    0     0  \n",
              "2023-03-30  0.603474  0.546980  0.190858    0     0  \n",
              "2023-03-31  0.559296  0.602367  0.240406    0     0  \n",
              "\n",
              "[1429 rows x 17 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-59aa11c5-8897-4391-91c7-f2c91754a62d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>buy</th>\n",
              "      <th>sell</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>날짜</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2017-06-13</th>\n",
              "      <td>0.012115</td>\n",
              "      <td>0.504448</td>\n",
              "      <td>0.124810</td>\n",
              "      <td>0.057120</td>\n",
              "      <td>0.012115</td>\n",
              "      <td>0.047985</td>\n",
              "      <td>0.545081</td>\n",
              "      <td>0.348929</td>\n",
              "      <td>0.716928</td>\n",
              "      <td>0.235175</td>\n",
              "      <td>0.034670</td>\n",
              "      <td>0.179775</td>\n",
              "      <td>0.808744</td>\n",
              "      <td>0.576052</td>\n",
              "      <td>0.066032</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-06-14</th>\n",
              "      <td>0.017621</td>\n",
              "      <td>0.441726</td>\n",
              "      <td>0.181050</td>\n",
              "      <td>0.092440</td>\n",
              "      <td>0.017621</td>\n",
              "      <td>0.017936</td>\n",
              "      <td>0.608513</td>\n",
              "      <td>0.345932</td>\n",
              "      <td>0.695973</td>\n",
              "      <td>0.221014</td>\n",
              "      <td>0.043928</td>\n",
              "      <td>0.219101</td>\n",
              "      <td>0.656892</td>\n",
              "      <td>0.503696</td>\n",
              "      <td>0.093254</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-06-15</th>\n",
              "      <td>0.031938</td>\n",
              "      <td>0.501335</td>\n",
              "      <td>0.187026</td>\n",
              "      <td>0.097748</td>\n",
              "      <td>0.031938</td>\n",
              "      <td>0.045423</td>\n",
              "      <td>0.632953</td>\n",
              "      <td>0.318290</td>\n",
              "      <td>0.717399</td>\n",
              "      <td>0.190564</td>\n",
              "      <td>0.050514</td>\n",
              "      <td>0.179775</td>\n",
              "      <td>0.576700</td>\n",
              "      <td>0.469491</td>\n",
              "      <td>0.071275</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-06-16</th>\n",
              "      <td>0.030837</td>\n",
              "      <td>0.396352</td>\n",
              "      <td>0.152671</td>\n",
              "      <td>0.078779</td>\n",
              "      <td>0.030837</td>\n",
              "      <td>0.024225</td>\n",
              "      <td>0.605827</td>\n",
              "      <td>0.377993</td>\n",
              "      <td>0.662088</td>\n",
              "      <td>0.241494</td>\n",
              "      <td>0.040556</td>\n",
              "      <td>0.123596</td>\n",
              "      <td>0.807038</td>\n",
              "      <td>0.512276</td>\n",
              "      <td>0.073266</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-06-19</th>\n",
              "      <td>0.056167</td>\n",
              "      <td>0.572954</td>\n",
              "      <td>0.229692</td>\n",
              "      <td>0.129390</td>\n",
              "      <td>0.056167</td>\n",
              "      <td>0.032844</td>\n",
              "      <td>0.630335</td>\n",
              "      <td>0.340728</td>\n",
              "      <td>0.692624</td>\n",
              "      <td>0.222360</td>\n",
              "      <td>0.059674</td>\n",
              "      <td>0.297753</td>\n",
              "      <td>0.726190</td>\n",
              "      <td>0.546329</td>\n",
              "      <td>0.060172</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-27</th>\n",
              "      <td>0.306167</td>\n",
              "      <td>0.312278</td>\n",
              "      <td>0.112526</td>\n",
              "      <td>0.098613</td>\n",
              "      <td>0.306167</td>\n",
              "      <td>0.013277</td>\n",
              "      <td>0.530788</td>\n",
              "      <td>0.389213</td>\n",
              "      <td>0.683206</td>\n",
              "      <td>0.202573</td>\n",
              "      <td>0.024920</td>\n",
              "      <td>0.241573</td>\n",
              "      <td>0.524489</td>\n",
              "      <td>0.489600</td>\n",
              "      <td>0.135160</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-28</th>\n",
              "      <td>0.338106</td>\n",
              "      <td>0.554715</td>\n",
              "      <td>0.110814</td>\n",
              "      <td>0.100121</td>\n",
              "      <td>0.338106</td>\n",
              "      <td>0.009783</td>\n",
              "      <td>0.656561</td>\n",
              "      <td>0.314290</td>\n",
              "      <td>0.709289</td>\n",
              "      <td>0.228041</td>\n",
              "      <td>0.030583</td>\n",
              "      <td>0.241573</td>\n",
              "      <td>0.617307</td>\n",
              "      <td>0.610105</td>\n",
              "      <td>0.141856</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-29</th>\n",
              "      <td>0.321586</td>\n",
              "      <td>0.328292</td>\n",
              "      <td>0.104689</td>\n",
              "      <td>0.094465</td>\n",
              "      <td>0.321586</td>\n",
              "      <td>0.014675</td>\n",
              "      <td>0.589986</td>\n",
              "      <td>0.387767</td>\n",
              "      <td>0.659848</td>\n",
              "      <td>0.220277</td>\n",
              "      <td>0.034649</td>\n",
              "      <td>0.421348</td>\n",
              "      <td>0.446341</td>\n",
              "      <td>0.545984</td>\n",
              "      <td>0.142272</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-30</th>\n",
              "      <td>0.342511</td>\n",
              "      <td>0.501335</td>\n",
              "      <td>0.171164</td>\n",
              "      <td>0.155454</td>\n",
              "      <td>0.342511</td>\n",
              "      <td>0.026555</td>\n",
              "      <td>0.634981</td>\n",
              "      <td>0.327826</td>\n",
              "      <td>0.703464</td>\n",
              "      <td>0.231948</td>\n",
              "      <td>0.049359</td>\n",
              "      <td>0.516854</td>\n",
              "      <td>0.603474</td>\n",
              "      <td>0.546980</td>\n",
              "      <td>0.190858</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023-03-31</th>\n",
              "      <td>0.340308</td>\n",
              "      <td>0.393683</td>\n",
              "      <td>0.082746</td>\n",
              "      <td>0.077720</td>\n",
              "      <td>0.340308</td>\n",
              "      <td>0.013976</td>\n",
              "      <td>0.463607</td>\n",
              "      <td>0.384222</td>\n",
              "      <td>0.713676</td>\n",
              "      <td>0.220392</td>\n",
              "      <td>0.029486</td>\n",
              "      <td>0.185393</td>\n",
              "      <td>0.559296</td>\n",
              "      <td>0.602367</td>\n",
              "      <td>0.240406</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1429 rows × 17 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-59aa11c5-8897-4391-91c7-f2c91754a62d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-59aa11c5-8897-4391-91c7-f2c91754a62d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-59aa11c5-8897-4391-91c7-f2c91754a62d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_sequences(sequence, n_steps): #windowing 함수\n",
        "  X, y = list(), list()\n",
        "  for i in range(len(sequence)):\n",
        "    end_ix = i + n_steps\n",
        "    if end_ix > len(sequence)-1:\n",
        "      break\n",
        "    seq_x, seq_y = sequence[i:end_ix], sequence[end_ix] #X는 2차원인 듯\n",
        "    X.append(seq_x)\n",
        "    y.append(seq_y)\n",
        "  return np.array(X), np.array(y)\n",
        "\n",
        "def data_setup(n_steps, sequence):\n",
        "  X, y = split_sequences(sequence, n_steps) #windowing 내부적으로 진행\n",
        "  n_features = X.shape[2]\n",
        "  X = X.reshape((len(X), n_steps, n_features))\n",
        "  new_y=[]\n",
        "  for term in y:\n",
        "    new_term = term[-1]\n",
        "    new_y.append(new_term)\n",
        "  return X, np.array(new_y), n_features\n",
        "\n",
        "n_steps = 4 # windowing 시점\n",
        "#n_seq=10000 \n",
        "rel_test_len=0.1 #test set 비율\n",
        "X,y,n_features = data_setup(n_steps, values) #n_seq\n",
        "\n",
        "X = X[:-1]\n",
        "y = y[1:]\n",
        "#train test split\n",
        "X_test,y_test = X[:int(len(X)*rel_test_len)],y[:int(len(X)*rel_test_len)]\n",
        "X_train,y_train=X[int(len(X)*rel_test_len):],y[int(len(X)*rel_test_len):]\n",
        "X.shape"
      ],
      "metadata": {
        "id": "KxDL6kNMorBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LSTM"
      ],
      "metadata": {
        "id": "LzzRr4HlO7Ex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import LSTM\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(64, activation=None, input_shape=(3, 26), return_sequences=True))\n",
        "model.add(LSTM(32, activation=None, return_sequences = True))\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256, activation=None))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(64, activation=None))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "CfrlVhAnphc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c0c997f-b60e-485a-8f23-bd96f0845187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 3, 64)             23296     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 3, 32)             12416     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 96)                0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 96)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               24832     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                16448     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 3)                 195       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 77,187\n",
            "Trainable params: 77,187\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "hX9GuPDgVNZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from keras import callbacks\n",
        "\n",
        "epochs=800\n",
        "verbosity = 2\n",
        "dirx ='/content/drive/MyDrive/2023-1학기-시계열팀주제분석/checkpoints'\n",
        "os.chdir(dirx)\n",
        "h5 = 'network.h5'\n",
        "checkpoint = callbacks.ModelCheckpoint(h5,\n",
        "                                       monitor='val_loss',\n",
        "                                       verbose=0,\n",
        "                                       save_best_only=True,\n",
        "                                       save_weights_only=True,\n",
        "                                       mode='auto',\n",
        "                                       period=1)\n",
        "\n",
        "callback = [checkpoint]\n",
        "json = 'network.json'\n",
        "model_json = model.to_json()\n",
        "\n",
        "with open(json, 'w') as json_file:\n",
        "  json_file.write(model_json)\n",
        "history = model.fit(X_train,\n",
        "                    y_train,\n",
        "                    epochs = epochs,\n",
        "                    batch_size =len(X_train)//4,\n",
        "                    validation_data = (X_test,y_test),\n",
        "                    verbose = verbosity,\n",
        "                    callbacks=callback)"
      ],
      "metadata": {
        "id": "SPBfApzIqQuc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ffd5080-7096-477a-b73a-d379e8d78387"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/800\n",
            "5/5 - 9s - loss: 0.9719 - accuracy: 0.6722 - val_loss: 0.7050 - val_accuracy: 0.7930 - 9s/epoch - 2s/step\n",
            "Epoch 2/800\n",
            "5/5 - 0s - loss: 0.6320 - accuracy: 0.8230 - val_loss: 0.7218 - val_accuracy: 0.7930 - 200ms/epoch - 40ms/step\n",
            "Epoch 3/800\n",
            "5/5 - 0s - loss: 0.6218 - accuracy: 0.8230 - val_loss: 0.6818 - val_accuracy: 0.7930 - 248ms/epoch - 50ms/step\n",
            "Epoch 4/800\n",
            "5/5 - 0s - loss: 0.5995 - accuracy: 0.8230 - val_loss: 0.6650 - val_accuracy: 0.7930 - 239ms/epoch - 48ms/step\n",
            "Epoch 5/800\n",
            "5/5 - 0s - loss: 0.5891 - accuracy: 0.8230 - val_loss: 0.6691 - val_accuracy: 0.7930 - 206ms/epoch - 41ms/step\n",
            "Epoch 6/800\n",
            "5/5 - 0s - loss: 0.5925 - accuracy: 0.8230 - val_loss: 0.6682 - val_accuracy: 0.7930 - 207ms/epoch - 41ms/step\n",
            "Epoch 7/800\n",
            "5/5 - 0s - loss: 0.6001 - accuracy: 0.8230 - val_loss: 0.6545 - val_accuracy: 0.7930 - 231ms/epoch - 46ms/step\n",
            "Epoch 8/800\n",
            "5/5 - 0s - loss: 0.5913 - accuracy: 0.8230 - val_loss: 0.6572 - val_accuracy: 0.7930 - 185ms/epoch - 37ms/step\n",
            "Epoch 9/800\n",
            "5/5 - 0s - loss: 0.6050 - accuracy: 0.8230 - val_loss: 0.6801 - val_accuracy: 0.7930 - 166ms/epoch - 33ms/step\n",
            "Epoch 10/800\n",
            "5/5 - 0s - loss: 0.6060 - accuracy: 0.8230 - val_loss: 0.7028 - val_accuracy: 0.7930 - 235ms/epoch - 47ms/step\n",
            "Epoch 11/800\n",
            "5/5 - 0s - loss: 0.6203 - accuracy: 0.8230 - val_loss: 0.6946 - val_accuracy: 0.7930 - 99ms/epoch - 20ms/step\n",
            "Epoch 12/800\n",
            "5/5 - 0s - loss: 0.6095 - accuracy: 0.8230 - val_loss: 0.6829 - val_accuracy: 0.7930 - 250ms/epoch - 50ms/step\n",
            "Epoch 13/800\n",
            "5/5 - 0s - loss: 0.5908 - accuracy: 0.8230 - val_loss: 0.6751 - val_accuracy: 0.7930 - 255ms/epoch - 51ms/step\n",
            "Epoch 14/800\n",
            "5/5 - 0s - loss: 0.5931 - accuracy: 0.8230 - val_loss: 0.6738 - val_accuracy: 0.7930 - 98ms/epoch - 20ms/step\n",
            "Epoch 15/800\n",
            "5/5 - 0s - loss: 0.5824 - accuracy: 0.8230 - val_loss: 0.6645 - val_accuracy: 0.7930 - 114ms/epoch - 23ms/step\n",
            "Epoch 16/800\n",
            "5/5 - 0s - loss: 0.5895 - accuracy: 0.8230 - val_loss: 0.6527 - val_accuracy: 0.7930 - 120ms/epoch - 24ms/step\n",
            "Epoch 17/800\n",
            "5/5 - 0s - loss: 0.6278 - accuracy: 0.8230 - val_loss: 0.6778 - val_accuracy: 0.7930 - 104ms/epoch - 21ms/step\n",
            "Epoch 18/800\n",
            "5/5 - 0s - loss: 0.6509 - accuracy: 0.8230 - val_loss: 0.6559 - val_accuracy: 0.7930 - 99ms/epoch - 20ms/step\n",
            "Epoch 19/800\n",
            "5/5 - 0s - loss: 0.6094 - accuracy: 0.8230 - val_loss: 0.6647 - val_accuracy: 0.7930 - 101ms/epoch - 20ms/step\n",
            "Epoch 20/800\n",
            "5/5 - 0s - loss: 0.5931 - accuracy: 0.8230 - val_loss: 0.6533 - val_accuracy: 0.7930 - 103ms/epoch - 21ms/step\n",
            "Epoch 21/800\n",
            "5/5 - 0s - loss: 0.5936 - accuracy: 0.8230 - val_loss: 0.6534 - val_accuracy: 0.7930 - 115ms/epoch - 23ms/step\n",
            "Epoch 22/800\n",
            "5/5 - 0s - loss: 0.5987 - accuracy: 0.8230 - val_loss: 0.6584 - val_accuracy: 0.7930 - 100ms/epoch - 20ms/step\n",
            "Epoch 23/800\n",
            "5/5 - 0s - loss: 0.6067 - accuracy: 0.8230 - val_loss: 0.6623 - val_accuracy: 0.7930 - 94ms/epoch - 19ms/step\n",
            "Epoch 24/800\n",
            "5/5 - 0s - loss: 0.5979 - accuracy: 0.8230 - val_loss: 0.6690 - val_accuracy: 0.7930 - 98ms/epoch - 20ms/step\n",
            "Epoch 25/800\n",
            "5/5 - 0s - loss: 0.5986 - accuracy: 0.8230 - val_loss: 0.6851 - val_accuracy: 0.7930 - 102ms/epoch - 20ms/step\n",
            "Epoch 26/800\n",
            "5/5 - 0s - loss: 0.5948 - accuracy: 0.8230 - val_loss: 0.7223 - val_accuracy: 0.7930 - 103ms/epoch - 21ms/step\n",
            "Epoch 27/800\n",
            "5/5 - 0s - loss: 0.6147 - accuracy: 0.8230 - val_loss: 0.7328 - val_accuracy: 0.7930 - 107ms/epoch - 21ms/step\n",
            "Epoch 28/800\n",
            "5/5 - 0s - loss: 0.6071 - accuracy: 0.8230 - val_loss: 0.7037 - val_accuracy: 0.7930 - 92ms/epoch - 18ms/step\n",
            "Epoch 29/800\n",
            "5/5 - 0s - loss: 0.6003 - accuracy: 0.8230 - val_loss: 0.6815 - val_accuracy: 0.7930 - 119ms/epoch - 24ms/step\n",
            "Epoch 30/800\n",
            "5/5 - 0s - loss: 0.5863 - accuracy: 0.8230 - val_loss: 0.6726 - val_accuracy: 0.7930 - 95ms/epoch - 19ms/step\n",
            "Epoch 31/800\n",
            "5/5 - 0s - loss: 0.5852 - accuracy: 0.8230 - val_loss: 0.6730 - val_accuracy: 0.7930 - 100ms/epoch - 20ms/step\n",
            "Epoch 32/800\n",
            "5/5 - 0s - loss: 0.5903 - accuracy: 0.8230 - val_loss: 0.6794 - val_accuracy: 0.7930 - 99ms/epoch - 20ms/step\n",
            "Epoch 33/800\n",
            "5/5 - 0s - loss: 0.5894 - accuracy: 0.8230 - val_loss: 0.6865 - val_accuracy: 0.7930 - 110ms/epoch - 22ms/step\n",
            "Epoch 34/800\n",
            "5/5 - 0s - loss: 0.5886 - accuracy: 0.8230 - val_loss: 0.6876 - val_accuracy: 0.7930 - 106ms/epoch - 21ms/step\n",
            "Epoch 35/800\n",
            "5/5 - 0s - loss: 0.5934 - accuracy: 0.8230 - val_loss: 0.6822 - val_accuracy: 0.7930 - 111ms/epoch - 22ms/step\n",
            "Epoch 36/800\n",
            "5/5 - 0s - loss: 0.5847 - accuracy: 0.8230 - val_loss: 0.6743 - val_accuracy: 0.7930 - 114ms/epoch - 23ms/step\n",
            "Epoch 37/800\n",
            "5/5 - 0s - loss: 0.5722 - accuracy: 0.8230 - val_loss: 0.6785 - val_accuracy: 0.7930 - 96ms/epoch - 19ms/step\n",
            "Epoch 38/800\n",
            "5/5 - 0s - loss: 0.5855 - accuracy: 0.8230 - val_loss: 0.6851 - val_accuracy: 0.7930 - 116ms/epoch - 23ms/step\n",
            "Epoch 39/800\n",
            "5/5 - 0s - loss: 0.5745 - accuracy: 0.8230 - val_loss: 0.6888 - val_accuracy: 0.7930 - 95ms/epoch - 19ms/step\n",
            "Epoch 40/800\n",
            "5/5 - 0s - loss: 0.5819 - accuracy: 0.8230 - val_loss: 0.6850 - val_accuracy: 0.7930 - 93ms/epoch - 19ms/step\n",
            "Epoch 41/800\n",
            "5/5 - 0s - loss: 0.5829 - accuracy: 0.8230 - val_loss: 0.6806 - val_accuracy: 0.7930 - 106ms/epoch - 21ms/step\n",
            "Epoch 42/800\n",
            "5/5 - 0s - loss: 0.5712 - accuracy: 0.8230 - val_loss: 0.6834 - val_accuracy: 0.7930 - 108ms/epoch - 22ms/step\n",
            "Epoch 43/800\n",
            "5/5 - 0s - loss: 0.5744 - accuracy: 0.8230 - val_loss: 0.6872 - val_accuracy: 0.7930 - 93ms/epoch - 19ms/step\n",
            "Epoch 44/800\n",
            "5/5 - 0s - loss: 0.5762 - accuracy: 0.8230 - val_loss: 0.6706 - val_accuracy: 0.7930 - 100ms/epoch - 20ms/step\n",
            "Epoch 45/800\n",
            "5/5 - 0s - loss: 0.5633 - accuracy: 0.8230 - val_loss: 0.6540 - val_accuracy: 0.7930 - 110ms/epoch - 22ms/step\n",
            "Epoch 46/800\n",
            "5/5 - 0s - loss: 0.5873 - accuracy: 0.8230 - val_loss: 0.6613 - val_accuracy: 0.7930 - 93ms/epoch - 19ms/step\n",
            "Epoch 47/800\n",
            "5/5 - 0s - loss: 0.5753 - accuracy: 0.8230 - val_loss: 0.6919 - val_accuracy: 0.7930 - 93ms/epoch - 19ms/step\n",
            "Epoch 48/800\n",
            "5/5 - 0s - loss: 0.5870 - accuracy: 0.8230 - val_loss: 0.7225 - val_accuracy: 0.7930 - 99ms/epoch - 20ms/step\n",
            "Epoch 49/800\n",
            "5/5 - 0s - loss: 0.5932 - accuracy: 0.8230 - val_loss: 0.7141 - val_accuracy: 0.7930 - 97ms/epoch - 19ms/step\n",
            "Epoch 50/800\n",
            "5/5 - 0s - loss: 0.5870 - accuracy: 0.8230 - val_loss: 0.6714 - val_accuracy: 0.7930 - 108ms/epoch - 22ms/step\n",
            "Epoch 51/800\n",
            "5/5 - 0s - loss: 0.5716 - accuracy: 0.8230 - val_loss: 0.6469 - val_accuracy: 0.7930 - 148ms/epoch - 30ms/step\n",
            "Epoch 52/800\n",
            "5/5 - 0s - loss: 0.5909 - accuracy: 0.8221 - val_loss: 0.6503 - val_accuracy: 0.7930 - 99ms/epoch - 20ms/step\n",
            "Epoch 53/800\n",
            "5/5 - 0s - loss: 0.5800 - accuracy: 0.8238 - val_loss: 0.6602 - val_accuracy: 0.7930 - 102ms/epoch - 20ms/step\n",
            "Epoch 54/800\n",
            "5/5 - 0s - loss: 0.5840 - accuracy: 0.8238 - val_loss: 0.6716 - val_accuracy: 0.7930 - 101ms/epoch - 20ms/step\n",
            "Epoch 55/800\n",
            "5/5 - 0s - loss: 0.6463 - accuracy: 0.8230 - val_loss: 0.7533 - val_accuracy: 0.7930 - 107ms/epoch - 21ms/step\n",
            "Epoch 56/800\n",
            "5/5 - 0s - loss: 0.7295 - accuracy: 0.8116 - val_loss: 0.7849 - val_accuracy: 0.7930 - 100ms/epoch - 20ms/step\n",
            "Epoch 57/800\n",
            "5/5 - 0s - loss: 0.7039 - accuracy: 0.8124 - val_loss: 0.7021 - val_accuracy: 0.7930 - 108ms/epoch - 22ms/step\n",
            "Epoch 58/800\n",
            "5/5 - 0s - loss: 0.6113 - accuracy: 0.8221 - val_loss: 0.6767 - val_accuracy: 0.7930 - 106ms/epoch - 21ms/step\n",
            "Epoch 59/800\n",
            "5/5 - 0s - loss: 0.5827 - accuracy: 0.8230 - val_loss: 0.6790 - val_accuracy: 0.7930 - 117ms/epoch - 23ms/step\n",
            "Epoch 60/800\n",
            "5/5 - 0s - loss: 0.5798 - accuracy: 0.8230 - val_loss: 0.6820 - val_accuracy: 0.7930 - 105ms/epoch - 21ms/step\n",
            "Epoch 61/800\n",
            "5/5 - 0s - loss: 0.5736 - accuracy: 0.8230 - val_loss: 0.6914 - val_accuracy: 0.7930 - 106ms/epoch - 21ms/step\n",
            "Epoch 62/800\n",
            "5/5 - 0s - loss: 0.5673 - accuracy: 0.8230 - val_loss: 0.6845 - val_accuracy: 0.7930 - 121ms/epoch - 24ms/step\n",
            "Epoch 63/800\n",
            "5/5 - 0s - loss: 0.5660 - accuracy: 0.8238 - val_loss: 0.6624 - val_accuracy: 0.7930 - 104ms/epoch - 21ms/step\n",
            "Epoch 64/800\n",
            "5/5 - 0s - loss: 0.5686 - accuracy: 0.8230 - val_loss: 0.6615 - val_accuracy: 0.7930 - 92ms/epoch - 18ms/step\n",
            "Epoch 65/800\n",
            "5/5 - 0s - loss: 0.5683 - accuracy: 0.8221 - val_loss: 0.6675 - val_accuracy: 0.7930 - 97ms/epoch - 19ms/step\n",
            "Epoch 66/800\n",
            "5/5 - 0s - loss: 0.5665 - accuracy: 0.8221 - val_loss: 0.6729 - val_accuracy: 0.7930 - 102ms/epoch - 20ms/step\n",
            "Epoch 67/800\n",
            "5/5 - 0s - loss: 0.5670 - accuracy: 0.8221 - val_loss: 0.6661 - val_accuracy: 0.7930 - 101ms/epoch - 20ms/step\n",
            "Epoch 68/800\n",
            "5/5 - 0s - loss: 0.5665 - accuracy: 0.8230 - val_loss: 0.6638 - val_accuracy: 0.7930 - 97ms/epoch - 19ms/step\n",
            "Epoch 69/800\n",
            "5/5 - 0s - loss: 0.5615 - accuracy: 0.8221 - val_loss: 0.6695 - val_accuracy: 0.7930 - 95ms/epoch - 19ms/step\n",
            "Epoch 70/800\n",
            "5/5 - 0s - loss: 0.5627 - accuracy: 0.8230 - val_loss: 0.6820 - val_accuracy: 0.7930 - 100ms/epoch - 20ms/step\n",
            "Epoch 71/800\n",
            "5/5 - 0s - loss: 0.5616 - accuracy: 0.8247 - val_loss: 0.6887 - val_accuracy: 0.7930 - 100ms/epoch - 20ms/step\n",
            "Epoch 72/800\n",
            "5/5 - 0s - loss: 0.5769 - accuracy: 0.8221 - val_loss: 0.6836 - val_accuracy: 0.7930 - 111ms/epoch - 22ms/step\n",
            "Epoch 73/800\n",
            "5/5 - 0s - loss: 0.5684 - accuracy: 0.8212 - val_loss: 0.6775 - val_accuracy: 0.7930 - 112ms/epoch - 22ms/step\n",
            "Epoch 74/800\n",
            "5/5 - 0s - loss: 0.5596 - accuracy: 0.8212 - val_loss: 0.6733 - val_accuracy: 0.7930 - 97ms/epoch - 19ms/step\n",
            "Epoch 75/800\n",
            "5/5 - 0s - loss: 0.5605 - accuracy: 0.8230 - val_loss: 0.6737 - val_accuracy: 0.7930 - 97ms/epoch - 19ms/step\n",
            "Epoch 76/800\n",
            "5/5 - 0s - loss: 0.5594 - accuracy: 0.8221 - val_loss: 0.6714 - val_accuracy: 0.7930 - 96ms/epoch - 19ms/step\n",
            "Epoch 77/800\n",
            "5/5 - 0s - loss: 0.5546 - accuracy: 0.8221 - val_loss: 0.6703 - val_accuracy: 0.7930 - 93ms/epoch - 19ms/step\n",
            "Epoch 78/800\n",
            "5/5 - 0s - loss: 0.5666 - accuracy: 0.8230 - val_loss: 0.6607 - val_accuracy: 0.7930 - 93ms/epoch - 19ms/step\n",
            "Epoch 79/800\n",
            "5/5 - 0s - loss: 0.5526 - accuracy: 0.8186 - val_loss: 0.6351 - val_accuracy: 0.7930 - 119ms/epoch - 24ms/step\n",
            "Epoch 80/800\n",
            "5/5 - 0s - loss: 0.5984 - accuracy: 0.8098 - val_loss: 0.7258 - val_accuracy: 0.7614 - 101ms/epoch - 20ms/step\n",
            "Epoch 81/800\n",
            "5/5 - 0s - loss: 0.7528 - accuracy: 0.7590 - val_loss: 0.6694 - val_accuracy: 0.7930 - 95ms/epoch - 19ms/step\n",
            "Epoch 82/800\n",
            "5/5 - 0s - loss: 0.6125 - accuracy: 0.8238 - val_loss: 0.6334 - val_accuracy: 0.7930 - 116ms/epoch - 23ms/step\n",
            "Epoch 83/800\n",
            "5/5 - 0s - loss: 0.5635 - accuracy: 0.8230 - val_loss: 0.6739 - val_accuracy: 0.7930 - 115ms/epoch - 23ms/step\n",
            "Epoch 84/800\n",
            "5/5 - 0s - loss: 0.5744 - accuracy: 0.8230 - val_loss: 0.7094 - val_accuracy: 0.7930 - 101ms/epoch - 20ms/step\n",
            "Epoch 85/800\n",
            "5/5 - 0s - loss: 0.5873 - accuracy: 0.8238 - val_loss: 0.7177 - val_accuracy: 0.7930 - 100ms/epoch - 20ms/step\n",
            "Epoch 86/800\n",
            "5/5 - 0s - loss: 0.5910 - accuracy: 0.8230 - val_loss: 0.6918 - val_accuracy: 0.7930 - 95ms/epoch - 19ms/step\n",
            "Epoch 87/800\n",
            "5/5 - 0s - loss: 0.5784 - accuracy: 0.8221 - val_loss: 0.6685 - val_accuracy: 0.7930 - 100ms/epoch - 20ms/step\n",
            "Epoch 88/800\n",
            "5/5 - 0s - loss: 0.5619 - accuracy: 0.8238 - val_loss: 0.6563 - val_accuracy: 0.7930 - 115ms/epoch - 23ms/step\n",
            "Epoch 89/800\n",
            "5/5 - 0s - loss: 0.5588 - accuracy: 0.8230 - val_loss: 0.6584 - val_accuracy: 0.7930 - 97ms/epoch - 19ms/step\n",
            "Epoch 90/800\n",
            "5/5 - 0s - loss: 0.5615 - accuracy: 0.8238 - val_loss: 0.6663 - val_accuracy: 0.7930 - 92ms/epoch - 18ms/step\n",
            "Epoch 91/800\n",
            "5/5 - 0s - loss: 0.5613 - accuracy: 0.8230 - val_loss: 0.6691 - val_accuracy: 0.7930 - 116ms/epoch - 23ms/step\n",
            "Epoch 92/800\n",
            "5/5 - 0s - loss: 0.5546 - accuracy: 0.8230 - val_loss: 0.6627 - val_accuracy: 0.7930 - 106ms/epoch - 21ms/step\n",
            "Epoch 93/800\n",
            "5/5 - 0s - loss: 0.5499 - accuracy: 0.8230 - val_loss: 0.6572 - val_accuracy: 0.7930 - 106ms/epoch - 21ms/step\n",
            "Epoch 94/800\n",
            "5/5 - 0s - loss: 0.5449 - accuracy: 0.8238 - val_loss: 0.6510 - val_accuracy: 0.7930 - 102ms/epoch - 20ms/step\n",
            "Epoch 95/800\n",
            "5/5 - 0s - loss: 0.5444 - accuracy: 0.8238 - val_loss: 0.6515 - val_accuracy: 0.7930 - 96ms/epoch - 19ms/step\n",
            "Epoch 96/800\n",
            "5/5 - 0s - loss: 0.5504 - accuracy: 0.8221 - val_loss: 0.6556 - val_accuracy: 0.7930 - 109ms/epoch - 22ms/step\n",
            "Epoch 97/800\n",
            "5/5 - 0s - loss: 0.5353 - accuracy: 0.8212 - val_loss: 0.6604 - val_accuracy: 0.7930 - 97ms/epoch - 19ms/step\n",
            "Epoch 98/800\n",
            "5/5 - 0s - loss: 0.5416 - accuracy: 0.8238 - val_loss: 0.6366 - val_accuracy: 0.7930 - 101ms/epoch - 20ms/step\n",
            "Epoch 99/800\n",
            "5/5 - 0s - loss: 0.5398 - accuracy: 0.8256 - val_loss: 0.6703 - val_accuracy: 0.7754 - 97ms/epoch - 19ms/step\n",
            "Epoch 100/800\n",
            "5/5 - 0s - loss: 0.7165 - accuracy: 0.7502 - val_loss: 0.7033 - val_accuracy: 0.7719 - 181ms/epoch - 36ms/step\n",
            "Epoch 101/800\n",
            "5/5 - 0s - loss: 0.6361 - accuracy: 0.8063 - val_loss: 0.6199 - val_accuracy: 0.7930 - 221ms/epoch - 44ms/step\n",
            "Epoch 102/800\n",
            "5/5 - 0s - loss: 0.5433 - accuracy: 0.8212 - val_loss: 0.6548 - val_accuracy: 0.7930 - 204ms/epoch - 41ms/step\n",
            "Epoch 103/800\n",
            "5/5 - 0s - loss: 0.5560 - accuracy: 0.8230 - val_loss: 0.6930 - val_accuracy: 0.7930 - 211ms/epoch - 42ms/step\n",
            "Epoch 104/800\n",
            "5/5 - 0s - loss: 0.5749 - accuracy: 0.8230 - val_loss: 0.6736 - val_accuracy: 0.7930 - 206ms/epoch - 41ms/step\n",
            "Epoch 105/800\n",
            "5/5 - 0s - loss: 0.5476 - accuracy: 0.8221 - val_loss: 0.6251 - val_accuracy: 0.7930 - 183ms/epoch - 37ms/step\n",
            "Epoch 106/800\n",
            "5/5 - 0s - loss: 0.5527 - accuracy: 0.8238 - val_loss: 0.6237 - val_accuracy: 0.7930 - 203ms/epoch - 41ms/step\n",
            "Epoch 107/800\n",
            "5/5 - 0s - loss: 0.5483 - accuracy: 0.8230 - val_loss: 0.6386 - val_accuracy: 0.7930 - 215ms/epoch - 43ms/step\n",
            "Epoch 108/800\n",
            "5/5 - 0s - loss: 0.5440 - accuracy: 0.8230 - val_loss: 0.6385 - val_accuracy: 0.7930 - 228ms/epoch - 46ms/step\n",
            "Epoch 109/800\n",
            "5/5 - 0s - loss: 0.5475 - accuracy: 0.8230 - val_loss: 0.6189 - val_accuracy: 0.7930 - 224ms/epoch - 45ms/step\n",
            "Epoch 110/800\n",
            "5/5 - 0s - loss: 0.5619 - accuracy: 0.8238 - val_loss: 0.6249 - val_accuracy: 0.7930 - 233ms/epoch - 47ms/step\n",
            "Epoch 111/800\n",
            "5/5 - 0s - loss: 0.5568 - accuracy: 0.8238 - val_loss: 0.6217 - val_accuracy: 0.7930 - 207ms/epoch - 41ms/step\n",
            "Epoch 112/800\n",
            "5/5 - 0s - loss: 0.5398 - accuracy: 0.8230 - val_loss: 0.6314 - val_accuracy: 0.7930 - 210ms/epoch - 42ms/step\n",
            "Epoch 113/800\n",
            "5/5 - 0s - loss: 0.5368 - accuracy: 0.8230 - val_loss: 0.6510 - val_accuracy: 0.7930 - 220ms/epoch - 44ms/step\n",
            "Epoch 114/800\n",
            "5/5 - 0s - loss: 0.5461 - accuracy: 0.8230 - val_loss: 0.6389 - val_accuracy: 0.7930 - 189ms/epoch - 38ms/step\n",
            "Epoch 115/800\n",
            "5/5 - 0s - loss: 0.5318 - accuracy: 0.8238 - val_loss: 0.6189 - val_accuracy: 0.7930 - 209ms/epoch - 42ms/step\n",
            "Epoch 116/800\n",
            "5/5 - 0s - loss: 0.5542 - accuracy: 0.8247 - val_loss: 0.6209 - val_accuracy: 0.7930 - 155ms/epoch - 31ms/step\n",
            "Epoch 117/800\n",
            "5/5 - 0s - loss: 0.5449 - accuracy: 0.8238 - val_loss: 0.6085 - val_accuracy: 0.7930 - 125ms/epoch - 25ms/step\n",
            "Epoch 118/800\n",
            "5/5 - 0s - loss: 0.5232 - accuracy: 0.8247 - val_loss: 0.6171 - val_accuracy: 0.7930 - 98ms/epoch - 20ms/step\n",
            "Epoch 119/800\n",
            "5/5 - 0s - loss: 0.5226 - accuracy: 0.8256 - val_loss: 0.6365 - val_accuracy: 0.7930 - 194ms/epoch - 39ms/step\n",
            "Epoch 120/800\n",
            "5/5 - 0s - loss: 0.5236 - accuracy: 0.8247 - val_loss: 0.6502 - val_accuracy: 0.7930 - 319ms/epoch - 64ms/step\n",
            "Epoch 121/800\n",
            "5/5 - 0s - loss: 0.5220 - accuracy: 0.8230 - val_loss: 0.6520 - val_accuracy: 0.7930 - 173ms/epoch - 35ms/step\n",
            "Epoch 122/800\n",
            "5/5 - 0s - loss: 0.5303 - accuracy: 0.8230 - val_loss: 0.6287 - val_accuracy: 0.7930 - 104ms/epoch - 21ms/step\n",
            "Epoch 123/800\n",
            "5/5 - 0s - loss: 0.5151 - accuracy: 0.8256 - val_loss: 0.5966 - val_accuracy: 0.7930 - 127ms/epoch - 25ms/step\n",
            "Epoch 124/800\n",
            "5/5 - 0s - loss: 0.5341 - accuracy: 0.8238 - val_loss: 0.5954 - val_accuracy: 0.7895 - 140ms/epoch - 28ms/step\n",
            "Epoch 125/800\n",
            "5/5 - 0s - loss: 0.5555 - accuracy: 0.8160 - val_loss: 0.5905 - val_accuracy: 0.7895 - 121ms/epoch - 24ms/step\n",
            "Epoch 126/800\n",
            "5/5 - 0s - loss: 0.5354 - accuracy: 0.8256 - val_loss: 0.5925 - val_accuracy: 0.7930 - 109ms/epoch - 22ms/step\n",
            "Epoch 127/800\n",
            "5/5 - 0s - loss: 0.5160 - accuracy: 0.8247 - val_loss: 0.6324 - val_accuracy: 0.7930 - 93ms/epoch - 19ms/step\n",
            "Epoch 128/800\n",
            "5/5 - 0s - loss: 0.5341 - accuracy: 0.8230 - val_loss: 0.6623 - val_accuracy: 0.7930 - 94ms/epoch - 19ms/step\n",
            "Epoch 129/800\n",
            "5/5 - 0s - loss: 0.5313 - accuracy: 0.8256 - val_loss: 0.6699 - val_accuracy: 0.7930 - 116ms/epoch - 23ms/step\n",
            "Epoch 130/800\n",
            "5/5 - 0s - loss: 0.5338 - accuracy: 0.8221 - val_loss: 0.6589 - val_accuracy: 0.7930 - 96ms/epoch - 19ms/step\n",
            "Epoch 131/800\n",
            "5/5 - 0s - loss: 0.5312 - accuracy: 0.8221 - val_loss: 0.6381 - val_accuracy: 0.7930 - 100ms/epoch - 20ms/step\n",
            "Epoch 132/800\n",
            "5/5 - 0s - loss: 0.5069 - accuracy: 0.8247 - val_loss: 0.6222 - val_accuracy: 0.7930 - 90ms/epoch - 18ms/step\n",
            "Epoch 133/800\n",
            "5/5 - 0s - loss: 0.4935 - accuracy: 0.8273 - val_loss: 0.6131 - val_accuracy: 0.7930 - 103ms/epoch - 21ms/step\n",
            "Epoch 134/800\n",
            "5/5 - 0s - loss: 0.4967 - accuracy: 0.8300 - val_loss: 0.6019 - val_accuracy: 0.7965 - 98ms/epoch - 20ms/step\n",
            "Epoch 135/800\n",
            "5/5 - 0s - loss: 0.5105 - accuracy: 0.8370 - val_loss: 0.5938 - val_accuracy: 0.8000 - 92ms/epoch - 18ms/step\n",
            "Epoch 136/800\n",
            "5/5 - 0s - loss: 0.5307 - accuracy: 0.8291 - val_loss: 0.5840 - val_accuracy: 0.7895 - 139ms/epoch - 28ms/step\n",
            "Epoch 137/800\n",
            "5/5 - 0s - loss: 0.4985 - accuracy: 0.8221 - val_loss: 0.6062 - val_accuracy: 0.7930 - 103ms/epoch - 21ms/step\n",
            "Epoch 138/800\n",
            "5/5 - 0s - loss: 0.4939 - accuracy: 0.8282 - val_loss: 0.6156 - val_accuracy: 0.7930 - 115ms/epoch - 23ms/step\n",
            "Epoch 139/800\n",
            "5/5 - 0s - loss: 0.4903 - accuracy: 0.8273 - val_loss: 0.5934 - val_accuracy: 0.7965 - 110ms/epoch - 22ms/step\n",
            "Epoch 140/800\n",
            "5/5 - 0s - loss: 0.4952 - accuracy: 0.8317 - val_loss: 0.5962 - val_accuracy: 0.7860 - 102ms/epoch - 20ms/step\n",
            "Epoch 141/800\n",
            "5/5 - 0s - loss: 0.5334 - accuracy: 0.8335 - val_loss: 0.5936 - val_accuracy: 0.8000 - 94ms/epoch - 19ms/step\n",
            "Epoch 142/800\n",
            "5/5 - 0s - loss: 0.5354 - accuracy: 0.8221 - val_loss: 0.5775 - val_accuracy: 0.7860 - 123ms/epoch - 25ms/step\n",
            "Epoch 143/800\n",
            "5/5 - 0s - loss: 0.5062 - accuracy: 0.8212 - val_loss: 0.5993 - val_accuracy: 0.7930 - 101ms/epoch - 20ms/step\n",
            "Epoch 144/800\n",
            "5/5 - 0s - loss: 0.4899 - accuracy: 0.8326 - val_loss: 0.6253 - val_accuracy: 0.7930 - 113ms/epoch - 23ms/step\n",
            "Epoch 145/800\n",
            "5/5 - 0s - loss: 0.5097 - accuracy: 0.8256 - val_loss: 0.6360 - val_accuracy: 0.7930 - 104ms/epoch - 21ms/step\n",
            "Epoch 146/800\n",
            "5/5 - 0s - loss: 0.5107 - accuracy: 0.8256 - val_loss: 0.6325 - val_accuracy: 0.7930 - 95ms/epoch - 19ms/step\n",
            "Epoch 147/800\n",
            "5/5 - 0s - loss: 0.4979 - accuracy: 0.8273 - val_loss: 0.6189 - val_accuracy: 0.7930 - 128ms/epoch - 26ms/step\n",
            "Epoch 148/800\n",
            "5/5 - 0s - loss: 0.4910 - accuracy: 0.8265 - val_loss: 0.5919 - val_accuracy: 0.7895 - 95ms/epoch - 19ms/step\n",
            "Epoch 149/800\n",
            "5/5 - 0s - loss: 0.4973 - accuracy: 0.8265 - val_loss: 0.5944 - val_accuracy: 0.8000 - 102ms/epoch - 20ms/step\n",
            "Epoch 150/800\n",
            "5/5 - 0s - loss: 0.5415 - accuracy: 0.8195 - val_loss: 0.5656 - val_accuracy: 0.8000 - 115ms/epoch - 23ms/step\n",
            "Epoch 151/800\n",
            "5/5 - 0s - loss: 0.4886 - accuracy: 0.8230 - val_loss: 0.5634 - val_accuracy: 0.7965 - 125ms/epoch - 25ms/step\n",
            "Epoch 152/800\n",
            "5/5 - 0s - loss: 0.4836 - accuracy: 0.8300 - val_loss: 0.5570 - val_accuracy: 0.8000 - 121ms/epoch - 24ms/step\n",
            "Epoch 153/800\n",
            "5/5 - 0s - loss: 0.4828 - accuracy: 0.8379 - val_loss: 0.5572 - val_accuracy: 0.8000 - 112ms/epoch - 22ms/step\n",
            "Epoch 154/800\n",
            "5/5 - 0s - loss: 0.4770 - accuracy: 0.8326 - val_loss: 0.5675 - val_accuracy: 0.7930 - 97ms/epoch - 19ms/step\n",
            "Epoch 155/800\n",
            "5/5 - 0s - loss: 0.4745 - accuracy: 0.8344 - val_loss: 0.5843 - val_accuracy: 0.7930 - 111ms/epoch - 22ms/step\n",
            "Epoch 156/800\n",
            "5/5 - 0s - loss: 0.4777 - accuracy: 0.8238 - val_loss: 0.6003 - val_accuracy: 0.7860 - 107ms/epoch - 21ms/step\n",
            "Epoch 157/800\n",
            "5/5 - 0s - loss: 0.4842 - accuracy: 0.8265 - val_loss: 0.6053 - val_accuracy: 0.7860 - 96ms/epoch - 19ms/step\n",
            "Epoch 158/800\n",
            "5/5 - 0s - loss: 0.4801 - accuracy: 0.8326 - val_loss: 0.5915 - val_accuracy: 0.7930 - 101ms/epoch - 20ms/step\n",
            "Epoch 159/800\n",
            "5/5 - 0s - loss: 0.4645 - accuracy: 0.8317 - val_loss: 0.5733 - val_accuracy: 0.8000 - 118ms/epoch - 24ms/step\n",
            "Epoch 160/800\n",
            "5/5 - 0s - loss: 0.4582 - accuracy: 0.8326 - val_loss: 0.5690 - val_accuracy: 0.8035 - 101ms/epoch - 20ms/step\n",
            "Epoch 161/800\n",
            "5/5 - 0s - loss: 0.4575 - accuracy: 0.8335 - val_loss: 0.5718 - val_accuracy: 0.8000 - 102ms/epoch - 20ms/step\n",
            "Epoch 162/800\n",
            "5/5 - 0s - loss: 0.4512 - accuracy: 0.8370 - val_loss: 0.5808 - val_accuracy: 0.7930 - 99ms/epoch - 20ms/step\n",
            "Epoch 163/800\n",
            "5/5 - 0s - loss: 0.4567 - accuracy: 0.8387 - val_loss: 0.5956 - val_accuracy: 0.8000 - 110ms/epoch - 22ms/step\n",
            "Epoch 164/800\n",
            "5/5 - 0s - loss: 0.4812 - accuracy: 0.8291 - val_loss: 0.6103 - val_accuracy: 0.8000 - 113ms/epoch - 23ms/step\n",
            "Epoch 165/800\n",
            "5/5 - 0s - loss: 0.4680 - accuracy: 0.8414 - val_loss: 0.5857 - val_accuracy: 0.8000 - 117ms/epoch - 23ms/step\n",
            "Epoch 166/800\n",
            "5/5 - 0s - loss: 0.4820 - accuracy: 0.8396 - val_loss: 0.5943 - val_accuracy: 0.7965 - 111ms/epoch - 22ms/step\n",
            "Epoch 167/800\n",
            "5/5 - 0s - loss: 0.4828 - accuracy: 0.8396 - val_loss: 0.5936 - val_accuracy: 0.7965 - 94ms/epoch - 19ms/step\n",
            "Epoch 168/800\n",
            "5/5 - 0s - loss: 0.4806 - accuracy: 0.8370 - val_loss: 0.5906 - val_accuracy: 0.8000 - 101ms/epoch - 20ms/step\n",
            "Epoch 169/800\n",
            "5/5 - 0s - loss: 0.4798 - accuracy: 0.8361 - val_loss: 0.5978 - val_accuracy: 0.8000 - 125ms/epoch - 25ms/step\n",
            "Epoch 170/800\n",
            "5/5 - 0s - loss: 0.4811 - accuracy: 0.8291 - val_loss: 0.6028 - val_accuracy: 0.7965 - 124ms/epoch - 25ms/step\n",
            "Epoch 171/800\n",
            "5/5 - 0s - loss: 0.4807 - accuracy: 0.8405 - val_loss: 0.6361 - val_accuracy: 0.7860 - 103ms/epoch - 21ms/step\n",
            "Epoch 172/800\n",
            "5/5 - 0s - loss: 0.5050 - accuracy: 0.8370 - val_loss: 0.6361 - val_accuracy: 0.7860 - 92ms/epoch - 18ms/step\n",
            "Epoch 173/800\n",
            "5/5 - 0s - loss: 0.5075 - accuracy: 0.8344 - val_loss: 0.6154 - val_accuracy: 0.7895 - 102ms/epoch - 20ms/step\n",
            "Epoch 174/800\n",
            "5/5 - 0s - loss: 0.4866 - accuracy: 0.8387 - val_loss: 0.5927 - val_accuracy: 0.8000 - 108ms/epoch - 22ms/step\n",
            "Epoch 175/800\n",
            "5/5 - 0s - loss: 0.4818 - accuracy: 0.8344 - val_loss: 0.5786 - val_accuracy: 0.8000 - 101ms/epoch - 20ms/step\n",
            "Epoch 176/800\n",
            "5/5 - 0s - loss: 0.4907 - accuracy: 0.8414 - val_loss: 0.5662 - val_accuracy: 0.8035 - 109ms/epoch - 22ms/step\n",
            "Epoch 177/800\n",
            "5/5 - 0s - loss: 0.4823 - accuracy: 0.8466 - val_loss: 0.5528 - val_accuracy: 0.8035 - 122ms/epoch - 24ms/step\n",
            "Epoch 178/800\n",
            "5/5 - 0s - loss: 0.4809 - accuracy: 0.8335 - val_loss: 0.5480 - val_accuracy: 0.8175 - 122ms/epoch - 24ms/step\n",
            "Epoch 179/800\n",
            "5/5 - 0s - loss: 0.4878 - accuracy: 0.8326 - val_loss: 0.5500 - val_accuracy: 0.8035 - 98ms/epoch - 20ms/step\n",
            "Epoch 180/800\n",
            "5/5 - 0s - loss: 0.4752 - accuracy: 0.8370 - val_loss: 0.5659 - val_accuracy: 0.7895 - 95ms/epoch - 19ms/step\n",
            "Epoch 181/800\n",
            "5/5 - 0s - loss: 0.4715 - accuracy: 0.8387 - val_loss: 0.5825 - val_accuracy: 0.7895 - 98ms/epoch - 20ms/step\n",
            "Epoch 182/800\n",
            "5/5 - 0s - loss: 0.4782 - accuracy: 0.8361 - val_loss: 0.5861 - val_accuracy: 0.7895 - 134ms/epoch - 27ms/step\n",
            "Epoch 183/800\n",
            "5/5 - 0s - loss: 0.4790 - accuracy: 0.8335 - val_loss: 0.5783 - val_accuracy: 0.7860 - 144ms/epoch - 29ms/step\n",
            "Epoch 184/800\n",
            "5/5 - 0s - loss: 0.4662 - accuracy: 0.8361 - val_loss: 0.5693 - val_accuracy: 0.7930 - 102ms/epoch - 20ms/step\n",
            "Epoch 185/800\n",
            "5/5 - 0s - loss: 0.4635 - accuracy: 0.8396 - val_loss: 0.5650 - val_accuracy: 0.8035 - 101ms/epoch - 20ms/step\n",
            "Epoch 186/800\n",
            "5/5 - 0s - loss: 0.4564 - accuracy: 0.8405 - val_loss: 0.5655 - val_accuracy: 0.8000 - 109ms/epoch - 22ms/step\n",
            "Epoch 187/800\n",
            "5/5 - 0s - loss: 0.4640 - accuracy: 0.8326 - val_loss: 0.5735 - val_accuracy: 0.7860 - 117ms/epoch - 23ms/step\n",
            "Epoch 188/800\n",
            "5/5 - 0s - loss: 0.4631 - accuracy: 0.8352 - val_loss: 0.5757 - val_accuracy: 0.7825 - 101ms/epoch - 20ms/step\n",
            "Epoch 189/800\n",
            "5/5 - 0s - loss: 0.4673 - accuracy: 0.8370 - val_loss: 0.5708 - val_accuracy: 0.7930 - 95ms/epoch - 19ms/step\n",
            "Epoch 190/800\n",
            "5/5 - 0s - loss: 0.4603 - accuracy: 0.8379 - val_loss: 0.5668 - val_accuracy: 0.8035 - 105ms/epoch - 21ms/step\n",
            "Epoch 191/800\n",
            "5/5 - 0s - loss: 0.4650 - accuracy: 0.8317 - val_loss: 0.5633 - val_accuracy: 0.8035 - 100ms/epoch - 20ms/step\n",
            "Epoch 192/800\n",
            "5/5 - 0s - loss: 0.4496 - accuracy: 0.8370 - val_loss: 0.5610 - val_accuracy: 0.8000 - 108ms/epoch - 22ms/step\n",
            "Epoch 193/800\n",
            "5/5 - 0s - loss: 0.4623 - accuracy: 0.8335 - val_loss: 0.5612 - val_accuracy: 0.7965 - 100ms/epoch - 20ms/step\n",
            "Epoch 194/800\n",
            "5/5 - 0s - loss: 0.4426 - accuracy: 0.8387 - val_loss: 0.5607 - val_accuracy: 0.7965 - 99ms/epoch - 20ms/step\n",
            "Epoch 195/800\n",
            "5/5 - 0s - loss: 0.4399 - accuracy: 0.8475 - val_loss: 0.5592 - val_accuracy: 0.7965 - 102ms/epoch - 20ms/step\n",
            "Epoch 196/800\n",
            "5/5 - 0s - loss: 0.4569 - accuracy: 0.8414 - val_loss: 0.5589 - val_accuracy: 0.7965 - 97ms/epoch - 19ms/step\n",
            "Epoch 197/800\n",
            "5/5 - 0s - loss: 0.4480 - accuracy: 0.8396 - val_loss: 0.5595 - val_accuracy: 0.8000 - 97ms/epoch - 19ms/step\n",
            "Epoch 198/800\n",
            "5/5 - 0s - loss: 0.4499 - accuracy: 0.8466 - val_loss: 0.5603 - val_accuracy: 0.8000 - 99ms/epoch - 20ms/step\n",
            "Epoch 199/800\n",
            "5/5 - 0s - loss: 0.4544 - accuracy: 0.8414 - val_loss: 0.5645 - val_accuracy: 0.7965 - 95ms/epoch - 19ms/step\n",
            "Epoch 200/800\n",
            "5/5 - 0s - loss: 0.4445 - accuracy: 0.8484 - val_loss: 0.5624 - val_accuracy: 0.8070 - 95ms/epoch - 19ms/step\n",
            "Epoch 201/800\n",
            "5/5 - 0s - loss: 0.4917 - accuracy: 0.8282 - val_loss: 0.6607 - val_accuracy: 0.7684 - 122ms/epoch - 24ms/step\n",
            "Epoch 202/800\n",
            "5/5 - 0s - loss: 0.5460 - accuracy: 0.8116 - val_loss: 0.5777 - val_accuracy: 0.8105 - 180ms/epoch - 36ms/step\n",
            "Epoch 203/800\n",
            "5/5 - 0s - loss: 0.5073 - accuracy: 0.8195 - val_loss: 0.5467 - val_accuracy: 0.8035 - 205ms/epoch - 41ms/step\n",
            "Epoch 204/800\n",
            "5/5 - 0s - loss: 0.4853 - accuracy: 0.8344 - val_loss: 0.5353 - val_accuracy: 0.8175 - 227ms/epoch - 45ms/step\n",
            "Epoch 205/800\n",
            "5/5 - 0s - loss: 0.4670 - accuracy: 0.8326 - val_loss: 0.5507 - val_accuracy: 0.8035 - 213ms/epoch - 43ms/step\n",
            "Epoch 206/800\n",
            "5/5 - 0s - loss: 0.4556 - accuracy: 0.8352 - val_loss: 0.5507 - val_accuracy: 0.8070 - 232ms/epoch - 46ms/step\n",
            "Epoch 207/800\n",
            "5/5 - 0s - loss: 0.4612 - accuracy: 0.8466 - val_loss: 0.5661 - val_accuracy: 0.8000 - 181ms/epoch - 36ms/step\n",
            "Epoch 208/800\n",
            "5/5 - 0s - loss: 0.4602 - accuracy: 0.8317 - val_loss: 0.5723 - val_accuracy: 0.7930 - 210ms/epoch - 42ms/step\n",
            "Epoch 209/800\n",
            "5/5 - 0s - loss: 0.4743 - accuracy: 0.8405 - val_loss: 0.5617 - val_accuracy: 0.8035 - 221ms/epoch - 44ms/step\n",
            "Epoch 210/800\n",
            "5/5 - 0s - loss: 0.4458 - accuracy: 0.8493 - val_loss: 0.5458 - val_accuracy: 0.8105 - 200ms/epoch - 40ms/step\n",
            "Epoch 211/800\n",
            "5/5 - 0s - loss: 0.4772 - accuracy: 0.8326 - val_loss: 0.5790 - val_accuracy: 0.7860 - 220ms/epoch - 44ms/step\n",
            "Epoch 212/800\n",
            "5/5 - 0s - loss: 0.4844 - accuracy: 0.8335 - val_loss: 0.5348 - val_accuracy: 0.8105 - 237ms/epoch - 47ms/step\n",
            "Epoch 213/800\n",
            "5/5 - 0s - loss: 0.4556 - accuracy: 0.8475 - val_loss: 0.5511 - val_accuracy: 0.8035 - 216ms/epoch - 43ms/step\n",
            "Epoch 214/800\n",
            "5/5 - 0s - loss: 0.4706 - accuracy: 0.8335 - val_loss: 0.5658 - val_accuracy: 0.7895 - 207ms/epoch - 41ms/step\n",
            "Epoch 215/800\n",
            "5/5 - 0s - loss: 0.4874 - accuracy: 0.8265 - val_loss: 0.5631 - val_accuracy: 0.7895 - 203ms/epoch - 41ms/step\n",
            "Epoch 216/800\n",
            "5/5 - 0s - loss: 0.4747 - accuracy: 0.8387 - val_loss: 0.5509 - val_accuracy: 0.7930 - 174ms/epoch - 35ms/step\n",
            "Epoch 217/800\n",
            "5/5 - 0s - loss: 0.4620 - accuracy: 0.8335 - val_loss: 0.5420 - val_accuracy: 0.7965 - 232ms/epoch - 46ms/step\n",
            "Epoch 218/800\n",
            "5/5 - 0s - loss: 0.4570 - accuracy: 0.8387 - val_loss: 0.5367 - val_accuracy: 0.8035 - 124ms/epoch - 25ms/step\n",
            "Epoch 219/800\n",
            "5/5 - 0s - loss: 0.4526 - accuracy: 0.8379 - val_loss: 0.5383 - val_accuracy: 0.8070 - 97ms/epoch - 19ms/step\n",
            "Epoch 220/800\n",
            "5/5 - 0s - loss: 0.4544 - accuracy: 0.8414 - val_loss: 0.5436 - val_accuracy: 0.8070 - 101ms/epoch - 20ms/step\n",
            "Epoch 221/800\n",
            "5/5 - 0s - loss: 0.4632 - accuracy: 0.8431 - val_loss: 0.5459 - val_accuracy: 0.8070 - 113ms/epoch - 23ms/step\n",
            "Epoch 222/800\n",
            "5/5 - 0s - loss: 0.4379 - accuracy: 0.8431 - val_loss: 0.5476 - val_accuracy: 0.8070 - 99ms/epoch - 20ms/step\n",
            "Epoch 223/800\n",
            "5/5 - 0s - loss: 0.4636 - accuracy: 0.8344 - val_loss: 0.5474 - val_accuracy: 0.8105 - 96ms/epoch - 19ms/step\n",
            "Epoch 224/800\n",
            "5/5 - 0s - loss: 0.4663 - accuracy: 0.8361 - val_loss: 0.5619 - val_accuracy: 0.7860 - 104ms/epoch - 21ms/step\n",
            "Epoch 225/800\n",
            "5/5 - 0s - loss: 0.4815 - accuracy: 0.8326 - val_loss: 0.5480 - val_accuracy: 0.8035 - 104ms/epoch - 21ms/step\n",
            "Epoch 226/800\n",
            "5/5 - 0s - loss: 0.4524 - accuracy: 0.8370 - val_loss: 0.5824 - val_accuracy: 0.7895 - 103ms/epoch - 21ms/step\n",
            "Epoch 227/800\n",
            "5/5 - 0s - loss: 0.4821 - accuracy: 0.8387 - val_loss: 0.5968 - val_accuracy: 0.7860 - 112ms/epoch - 22ms/step\n",
            "Epoch 228/800\n",
            "5/5 - 0s - loss: 0.5039 - accuracy: 0.8247 - val_loss: 0.5817 - val_accuracy: 0.7860 - 94ms/epoch - 19ms/step\n",
            "Epoch 229/800\n",
            "5/5 - 0s - loss: 0.4866 - accuracy: 0.8300 - val_loss: 0.5602 - val_accuracy: 0.7930 - 94ms/epoch - 19ms/step\n",
            "Epoch 230/800\n",
            "5/5 - 0s - loss: 0.4658 - accuracy: 0.8300 - val_loss: 0.5480 - val_accuracy: 0.8070 - 98ms/epoch - 20ms/step\n",
            "Epoch 231/800\n",
            "5/5 - 0s - loss: 0.4527 - accuracy: 0.8405 - val_loss: 0.5414 - val_accuracy: 0.8105 - 109ms/epoch - 22ms/step\n",
            "Epoch 232/800\n",
            "5/5 - 0s - loss: 0.4566 - accuracy: 0.8256 - val_loss: 0.5368 - val_accuracy: 0.8105 - 96ms/epoch - 19ms/step\n",
            "Epoch 233/800\n",
            "5/5 - 0s - loss: 0.4432 - accuracy: 0.8414 - val_loss: 0.5364 - val_accuracy: 0.8000 - 115ms/epoch - 23ms/step\n",
            "Epoch 234/800\n",
            "5/5 - 0s - loss: 0.4424 - accuracy: 0.8387 - val_loss: 0.5392 - val_accuracy: 0.8000 - 96ms/epoch - 19ms/step\n",
            "Epoch 235/800\n",
            "5/5 - 0s - loss: 0.4445 - accuracy: 0.8282 - val_loss: 0.5410 - val_accuracy: 0.8000 - 100ms/epoch - 20ms/step\n",
            "Epoch 236/800\n",
            "5/5 - 0s - loss: 0.4373 - accuracy: 0.8466 - val_loss: 0.5446 - val_accuracy: 0.8000 - 100ms/epoch - 20ms/step\n",
            "Epoch 237/800\n",
            "5/5 - 0s - loss: 0.4467 - accuracy: 0.8431 - val_loss: 0.5481 - val_accuracy: 0.8000 - 95ms/epoch - 19ms/step\n",
            "Epoch 238/800\n",
            "5/5 - 0s - loss: 0.4411 - accuracy: 0.8475 - val_loss: 0.5465 - val_accuracy: 0.7965 - 96ms/epoch - 19ms/step\n",
            "Epoch 239/800\n",
            "5/5 - 0s - loss: 0.4366 - accuracy: 0.8422 - val_loss: 0.5453 - val_accuracy: 0.8000 - 117ms/epoch - 23ms/step\n",
            "Epoch 240/800\n",
            "5/5 - 0s - loss: 0.4430 - accuracy: 0.8414 - val_loss: 0.5479 - val_accuracy: 0.8070 - 98ms/epoch - 20ms/step\n",
            "Epoch 241/800\n",
            "5/5 - 0s - loss: 0.4568 - accuracy: 0.8352 - val_loss: 0.5748 - val_accuracy: 0.7754 - 112ms/epoch - 22ms/step\n",
            "Epoch 242/800\n",
            "5/5 - 0s - loss: 0.4737 - accuracy: 0.8370 - val_loss: 0.5629 - val_accuracy: 0.7860 - 99ms/epoch - 20ms/step\n",
            "Epoch 243/800\n",
            "5/5 - 0s - loss: 0.4562 - accuracy: 0.8440 - val_loss: 0.5787 - val_accuracy: 0.8035 - 131ms/epoch - 26ms/step\n",
            "Epoch 244/800\n",
            "5/5 - 0s - loss: 0.4706 - accuracy: 0.8300 - val_loss: 0.5836 - val_accuracy: 0.8035 - 100ms/epoch - 20ms/step\n",
            "Epoch 245/800\n",
            "5/5 - 0s - loss: 0.4634 - accuracy: 0.8352 - val_loss: 0.5715 - val_accuracy: 0.8000 - 117ms/epoch - 23ms/step\n",
            "Epoch 246/800\n",
            "5/5 - 0s - loss: 0.4516 - accuracy: 0.8449 - val_loss: 0.5719 - val_accuracy: 0.8070 - 119ms/epoch - 24ms/step\n",
            "Epoch 247/800\n",
            "5/5 - 0s - loss: 0.4552 - accuracy: 0.8387 - val_loss: 0.5631 - val_accuracy: 0.8070 - 99ms/epoch - 20ms/step\n",
            "Epoch 248/800\n",
            "5/5 - 0s - loss: 0.4491 - accuracy: 0.8431 - val_loss: 0.5404 - val_accuracy: 0.8070 - 94ms/epoch - 19ms/step\n",
            "Epoch 249/800\n",
            "5/5 - 0s - loss: 0.4497 - accuracy: 0.8379 - val_loss: 0.5377 - val_accuracy: 0.8035 - 94ms/epoch - 19ms/step\n",
            "Epoch 250/800\n",
            "5/5 - 0s - loss: 0.4753 - accuracy: 0.8379 - val_loss: 0.5313 - val_accuracy: 0.8140 - 140ms/epoch - 28ms/step\n",
            "Epoch 251/800\n",
            "5/5 - 0s - loss: 0.4537 - accuracy: 0.8379 - val_loss: 0.5261 - val_accuracy: 0.8105 - 129ms/epoch - 26ms/step\n",
            "Epoch 252/800\n",
            "5/5 - 0s - loss: 0.4392 - accuracy: 0.8484 - val_loss: 0.5332 - val_accuracy: 0.8070 - 97ms/epoch - 19ms/step\n",
            "Epoch 253/800\n",
            "5/5 - 0s - loss: 0.4500 - accuracy: 0.8396 - val_loss: 0.5492 - val_accuracy: 0.8140 - 97ms/epoch - 19ms/step\n",
            "Epoch 254/800\n",
            "5/5 - 0s - loss: 0.4820 - accuracy: 0.8344 - val_loss: 0.5366 - val_accuracy: 0.8070 - 116ms/epoch - 23ms/step\n",
            "Epoch 255/800\n",
            "5/5 - 0s - loss: 0.4597 - accuracy: 0.8379 - val_loss: 0.5307 - val_accuracy: 0.7965 - 99ms/epoch - 20ms/step\n",
            "Epoch 256/800\n",
            "5/5 - 0s - loss: 0.4453 - accuracy: 0.8326 - val_loss: 0.5519 - val_accuracy: 0.7895 - 102ms/epoch - 20ms/step\n",
            "Epoch 257/800\n",
            "5/5 - 0s - loss: 0.4493 - accuracy: 0.8431 - val_loss: 0.5448 - val_accuracy: 0.7860 - 96ms/epoch - 19ms/step\n",
            "Epoch 258/800\n",
            "5/5 - 0s - loss: 0.4452 - accuracy: 0.8414 - val_loss: 0.5348 - val_accuracy: 0.8070 - 97ms/epoch - 19ms/step\n",
            "Epoch 259/800\n",
            "5/5 - 0s - loss: 0.4603 - accuracy: 0.8440 - val_loss: 0.5412 - val_accuracy: 0.8105 - 113ms/epoch - 23ms/step\n",
            "Epoch 260/800\n",
            "5/5 - 0s - loss: 0.4717 - accuracy: 0.8370 - val_loss: 0.5307 - val_accuracy: 0.8000 - 96ms/epoch - 19ms/step\n",
            "Epoch 261/800\n",
            "5/5 - 0s - loss: 0.4691 - accuracy: 0.8379 - val_loss: 0.5309 - val_accuracy: 0.7825 - 109ms/epoch - 22ms/step\n",
            "Epoch 262/800\n",
            "5/5 - 0s - loss: 0.4422 - accuracy: 0.8326 - val_loss: 0.5365 - val_accuracy: 0.7895 - 109ms/epoch - 22ms/step\n",
            "Epoch 263/800\n",
            "5/5 - 0s - loss: 0.4378 - accuracy: 0.8414 - val_loss: 0.5269 - val_accuracy: 0.8000 - 106ms/epoch - 21ms/step\n",
            "Epoch 264/800\n",
            "5/5 - 0s - loss: 0.4562 - accuracy: 0.8344 - val_loss: 0.5436 - val_accuracy: 0.8035 - 109ms/epoch - 22ms/step\n",
            "Epoch 265/800\n",
            "5/5 - 0s - loss: 0.4868 - accuracy: 0.8282 - val_loss: 0.5445 - val_accuracy: 0.7930 - 107ms/epoch - 21ms/step\n",
            "Epoch 266/800\n",
            "5/5 - 0s - loss: 0.4626 - accuracy: 0.8379 - val_loss: 0.5397 - val_accuracy: 0.8105 - 106ms/epoch - 21ms/step\n",
            "Epoch 267/800\n",
            "5/5 - 0s - loss: 0.4391 - accuracy: 0.8370 - val_loss: 0.5617 - val_accuracy: 0.8070 - 98ms/epoch - 20ms/step\n",
            "Epoch 268/800\n",
            "5/5 - 0s - loss: 0.4460 - accuracy: 0.8405 - val_loss: 0.5876 - val_accuracy: 0.8000 - 97ms/epoch - 19ms/step\n",
            "Epoch 269/800\n",
            "5/5 - 0s - loss: 0.4900 - accuracy: 0.8335 - val_loss: 0.6565 - val_accuracy: 0.7649 - 96ms/epoch - 19ms/step\n",
            "Epoch 270/800\n",
            "5/5 - 0s - loss: 0.5367 - accuracy: 0.8151 - val_loss: 0.5595 - val_accuracy: 0.8035 - 116ms/epoch - 23ms/step\n",
            "Epoch 271/800\n",
            "5/5 - 0s - loss: 0.4526 - accuracy: 0.8431 - val_loss: 0.5520 - val_accuracy: 0.8070 - 101ms/epoch - 20ms/step\n",
            "Epoch 272/800\n",
            "5/5 - 0s - loss: 0.4595 - accuracy: 0.8361 - val_loss: 0.5768 - val_accuracy: 0.8105 - 106ms/epoch - 21ms/step\n",
            "Epoch 273/800\n",
            "5/5 - 0s - loss: 0.4597 - accuracy: 0.8405 - val_loss: 0.5791 - val_accuracy: 0.8070 - 98ms/epoch - 20ms/step\n",
            "Epoch 274/800\n",
            "5/5 - 0s - loss: 0.4578 - accuracy: 0.8370 - val_loss: 0.5665 - val_accuracy: 0.8035 - 103ms/epoch - 21ms/step\n",
            "Epoch 275/800\n",
            "5/5 - 0s - loss: 0.4394 - accuracy: 0.8422 - val_loss: 0.5537 - val_accuracy: 0.8000 - 121ms/epoch - 24ms/step\n",
            "Epoch 276/800\n",
            "5/5 - 0s - loss: 0.4434 - accuracy: 0.8484 - val_loss: 0.5492 - val_accuracy: 0.8035 - 99ms/epoch - 20ms/step\n",
            "Epoch 277/800\n",
            "5/5 - 0s - loss: 0.4454 - accuracy: 0.8449 - val_loss: 0.5463 - val_accuracy: 0.7965 - 107ms/epoch - 21ms/step\n",
            "Epoch 278/800\n",
            "5/5 - 0s - loss: 0.4537 - accuracy: 0.8387 - val_loss: 0.5418 - val_accuracy: 0.8070 - 95ms/epoch - 19ms/step\n",
            "Epoch 279/800\n",
            "5/5 - 0s - loss: 0.4757 - accuracy: 0.8273 - val_loss: 0.5707 - val_accuracy: 0.7965 - 121ms/epoch - 24ms/step\n",
            "Epoch 280/800\n",
            "5/5 - 0s - loss: 0.4927 - accuracy: 0.8168 - val_loss: 0.5412 - val_accuracy: 0.8140 - 96ms/epoch - 19ms/step\n",
            "Epoch 281/800\n",
            "5/5 - 0s - loss: 0.4606 - accuracy: 0.8361 - val_loss: 0.5261 - val_accuracy: 0.8140 - 138ms/epoch - 28ms/step\n",
            "Epoch 282/800\n",
            "5/5 - 0s - loss: 0.4254 - accuracy: 0.8493 - val_loss: 0.5423 - val_accuracy: 0.8105 - 104ms/epoch - 21ms/step\n",
            "Epoch 283/800\n",
            "5/5 - 0s - loss: 0.4375 - accuracy: 0.8431 - val_loss: 0.5525 - val_accuracy: 0.8105 - 98ms/epoch - 20ms/step\n",
            "Epoch 284/800\n",
            "5/5 - 0s - loss: 0.4378 - accuracy: 0.8440 - val_loss: 0.5543 - val_accuracy: 0.8000 - 104ms/epoch - 21ms/step\n",
            "Epoch 285/800\n",
            "5/5 - 0s - loss: 0.4315 - accuracy: 0.8484 - val_loss: 0.5595 - val_accuracy: 0.7895 - 105ms/epoch - 21ms/step\n",
            "Epoch 286/800\n",
            "5/5 - 0s - loss: 0.4500 - accuracy: 0.8317 - val_loss: 0.5561 - val_accuracy: 0.7930 - 104ms/epoch - 21ms/step\n",
            "Epoch 287/800\n",
            "5/5 - 0s - loss: 0.4422 - accuracy: 0.8396 - val_loss: 0.5464 - val_accuracy: 0.7930 - 96ms/epoch - 19ms/step\n",
            "Epoch 288/800\n",
            "5/5 - 0s - loss: 0.4330 - accuracy: 0.8396 - val_loss: 0.5368 - val_accuracy: 0.7965 - 101ms/epoch - 20ms/step\n",
            "Epoch 289/800\n",
            "5/5 - 0s - loss: 0.4391 - accuracy: 0.8440 - val_loss: 0.5313 - val_accuracy: 0.8105 - 99ms/epoch - 20ms/step\n",
            "Epoch 290/800\n",
            "5/5 - 0s - loss: 0.4252 - accuracy: 0.8449 - val_loss: 0.5299 - val_accuracy: 0.8105 - 97ms/epoch - 19ms/step\n",
            "Epoch 291/800\n",
            "5/5 - 0s - loss: 0.4290 - accuracy: 0.8466 - val_loss: 0.5334 - val_accuracy: 0.8105 - 100ms/epoch - 20ms/step\n",
            "Epoch 292/800\n",
            "5/5 - 0s - loss: 0.4294 - accuracy: 0.8440 - val_loss: 0.5423 - val_accuracy: 0.7965 - 102ms/epoch - 20ms/step\n",
            "Epoch 293/800\n",
            "5/5 - 0s - loss: 0.4442 - accuracy: 0.8431 - val_loss: 0.5570 - val_accuracy: 0.7930 - 107ms/epoch - 21ms/step\n",
            "Epoch 294/800\n",
            "5/5 - 0s - loss: 0.4543 - accuracy: 0.8396 - val_loss: 0.6101 - val_accuracy: 0.8000 - 95ms/epoch - 19ms/step\n",
            "Epoch 295/800\n",
            "5/5 - 0s - loss: 0.5010 - accuracy: 0.8300 - val_loss: 0.6100 - val_accuracy: 0.8035 - 92ms/epoch - 18ms/step\n",
            "Epoch 296/800\n",
            "5/5 - 0s - loss: 0.5112 - accuracy: 0.8256 - val_loss: 0.6101 - val_accuracy: 0.7754 - 100ms/epoch - 20ms/step\n",
            "Epoch 297/800\n",
            "5/5 - 0s - loss: 0.5054 - accuracy: 0.8212 - val_loss: 0.5530 - val_accuracy: 0.8035 - 99ms/epoch - 20ms/step\n",
            "Epoch 298/800\n",
            "5/5 - 0s - loss: 0.4516 - accuracy: 0.8344 - val_loss: 0.5478 - val_accuracy: 0.7895 - 108ms/epoch - 22ms/step\n",
            "Epoch 299/800\n",
            "5/5 - 0s - loss: 0.4476 - accuracy: 0.8379 - val_loss: 0.5780 - val_accuracy: 0.7895 - 96ms/epoch - 19ms/step\n",
            "Epoch 300/800\n",
            "5/5 - 0s - loss: 0.4667 - accuracy: 0.8440 - val_loss: 0.5843 - val_accuracy: 0.7930 - 107ms/epoch - 21ms/step\n",
            "Epoch 301/800\n",
            "5/5 - 0s - loss: 0.4738 - accuracy: 0.8379 - val_loss: 0.5681 - val_accuracy: 0.7930 - 98ms/epoch - 20ms/step\n",
            "Epoch 302/800\n",
            "5/5 - 0s - loss: 0.4501 - accuracy: 0.8405 - val_loss: 0.5488 - val_accuracy: 0.7860 - 100ms/epoch - 20ms/step\n",
            "Epoch 303/800\n",
            "5/5 - 0s - loss: 0.4562 - accuracy: 0.8405 - val_loss: 0.5355 - val_accuracy: 0.7895 - 98ms/epoch - 20ms/step\n",
            "Epoch 304/800\n",
            "5/5 - 0s - loss: 0.4454 - accuracy: 0.8414 - val_loss: 0.5297 - val_accuracy: 0.7965 - 96ms/epoch - 19ms/step\n",
            "Epoch 305/800\n",
            "5/5 - 0s - loss: 0.4335 - accuracy: 0.8379 - val_loss: 0.5291 - val_accuracy: 0.7965 - 95ms/epoch - 19ms/step\n",
            "Epoch 306/800\n",
            "5/5 - 0s - loss: 0.4321 - accuracy: 0.8484 - val_loss: 0.5308 - val_accuracy: 0.8035 - 92ms/epoch - 18ms/step\n",
            "Epoch 307/800\n",
            "5/5 - 0s - loss: 0.4340 - accuracy: 0.8396 - val_loss: 0.5364 - val_accuracy: 0.8000 - 106ms/epoch - 21ms/step\n",
            "Epoch 308/800\n",
            "5/5 - 0s - loss: 0.4245 - accuracy: 0.8528 - val_loss: 0.5439 - val_accuracy: 0.8000 - 111ms/epoch - 22ms/step\n",
            "Epoch 309/800\n",
            "5/5 - 0s - loss: 0.4320 - accuracy: 0.8457 - val_loss: 0.5661 - val_accuracy: 0.7965 - 125ms/epoch - 25ms/step\n",
            "Epoch 310/800\n",
            "5/5 - 0s - loss: 0.4456 - accuracy: 0.8457 - val_loss: 0.5686 - val_accuracy: 0.7965 - 197ms/epoch - 39ms/step\n",
            "Epoch 311/800\n",
            "5/5 - 0s - loss: 0.4374 - accuracy: 0.8431 - val_loss: 0.5602 - val_accuracy: 0.7965 - 208ms/epoch - 42ms/step\n",
            "Epoch 312/800\n",
            "5/5 - 0s - loss: 0.4476 - accuracy: 0.8414 - val_loss: 0.5766 - val_accuracy: 0.8000 - 192ms/epoch - 38ms/step\n",
            "Epoch 313/800\n",
            "5/5 - 0s - loss: 0.4573 - accuracy: 0.8352 - val_loss: 0.5807 - val_accuracy: 0.8070 - 213ms/epoch - 43ms/step\n",
            "Epoch 314/800\n",
            "5/5 - 0s - loss: 0.4530 - accuracy: 0.8309 - val_loss: 0.5668 - val_accuracy: 0.8105 - 208ms/epoch - 42ms/step\n",
            "Epoch 315/800\n",
            "5/5 - 0s - loss: 0.4693 - accuracy: 0.8326 - val_loss: 0.6028 - val_accuracy: 0.7860 - 217ms/epoch - 43ms/step\n",
            "Epoch 316/800\n",
            "5/5 - 0s - loss: 0.5270 - accuracy: 0.8177 - val_loss: 0.5697 - val_accuracy: 0.8105 - 186ms/epoch - 37ms/step\n",
            "Epoch 317/800\n",
            "5/5 - 0s - loss: 0.4606 - accuracy: 0.8431 - val_loss: 0.5278 - val_accuracy: 0.7965 - 182ms/epoch - 36ms/step\n",
            "Epoch 318/800\n",
            "5/5 - 0s - loss: 0.4588 - accuracy: 0.8352 - val_loss: 0.5422 - val_accuracy: 0.7895 - 199ms/epoch - 40ms/step\n",
            "Epoch 319/800\n",
            "5/5 - 0s - loss: 0.4858 - accuracy: 0.8247 - val_loss: 0.5493 - val_accuracy: 0.7930 - 169ms/epoch - 34ms/step\n",
            "Epoch 320/800\n",
            "5/5 - 0s - loss: 0.4810 - accuracy: 0.8238 - val_loss: 0.5401 - val_accuracy: 0.8070 - 204ms/epoch - 41ms/step\n",
            "Epoch 321/800\n",
            "5/5 - 0s - loss: 0.4650 - accuracy: 0.8352 - val_loss: 0.5455 - val_accuracy: 0.7965 - 219ms/epoch - 44ms/step\n",
            "Epoch 322/800\n",
            "5/5 - 0s - loss: 0.4444 - accuracy: 0.8422 - val_loss: 0.5573 - val_accuracy: 0.7895 - 229ms/epoch - 46ms/step\n",
            "Epoch 323/800\n",
            "5/5 - 0s - loss: 0.4489 - accuracy: 0.8431 - val_loss: 0.5621 - val_accuracy: 0.7895 - 201ms/epoch - 40ms/step\n",
            "Epoch 324/800\n",
            "5/5 - 0s - loss: 0.4527 - accuracy: 0.8379 - val_loss: 0.5507 - val_accuracy: 0.7825 - 211ms/epoch - 42ms/step\n",
            "Epoch 325/800\n",
            "5/5 - 0s - loss: 0.4479 - accuracy: 0.8449 - val_loss: 0.5586 - val_accuracy: 0.8140 - 213ms/epoch - 43ms/step\n",
            "Epoch 326/800\n",
            "5/5 - 0s - loss: 0.4809 - accuracy: 0.8440 - val_loss: 0.5535 - val_accuracy: 0.8211 - 116ms/epoch - 23ms/step\n",
            "Epoch 327/800\n",
            "5/5 - 0s - loss: 0.4962 - accuracy: 0.8282 - val_loss: 0.5329 - val_accuracy: 0.8175 - 108ms/epoch - 22ms/step\n",
            "Epoch 328/800\n",
            "5/5 - 0s - loss: 0.4485 - accuracy: 0.8414 - val_loss: 0.5267 - val_accuracy: 0.8000 - 95ms/epoch - 19ms/step\n",
            "Epoch 329/800\n",
            "5/5 - 0s - loss: 0.4321 - accuracy: 0.8510 - val_loss: 0.5404 - val_accuracy: 0.7825 - 97ms/epoch - 19ms/step\n",
            "Epoch 330/800\n",
            "5/5 - 0s - loss: 0.4483 - accuracy: 0.8387 - val_loss: 0.5686 - val_accuracy: 0.7895 - 102ms/epoch - 20ms/step\n",
            "Epoch 331/800\n",
            "5/5 - 0s - loss: 0.4477 - accuracy: 0.8405 - val_loss: 0.5711 - val_accuracy: 0.7965 - 106ms/epoch - 21ms/step\n",
            "Epoch 332/800\n",
            "5/5 - 0s - loss: 0.4566 - accuracy: 0.8414 - val_loss: 0.5585 - val_accuracy: 0.8140 - 99ms/epoch - 20ms/step\n",
            "Epoch 333/800\n",
            "5/5 - 0s - loss: 0.4615 - accuracy: 0.8457 - val_loss: 0.5545 - val_accuracy: 0.8140 - 103ms/epoch - 21ms/step\n",
            "Epoch 334/800\n",
            "5/5 - 0s - loss: 0.4719 - accuracy: 0.8238 - val_loss: 0.5397 - val_accuracy: 0.8140 - 102ms/epoch - 20ms/step\n",
            "Epoch 335/800\n",
            "5/5 - 0s - loss: 0.4378 - accuracy: 0.8414 - val_loss: 0.5402 - val_accuracy: 0.7965 - 96ms/epoch - 19ms/step\n",
            "Epoch 336/800\n",
            "5/5 - 0s - loss: 0.4377 - accuracy: 0.8440 - val_loss: 0.5442 - val_accuracy: 0.8035 - 95ms/epoch - 19ms/step\n",
            "Epoch 337/800\n",
            "5/5 - 0s - loss: 0.4407 - accuracy: 0.8370 - val_loss: 0.5326 - val_accuracy: 0.8175 - 98ms/epoch - 20ms/step\n",
            "Epoch 338/800\n",
            "5/5 - 0s - loss: 0.4453 - accuracy: 0.8370 - val_loss: 0.5486 - val_accuracy: 0.7895 - 104ms/epoch - 21ms/step\n",
            "Epoch 339/800\n",
            "5/5 - 0s - loss: 0.4670 - accuracy: 0.8291 - val_loss: 0.5423 - val_accuracy: 0.8000 - 104ms/epoch - 21ms/step\n",
            "Epoch 340/800\n",
            "5/5 - 0s - loss: 0.4586 - accuracy: 0.8326 - val_loss: 0.5339 - val_accuracy: 0.8175 - 103ms/epoch - 21ms/step\n",
            "Epoch 341/800\n",
            "5/5 - 0s - loss: 0.4412 - accuracy: 0.8361 - val_loss: 0.5429 - val_accuracy: 0.8175 - 100ms/epoch - 20ms/step\n",
            "Epoch 342/800\n",
            "5/5 - 0s - loss: 0.4285 - accuracy: 0.8528 - val_loss: 0.5554 - val_accuracy: 0.8070 - 102ms/epoch - 20ms/step\n",
            "Epoch 343/800\n",
            "5/5 - 0s - loss: 0.4371 - accuracy: 0.8449 - val_loss: 0.5574 - val_accuracy: 0.8070 - 95ms/epoch - 19ms/step\n",
            "Epoch 344/800\n",
            "5/5 - 0s - loss: 0.4310 - accuracy: 0.8484 - val_loss: 0.5467 - val_accuracy: 0.8175 - 103ms/epoch - 21ms/step\n",
            "Epoch 345/800\n",
            "5/5 - 0s - loss: 0.4287 - accuracy: 0.8580 - val_loss: 0.5625 - val_accuracy: 0.8175 - 98ms/epoch - 20ms/step\n",
            "Epoch 346/800\n",
            "5/5 - 0s - loss: 0.4993 - accuracy: 0.8160 - val_loss: 0.6296 - val_accuracy: 0.7930 - 95ms/epoch - 19ms/step\n",
            "Epoch 347/800\n",
            "5/5 - 0s - loss: 0.5162 - accuracy: 0.8098 - val_loss: 0.5527 - val_accuracy: 0.8246 - 101ms/epoch - 20ms/step\n",
            "Epoch 348/800\n",
            "5/5 - 0s - loss: 0.4456 - accuracy: 0.8440 - val_loss: 0.5717 - val_accuracy: 0.8175 - 101ms/epoch - 20ms/step\n",
            "Epoch 349/800\n",
            "5/5 - 0s - loss: 0.4560 - accuracy: 0.8493 - val_loss: 0.6307 - val_accuracy: 0.8211 - 128ms/epoch - 26ms/step\n",
            "Epoch 350/800\n",
            "5/5 - 0s - loss: 0.4878 - accuracy: 0.8475 - val_loss: 0.6694 - val_accuracy: 0.8140 - 115ms/epoch - 23ms/step\n",
            "Epoch 351/800\n",
            "5/5 - 0s - loss: 0.4959 - accuracy: 0.8379 - val_loss: 0.6547 - val_accuracy: 0.8105 - 97ms/epoch - 19ms/step\n",
            "Epoch 352/800\n",
            "5/5 - 0s - loss: 0.4650 - accuracy: 0.8414 - val_loss: 0.5955 - val_accuracy: 0.8140 - 100ms/epoch - 20ms/step\n",
            "Epoch 353/800\n",
            "5/5 - 0s - loss: 0.4582 - accuracy: 0.8396 - val_loss: 0.5559 - val_accuracy: 0.8140 - 110ms/epoch - 22ms/step\n",
            "Epoch 354/800\n",
            "5/5 - 0s - loss: 0.4705 - accuracy: 0.8361 - val_loss: 0.5581 - val_accuracy: 0.7895 - 99ms/epoch - 20ms/step\n",
            "Epoch 355/800\n",
            "5/5 - 0s - loss: 0.4849 - accuracy: 0.8335 - val_loss: 0.5243 - val_accuracy: 0.8175 - 130ms/epoch - 26ms/step\n",
            "Epoch 356/800\n",
            "5/5 - 0s - loss: 0.4548 - accuracy: 0.8414 - val_loss: 0.5145 - val_accuracy: 0.8316 - 144ms/epoch - 29ms/step\n",
            "Epoch 357/800\n",
            "5/5 - 0s - loss: 0.4424 - accuracy: 0.8510 - val_loss: 0.5213 - val_accuracy: 0.8246 - 100ms/epoch - 20ms/step\n",
            "Epoch 358/800\n",
            "5/5 - 0s - loss: 0.4340 - accuracy: 0.8528 - val_loss: 0.5340 - val_accuracy: 0.8035 - 122ms/epoch - 24ms/step\n",
            "Epoch 359/800\n",
            "5/5 - 0s - loss: 0.4293 - accuracy: 0.8501 - val_loss: 0.5374 - val_accuracy: 0.8105 - 95ms/epoch - 19ms/step\n",
            "Epoch 360/800\n",
            "5/5 - 0s - loss: 0.4257 - accuracy: 0.8422 - val_loss: 0.5354 - val_accuracy: 0.8175 - 99ms/epoch - 20ms/step\n",
            "Epoch 361/800\n",
            "5/5 - 0s - loss: 0.4513 - accuracy: 0.8344 - val_loss: 0.5378 - val_accuracy: 0.8175 - 108ms/epoch - 22ms/step\n",
            "Epoch 362/800\n",
            "5/5 - 0s - loss: 0.4497 - accuracy: 0.8414 - val_loss: 0.5313 - val_accuracy: 0.8140 - 113ms/epoch - 23ms/step\n",
            "Epoch 363/800\n",
            "5/5 - 0s - loss: 0.4425 - accuracy: 0.8440 - val_loss: 0.5228 - val_accuracy: 0.8175 - 96ms/epoch - 19ms/step\n",
            "Epoch 364/800\n",
            "5/5 - 0s - loss: 0.4310 - accuracy: 0.8405 - val_loss: 0.5332 - val_accuracy: 0.8211 - 99ms/epoch - 20ms/step\n",
            "Epoch 365/800\n",
            "5/5 - 0s - loss: 0.4342 - accuracy: 0.8457 - val_loss: 0.5319 - val_accuracy: 0.8140 - 95ms/epoch - 19ms/step\n",
            "Epoch 366/800\n",
            "5/5 - 0s - loss: 0.4291 - accuracy: 0.8449 - val_loss: 0.5390 - val_accuracy: 0.8140 - 98ms/epoch - 20ms/step\n",
            "Epoch 367/800\n",
            "5/5 - 0s - loss: 0.4424 - accuracy: 0.8440 - val_loss: 0.5509 - val_accuracy: 0.8105 - 95ms/epoch - 19ms/step\n",
            "Epoch 368/800\n",
            "5/5 - 0s - loss: 0.4439 - accuracy: 0.8457 - val_loss: 0.5568 - val_accuracy: 0.8105 - 123ms/epoch - 25ms/step\n",
            "Epoch 369/800\n",
            "5/5 - 0s - loss: 0.4419 - accuracy: 0.8405 - val_loss: 0.5556 - val_accuracy: 0.8105 - 99ms/epoch - 20ms/step\n",
            "Epoch 370/800\n",
            "5/5 - 0s - loss: 0.4423 - accuracy: 0.8484 - val_loss: 0.5431 - val_accuracy: 0.8035 - 99ms/epoch - 20ms/step\n",
            "Epoch 371/800\n",
            "5/5 - 0s - loss: 0.4255 - accuracy: 0.8484 - val_loss: 0.5911 - val_accuracy: 0.7860 - 99ms/epoch - 20ms/step\n",
            "Epoch 372/800\n",
            "5/5 - 0s - loss: 0.5057 - accuracy: 0.8238 - val_loss: 0.6316 - val_accuracy: 0.7719 - 98ms/epoch - 20ms/step\n",
            "Epoch 373/800\n",
            "5/5 - 0s - loss: 0.4742 - accuracy: 0.8344 - val_loss: 0.5410 - val_accuracy: 0.8000 - 94ms/epoch - 19ms/step\n",
            "Epoch 374/800\n",
            "5/5 - 0s - loss: 0.4298 - accuracy: 0.8536 - val_loss: 0.5256 - val_accuracy: 0.8035 - 109ms/epoch - 22ms/step\n",
            "Epoch 375/800\n",
            "5/5 - 0s - loss: 0.4450 - accuracy: 0.8379 - val_loss: 0.5360 - val_accuracy: 0.8140 - 95ms/epoch - 19ms/step\n",
            "Epoch 376/800\n",
            "5/5 - 0s - loss: 0.4417 - accuracy: 0.8536 - val_loss: 0.5412 - val_accuracy: 0.8070 - 95ms/epoch - 19ms/step\n",
            "Epoch 377/800\n",
            "5/5 - 0s - loss: 0.4301 - accuracy: 0.8536 - val_loss: 0.5455 - val_accuracy: 0.7965 - 102ms/epoch - 20ms/step\n",
            "Epoch 378/800\n",
            "5/5 - 0s - loss: 0.4261 - accuracy: 0.8422 - val_loss: 0.5534 - val_accuracy: 0.7860 - 100ms/epoch - 20ms/step\n",
            "Epoch 379/800\n",
            "5/5 - 0s - loss: 0.4206 - accuracy: 0.8475 - val_loss: 0.5602 - val_accuracy: 0.7895 - 95ms/epoch - 19ms/step\n",
            "Epoch 380/800\n",
            "5/5 - 0s - loss: 0.4327 - accuracy: 0.8510 - val_loss: 0.5575 - val_accuracy: 0.7895 - 94ms/epoch - 19ms/step\n",
            "Epoch 381/800\n",
            "5/5 - 0s - loss: 0.4341 - accuracy: 0.8405 - val_loss: 0.5395 - val_accuracy: 0.8000 - 102ms/epoch - 20ms/step\n",
            "Epoch 382/800\n",
            "5/5 - 0s - loss: 0.4158 - accuracy: 0.8545 - val_loss: 0.5235 - val_accuracy: 0.8140 - 101ms/epoch - 20ms/step\n",
            "Epoch 383/800\n",
            "5/5 - 0s - loss: 0.4557 - accuracy: 0.8396 - val_loss: 0.5220 - val_accuracy: 0.8175 - 94ms/epoch - 19ms/step\n",
            "Epoch 384/800\n",
            "5/5 - 0s - loss: 0.4487 - accuracy: 0.8379 - val_loss: 0.5738 - val_accuracy: 0.8070 - 102ms/epoch - 20ms/step\n",
            "Epoch 385/800\n",
            "5/5 - 0s - loss: 0.4807 - accuracy: 0.8256 - val_loss: 0.6050 - val_accuracy: 0.7825 - 96ms/epoch - 19ms/step\n",
            "Epoch 386/800\n",
            "5/5 - 0s - loss: 0.4776 - accuracy: 0.8396 - val_loss: 0.5750 - val_accuracy: 0.8000 - 116ms/epoch - 23ms/step\n",
            "Epoch 387/800\n",
            "5/5 - 0s - loss: 0.4671 - accuracy: 0.8440 - val_loss: 0.5621 - val_accuracy: 0.7895 - 133ms/epoch - 27ms/step\n",
            "Epoch 388/800\n",
            "5/5 - 0s - loss: 0.4508 - accuracy: 0.8431 - val_loss: 0.5517 - val_accuracy: 0.7895 - 119ms/epoch - 24ms/step\n",
            "Epoch 389/800\n",
            "5/5 - 0s - loss: 0.4263 - accuracy: 0.8396 - val_loss: 0.5244 - val_accuracy: 0.8105 - 101ms/epoch - 20ms/step\n",
            "Epoch 390/800\n",
            "5/5 - 0s - loss: 0.4345 - accuracy: 0.8466 - val_loss: 0.5224 - val_accuracy: 0.8070 - 100ms/epoch - 20ms/step\n",
            "Epoch 391/800\n",
            "5/5 - 0s - loss: 0.4239 - accuracy: 0.8501 - val_loss: 0.5258 - val_accuracy: 0.8000 - 96ms/epoch - 19ms/step\n",
            "Epoch 392/800\n",
            "5/5 - 0s - loss: 0.4240 - accuracy: 0.8501 - val_loss: 0.5311 - val_accuracy: 0.7965 - 97ms/epoch - 19ms/step\n",
            "Epoch 393/800\n",
            "5/5 - 0s - loss: 0.4112 - accuracy: 0.8528 - val_loss: 0.5359 - val_accuracy: 0.7930 - 100ms/epoch - 20ms/step\n",
            "Epoch 394/800\n",
            "5/5 - 0s - loss: 0.4158 - accuracy: 0.8466 - val_loss: 0.5383 - val_accuracy: 0.7895 - 114ms/epoch - 23ms/step\n",
            "Epoch 395/800\n",
            "5/5 - 0s - loss: 0.4367 - accuracy: 0.8519 - val_loss: 0.5401 - val_accuracy: 0.7930 - 110ms/epoch - 22ms/step\n",
            "Epoch 396/800\n",
            "5/5 - 0s - loss: 0.4345 - accuracy: 0.8422 - val_loss: 0.5412 - val_accuracy: 0.7930 - 112ms/epoch - 22ms/step\n",
            "Epoch 397/800\n",
            "5/5 - 0s - loss: 0.4171 - accuracy: 0.8545 - val_loss: 0.5431 - val_accuracy: 0.7930 - 119ms/epoch - 24ms/step\n",
            "Epoch 398/800\n",
            "5/5 - 0s - loss: 0.4169 - accuracy: 0.8554 - val_loss: 0.5383 - val_accuracy: 0.8000 - 100ms/epoch - 20ms/step\n",
            "Epoch 399/800\n",
            "5/5 - 0s - loss: 0.4339 - accuracy: 0.8475 - val_loss: 0.5498 - val_accuracy: 0.8000 - 98ms/epoch - 20ms/step\n",
            "Epoch 400/800\n",
            "5/5 - 0s - loss: 0.4772 - accuracy: 0.8309 - val_loss: 0.5289 - val_accuracy: 0.8105 - 99ms/epoch - 20ms/step\n",
            "Epoch 401/800\n",
            "5/5 - 0s - loss: 0.4474 - accuracy: 0.8405 - val_loss: 0.5165 - val_accuracy: 0.8175 - 119ms/epoch - 24ms/step\n",
            "Epoch 402/800\n",
            "5/5 - 0s - loss: 0.4251 - accuracy: 0.8580 - val_loss: 0.5328 - val_accuracy: 0.8175 - 104ms/epoch - 21ms/step\n",
            "Epoch 403/800\n",
            "5/5 - 0s - loss: 0.4286 - accuracy: 0.8387 - val_loss: 0.5470 - val_accuracy: 0.8175 - 104ms/epoch - 21ms/step\n",
            "Epoch 404/800\n",
            "5/5 - 0s - loss: 0.4279 - accuracy: 0.8466 - val_loss: 0.5502 - val_accuracy: 0.8105 - 100ms/epoch - 20ms/step\n",
            "Epoch 405/800\n",
            "5/5 - 0s - loss: 0.4195 - accuracy: 0.8519 - val_loss: 0.5494 - val_accuracy: 0.8035 - 118ms/epoch - 24ms/step\n",
            "Epoch 406/800\n",
            "5/5 - 0s - loss: 0.4192 - accuracy: 0.8457 - val_loss: 0.5448 - val_accuracy: 0.8035 - 100ms/epoch - 20ms/step\n",
            "Epoch 407/800\n",
            "5/5 - 0s - loss: 0.4223 - accuracy: 0.8554 - val_loss: 0.5421 - val_accuracy: 0.7965 - 98ms/epoch - 20ms/step\n",
            "Epoch 408/800\n",
            "5/5 - 0s - loss: 0.4166 - accuracy: 0.8519 - val_loss: 0.5425 - val_accuracy: 0.8070 - 110ms/epoch - 22ms/step\n",
            "Epoch 409/800\n",
            "5/5 - 0s - loss: 0.4216 - accuracy: 0.8510 - val_loss: 0.5467 - val_accuracy: 0.8175 - 95ms/epoch - 19ms/step\n",
            "Epoch 410/800\n",
            "5/5 - 0s - loss: 0.4290 - accuracy: 0.8414 - val_loss: 0.5453 - val_accuracy: 0.8140 - 95ms/epoch - 19ms/step\n",
            "Epoch 411/800\n",
            "5/5 - 0s - loss: 0.4306 - accuracy: 0.8466 - val_loss: 0.5416 - val_accuracy: 0.8140 - 97ms/epoch - 19ms/step\n",
            "Epoch 412/800\n",
            "5/5 - 0s - loss: 0.4276 - accuracy: 0.8431 - val_loss: 0.5391 - val_accuracy: 0.8035 - 105ms/epoch - 21ms/step\n",
            "Epoch 413/800\n",
            "5/5 - 0s - loss: 0.4180 - accuracy: 0.8457 - val_loss: 0.5397 - val_accuracy: 0.8070 - 111ms/epoch - 22ms/step\n",
            "Epoch 414/800\n",
            "5/5 - 0s - loss: 0.4050 - accuracy: 0.8519 - val_loss: 0.5392 - val_accuracy: 0.8105 - 126ms/epoch - 25ms/step\n",
            "Epoch 415/800\n",
            "5/5 - 0s - loss: 0.4373 - accuracy: 0.8422 - val_loss: 0.6371 - val_accuracy: 0.7719 - 94ms/epoch - 19ms/step\n",
            "Epoch 416/800\n",
            "5/5 - 0s - loss: 0.5635 - accuracy: 0.7958 - val_loss: 0.6886 - val_accuracy: 0.7719 - 100ms/epoch - 20ms/step\n",
            "Epoch 417/800\n",
            "5/5 - 0s - loss: 0.5114 - accuracy: 0.8133 - val_loss: 0.5241 - val_accuracy: 0.8211 - 162ms/epoch - 32ms/step\n",
            "Epoch 418/800\n",
            "5/5 - 0s - loss: 0.4497 - accuracy: 0.8370 - val_loss: 0.5325 - val_accuracy: 0.8175 - 171ms/epoch - 34ms/step\n",
            "Epoch 419/800\n",
            "5/5 - 0s - loss: 0.4558 - accuracy: 0.8370 - val_loss: 0.5629 - val_accuracy: 0.8175 - 217ms/epoch - 43ms/step\n",
            "Epoch 420/800\n",
            "5/5 - 0s - loss: 0.4556 - accuracy: 0.8396 - val_loss: 0.5902 - val_accuracy: 0.7965 - 223ms/epoch - 45ms/step\n",
            "Epoch 421/800\n",
            "5/5 - 0s - loss: 0.4672 - accuracy: 0.8440 - val_loss: 0.5872 - val_accuracy: 0.7895 - 201ms/epoch - 40ms/step\n",
            "Epoch 422/800\n",
            "5/5 - 0s - loss: 0.4332 - accuracy: 0.8440 - val_loss: 0.5361 - val_accuracy: 0.8105 - 230ms/epoch - 46ms/step\n",
            "Epoch 423/800\n",
            "5/5 - 0s - loss: 0.4264 - accuracy: 0.8457 - val_loss: 0.5174 - val_accuracy: 0.8175 - 200ms/epoch - 40ms/step\n",
            "Epoch 424/800\n",
            "5/5 - 0s - loss: 0.4248 - accuracy: 0.8519 - val_loss: 0.5101 - val_accuracy: 0.8140 - 457ms/epoch - 91ms/step\n",
            "Epoch 425/800\n",
            "5/5 - 0s - loss: 0.4325 - accuracy: 0.8422 - val_loss: 0.5430 - val_accuracy: 0.8070 - 230ms/epoch - 46ms/step\n",
            "Epoch 426/800\n",
            "5/5 - 0s - loss: 0.4546 - accuracy: 0.8457 - val_loss: 0.5793 - val_accuracy: 0.8105 - 226ms/epoch - 45ms/step\n",
            "Epoch 427/800\n",
            "5/5 - 0s - loss: 0.5048 - accuracy: 0.8370 - val_loss: 0.6198 - val_accuracy: 0.7860 - 196ms/epoch - 39ms/step\n",
            "Epoch 428/800\n",
            "5/5 - 0s - loss: 0.4940 - accuracy: 0.8396 - val_loss: 0.5420 - val_accuracy: 0.8140 - 193ms/epoch - 39ms/step\n",
            "Epoch 429/800\n",
            "5/5 - 0s - loss: 0.4694 - accuracy: 0.8387 - val_loss: 0.5279 - val_accuracy: 0.8000 - 205ms/epoch - 41ms/step\n",
            "Epoch 430/800\n",
            "5/5 - 0s - loss: 0.5021 - accuracy: 0.8291 - val_loss: 0.5282 - val_accuracy: 0.7965 - 212ms/epoch - 42ms/step\n",
            "Epoch 431/800\n",
            "5/5 - 0s - loss: 0.4869 - accuracy: 0.8309 - val_loss: 0.5127 - val_accuracy: 0.8281 - 208ms/epoch - 42ms/step\n",
            "Epoch 432/800\n",
            "5/5 - 0s - loss: 0.4572 - accuracy: 0.8493 - val_loss: 0.5238 - val_accuracy: 0.8105 - 164ms/epoch - 33ms/step\n",
            "Epoch 433/800\n",
            "5/5 - 0s - loss: 0.4493 - accuracy: 0.8475 - val_loss: 0.5402 - val_accuracy: 0.8000 - 106ms/epoch - 21ms/step\n",
            "Epoch 434/800\n",
            "5/5 - 0s - loss: 0.4391 - accuracy: 0.8449 - val_loss: 0.5477 - val_accuracy: 0.7965 - 95ms/epoch - 19ms/step\n",
            "Epoch 435/800\n",
            "5/5 - 0s - loss: 0.4538 - accuracy: 0.8387 - val_loss: 0.5355 - val_accuracy: 0.7930 - 108ms/epoch - 22ms/step\n",
            "Epoch 436/800\n",
            "5/5 - 0s - loss: 0.4263 - accuracy: 0.8475 - val_loss: 0.5138 - val_accuracy: 0.8211 - 101ms/epoch - 20ms/step\n",
            "Epoch 437/800\n",
            "5/5 - 0s - loss: 0.4428 - accuracy: 0.8422 - val_loss: 0.5162 - val_accuracy: 0.8175 - 97ms/epoch - 19ms/step\n",
            "Epoch 438/800\n",
            "5/5 - 0s - loss: 0.4476 - accuracy: 0.8431 - val_loss: 0.5141 - val_accuracy: 0.8175 - 92ms/epoch - 18ms/step\n",
            "Epoch 439/800\n",
            "5/5 - 0s - loss: 0.4347 - accuracy: 0.8484 - val_loss: 0.5137 - val_accuracy: 0.8211 - 105ms/epoch - 21ms/step\n",
            "Epoch 440/800\n",
            "5/5 - 0s - loss: 0.4178 - accuracy: 0.8510 - val_loss: 0.5268 - val_accuracy: 0.8070 - 107ms/epoch - 21ms/step\n",
            "Epoch 441/800\n",
            "5/5 - 0s - loss: 0.4245 - accuracy: 0.8501 - val_loss: 0.5310 - val_accuracy: 0.8070 - 115ms/epoch - 23ms/step\n",
            "Epoch 442/800\n",
            "5/5 - 0s - loss: 0.4204 - accuracy: 0.8501 - val_loss: 0.5197 - val_accuracy: 0.8105 - 104ms/epoch - 21ms/step\n",
            "Epoch 443/800\n",
            "5/5 - 0s - loss: 0.4593 - accuracy: 0.8300 - val_loss: 0.5510 - val_accuracy: 0.7965 - 97ms/epoch - 19ms/step\n",
            "Epoch 444/800\n",
            "5/5 - 0s - loss: 0.5182 - accuracy: 0.8151 - val_loss: 0.5332 - val_accuracy: 0.8070 - 107ms/epoch - 21ms/step\n",
            "Epoch 445/800\n",
            "5/5 - 0s - loss: 0.4752 - accuracy: 0.8379 - val_loss: 0.5152 - val_accuracy: 0.8211 - 104ms/epoch - 21ms/step\n",
            "Epoch 446/800\n",
            "5/5 - 0s - loss: 0.4513 - accuracy: 0.8361 - val_loss: 0.5372 - val_accuracy: 0.8246 - 96ms/epoch - 19ms/step\n",
            "Epoch 447/800\n",
            "5/5 - 0s - loss: 0.4446 - accuracy: 0.8414 - val_loss: 0.5366 - val_accuracy: 0.8175 - 104ms/epoch - 21ms/step\n",
            "Epoch 448/800\n",
            "5/5 - 0s - loss: 0.4238 - accuracy: 0.8501 - val_loss: 0.5319 - val_accuracy: 0.8175 - 102ms/epoch - 20ms/step\n",
            "Epoch 449/800\n",
            "5/5 - 0s - loss: 0.4125 - accuracy: 0.8519 - val_loss: 0.5472 - val_accuracy: 0.8000 - 114ms/epoch - 23ms/step\n",
            "Epoch 450/800\n",
            "5/5 - 0s - loss: 0.4357 - accuracy: 0.8493 - val_loss: 0.5789 - val_accuracy: 0.7965 - 97ms/epoch - 19ms/step\n",
            "Epoch 451/800\n",
            "5/5 - 0s - loss: 0.4503 - accuracy: 0.8457 - val_loss: 0.5837 - val_accuracy: 0.7965 - 109ms/epoch - 22ms/step\n",
            "Epoch 452/800\n",
            "5/5 - 0s - loss: 0.4509 - accuracy: 0.8422 - val_loss: 0.5708 - val_accuracy: 0.7895 - 94ms/epoch - 19ms/step\n",
            "Epoch 453/800\n",
            "5/5 - 0s - loss: 0.4502 - accuracy: 0.8493 - val_loss: 0.5536 - val_accuracy: 0.7930 - 91ms/epoch - 18ms/step\n",
            "Epoch 454/800\n",
            "5/5 - 0s - loss: 0.4484 - accuracy: 0.8449 - val_loss: 0.5350 - val_accuracy: 0.7965 - 115ms/epoch - 23ms/step\n",
            "Epoch 455/800\n",
            "5/5 - 0s - loss: 0.4384 - accuracy: 0.8519 - val_loss: 0.5246 - val_accuracy: 0.8105 - 95ms/epoch - 19ms/step\n",
            "Epoch 456/800\n",
            "5/5 - 0s - loss: 0.4180 - accuracy: 0.8563 - val_loss: 0.5198 - val_accuracy: 0.8211 - 97ms/epoch - 19ms/step\n",
            "Epoch 457/800\n",
            "5/5 - 0s - loss: 0.4224 - accuracy: 0.8466 - val_loss: 0.5191 - val_accuracy: 0.8211 - 95ms/epoch - 19ms/step\n",
            "Epoch 458/800\n",
            "5/5 - 0s - loss: 0.4142 - accuracy: 0.8571 - val_loss: 0.5203 - val_accuracy: 0.8211 - 101ms/epoch - 20ms/step\n",
            "Epoch 459/800\n",
            "5/5 - 0s - loss: 0.4214 - accuracy: 0.8536 - val_loss: 0.5235 - val_accuracy: 0.8211 - 96ms/epoch - 19ms/step\n",
            "Epoch 460/800\n",
            "5/5 - 0s - loss: 0.4001 - accuracy: 0.8642 - val_loss: 0.5282 - val_accuracy: 0.8105 - 95ms/epoch - 19ms/step\n",
            "Epoch 461/800\n",
            "5/5 - 0s - loss: 0.4255 - accuracy: 0.8580 - val_loss: 0.5335 - val_accuracy: 0.8105 - 116ms/epoch - 23ms/step\n",
            "Epoch 462/800\n",
            "5/5 - 0s - loss: 0.4231 - accuracy: 0.8466 - val_loss: 0.5394 - val_accuracy: 0.8035 - 98ms/epoch - 20ms/step\n",
            "Epoch 463/800\n",
            "5/5 - 0s - loss: 0.4444 - accuracy: 0.8379 - val_loss: 0.5878 - val_accuracy: 0.7860 - 102ms/epoch - 20ms/step\n",
            "Epoch 464/800\n",
            "5/5 - 0s - loss: 0.4725 - accuracy: 0.8317 - val_loss: 0.5683 - val_accuracy: 0.8000 - 110ms/epoch - 22ms/step\n",
            "Epoch 465/800\n",
            "5/5 - 0s - loss: 0.4478 - accuracy: 0.8344 - val_loss: 0.5150 - val_accuracy: 0.8281 - 99ms/epoch - 20ms/step\n",
            "Epoch 466/800\n",
            "5/5 - 0s - loss: 0.4160 - accuracy: 0.8466 - val_loss: 0.5227 - val_accuracy: 0.8211 - 95ms/epoch - 19ms/step\n",
            "Epoch 467/800\n",
            "5/5 - 0s - loss: 0.4339 - accuracy: 0.8519 - val_loss: 0.5426 - val_accuracy: 0.8175 - 95ms/epoch - 19ms/step\n",
            "Epoch 468/800\n",
            "5/5 - 0s - loss: 0.4415 - accuracy: 0.8361 - val_loss: 0.5510 - val_accuracy: 0.8105 - 97ms/epoch - 19ms/step\n",
            "Epoch 469/800\n",
            "5/5 - 0s - loss: 0.4203 - accuracy: 0.8457 - val_loss: 0.5544 - val_accuracy: 0.7965 - 105ms/epoch - 21ms/step\n",
            "Epoch 470/800\n",
            "5/5 - 0s - loss: 0.4399 - accuracy: 0.8484 - val_loss: 0.5551 - val_accuracy: 0.7930 - 99ms/epoch - 20ms/step\n",
            "Epoch 471/800\n",
            "5/5 - 0s - loss: 0.4358 - accuracy: 0.8466 - val_loss: 0.5470 - val_accuracy: 0.7965 - 94ms/epoch - 19ms/step\n",
            "Epoch 472/800\n",
            "5/5 - 0s - loss: 0.4177 - accuracy: 0.8501 - val_loss: 0.5369 - val_accuracy: 0.8070 - 93ms/epoch - 19ms/step\n",
            "Epoch 473/800\n",
            "5/5 - 0s - loss: 0.4208 - accuracy: 0.8484 - val_loss: 0.5307 - val_accuracy: 0.8140 - 124ms/epoch - 25ms/step\n",
            "Epoch 474/800\n",
            "5/5 - 0s - loss: 0.4230 - accuracy: 0.8484 - val_loss: 0.5297 - val_accuracy: 0.8211 - 93ms/epoch - 19ms/step\n",
            "Epoch 475/800\n",
            "5/5 - 0s - loss: 0.4115 - accuracy: 0.8510 - val_loss: 0.5297 - val_accuracy: 0.8211 - 94ms/epoch - 19ms/step\n",
            "Epoch 476/800\n",
            "5/5 - 0s - loss: 0.4264 - accuracy: 0.8528 - val_loss: 0.5168 - val_accuracy: 0.8281 - 109ms/epoch - 22ms/step\n",
            "Epoch 477/800\n",
            "5/5 - 0s - loss: 0.4227 - accuracy: 0.8396 - val_loss: 0.5257 - val_accuracy: 0.8211 - 98ms/epoch - 20ms/step\n",
            "Epoch 478/800\n",
            "5/5 - 0s - loss: 0.4738 - accuracy: 0.8352 - val_loss: 0.5240 - val_accuracy: 0.8246 - 97ms/epoch - 19ms/step\n",
            "Epoch 479/800\n",
            "5/5 - 0s - loss: 0.4539 - accuracy: 0.8414 - val_loss: 0.5042 - val_accuracy: 0.8281 - 127ms/epoch - 25ms/step\n",
            "Epoch 480/800\n",
            "5/5 - 0s - loss: 0.4349 - accuracy: 0.8414 - val_loss: 0.5238 - val_accuracy: 0.8000 - 98ms/epoch - 20ms/step\n",
            "Epoch 481/800\n",
            "5/5 - 0s - loss: 0.4428 - accuracy: 0.8405 - val_loss: 0.5465 - val_accuracy: 0.8035 - 99ms/epoch - 20ms/step\n",
            "Epoch 482/800\n",
            "5/5 - 0s - loss: 0.4597 - accuracy: 0.8317 - val_loss: 0.5545 - val_accuracy: 0.8000 - 104ms/epoch - 21ms/step\n",
            "Epoch 483/800\n",
            "5/5 - 0s - loss: 0.4644 - accuracy: 0.8370 - val_loss: 0.5557 - val_accuracy: 0.8000 - 104ms/epoch - 21ms/step\n",
            "Epoch 484/800\n",
            "5/5 - 0s - loss: 0.4511 - accuracy: 0.8344 - val_loss: 0.5477 - val_accuracy: 0.8000 - 103ms/epoch - 21ms/step\n",
            "Epoch 485/800\n",
            "5/5 - 0s - loss: 0.4427 - accuracy: 0.8361 - val_loss: 0.5369 - val_accuracy: 0.7965 - 96ms/epoch - 19ms/step\n",
            "Epoch 486/800\n",
            "5/5 - 0s - loss: 0.4361 - accuracy: 0.8387 - val_loss: 0.5155 - val_accuracy: 0.7965 - 103ms/epoch - 21ms/step\n",
            "Epoch 487/800\n",
            "5/5 - 0s - loss: 0.4336 - accuracy: 0.8396 - val_loss: 0.5681 - val_accuracy: 0.7789 - 94ms/epoch - 19ms/step\n",
            "Epoch 488/800\n",
            "5/5 - 0s - loss: 0.4765 - accuracy: 0.8352 - val_loss: 0.5721 - val_accuracy: 0.7860 - 99ms/epoch - 20ms/step\n",
            "Epoch 489/800\n",
            "5/5 - 0s - loss: 0.4514 - accuracy: 0.8379 - val_loss: 0.5170 - val_accuracy: 0.8246 - 97ms/epoch - 19ms/step\n",
            "Epoch 490/800\n",
            "5/5 - 0s - loss: 0.4390 - accuracy: 0.8449 - val_loss: 0.5228 - val_accuracy: 0.8246 - 95ms/epoch - 19ms/step\n",
            "Epoch 491/800\n",
            "5/5 - 0s - loss: 0.4935 - accuracy: 0.8247 - val_loss: 0.5450 - val_accuracy: 0.8105 - 95ms/epoch - 19ms/step\n",
            "Epoch 492/800\n",
            "5/5 - 0s - loss: 0.4925 - accuracy: 0.8309 - val_loss: 0.5127 - val_accuracy: 0.8140 - 120ms/epoch - 24ms/step\n",
            "Epoch 493/800\n",
            "5/5 - 0s - loss: 0.4383 - accuracy: 0.8431 - val_loss: 0.5043 - val_accuracy: 0.8140 - 100ms/epoch - 20ms/step\n",
            "Epoch 494/800\n",
            "5/5 - 0s - loss: 0.4310 - accuracy: 0.8457 - val_loss: 0.5101 - val_accuracy: 0.8035 - 111ms/epoch - 22ms/step\n",
            "Epoch 495/800\n",
            "5/5 - 0s - loss: 0.4358 - accuracy: 0.8484 - val_loss: 0.5035 - val_accuracy: 0.8211 - 128ms/epoch - 26ms/step\n",
            "Epoch 496/800\n",
            "5/5 - 0s - loss: 0.4245 - accuracy: 0.8484 - val_loss: 0.5027 - val_accuracy: 0.8246 - 119ms/epoch - 24ms/step\n",
            "Epoch 497/800\n",
            "5/5 - 0s - loss: 0.4253 - accuracy: 0.8484 - val_loss: 0.5028 - val_accuracy: 0.8246 - 106ms/epoch - 21ms/step\n",
            "Epoch 498/800\n",
            "5/5 - 0s - loss: 0.4204 - accuracy: 0.8405 - val_loss: 0.5047 - val_accuracy: 0.8246 - 97ms/epoch - 19ms/step\n",
            "Epoch 499/800\n",
            "5/5 - 0s - loss: 0.4346 - accuracy: 0.8422 - val_loss: 0.5124 - val_accuracy: 0.8070 - 106ms/epoch - 21ms/step\n",
            "Epoch 500/800\n",
            "5/5 - 0s - loss: 0.4352 - accuracy: 0.8457 - val_loss: 0.5242 - val_accuracy: 0.8035 - 101ms/epoch - 20ms/step\n",
            "Epoch 501/800\n",
            "5/5 - 0s - loss: 0.4570 - accuracy: 0.8379 - val_loss: 0.5361 - val_accuracy: 0.8035 - 115ms/epoch - 23ms/step\n",
            "Epoch 502/800\n",
            "5/5 - 0s - loss: 0.4265 - accuracy: 0.8440 - val_loss: 0.5318 - val_accuracy: 0.8070 - 97ms/epoch - 19ms/step\n",
            "Epoch 503/800\n",
            "5/5 - 0s - loss: 0.4319 - accuracy: 0.8493 - val_loss: 0.5677 - val_accuracy: 0.7965 - 95ms/epoch - 19ms/step\n",
            "Epoch 504/800\n",
            "5/5 - 0s - loss: 0.4694 - accuracy: 0.8414 - val_loss: 0.5810 - val_accuracy: 0.7930 - 101ms/epoch - 20ms/step\n",
            "Epoch 505/800\n",
            "5/5 - 0s - loss: 0.4642 - accuracy: 0.8422 - val_loss: 0.5774 - val_accuracy: 0.7930 - 103ms/epoch - 21ms/step\n",
            "Epoch 506/800\n",
            "5/5 - 0s - loss: 0.4712 - accuracy: 0.8370 - val_loss: 0.5649 - val_accuracy: 0.7965 - 100ms/epoch - 20ms/step\n",
            "Epoch 507/800\n",
            "5/5 - 0s - loss: 0.4836 - accuracy: 0.8317 - val_loss: 0.5507 - val_accuracy: 0.7965 - 111ms/epoch - 22ms/step\n",
            "Epoch 508/800\n",
            "5/5 - 0s - loss: 0.4454 - accuracy: 0.8414 - val_loss: 0.5393 - val_accuracy: 0.7965 - 103ms/epoch - 21ms/step\n",
            "Epoch 509/800\n",
            "5/5 - 0s - loss: 0.4445 - accuracy: 0.8405 - val_loss: 0.5331 - val_accuracy: 0.7965 - 126ms/epoch - 25ms/step\n",
            "Epoch 510/800\n",
            "5/5 - 0s - loss: 0.4336 - accuracy: 0.8457 - val_loss: 0.5290 - val_accuracy: 0.7930 - 114ms/epoch - 23ms/step\n",
            "Epoch 511/800\n",
            "5/5 - 0s - loss: 0.4422 - accuracy: 0.8422 - val_loss: 0.5340 - val_accuracy: 0.8000 - 113ms/epoch - 23ms/step\n",
            "Epoch 512/800\n",
            "5/5 - 0s - loss: 0.4584 - accuracy: 0.8449 - val_loss: 0.5326 - val_accuracy: 0.8035 - 104ms/epoch - 21ms/step\n",
            "Epoch 513/800\n",
            "5/5 - 0s - loss: 0.4537 - accuracy: 0.8405 - val_loss: 0.5153 - val_accuracy: 0.8140 - 104ms/epoch - 21ms/step\n",
            "Epoch 514/800\n",
            "5/5 - 0s - loss: 0.4380 - accuracy: 0.8422 - val_loss: 0.5066 - val_accuracy: 0.8211 - 111ms/epoch - 22ms/step\n",
            "Epoch 515/800\n",
            "5/5 - 0s - loss: 0.4237 - accuracy: 0.8475 - val_loss: 0.5094 - val_accuracy: 0.8246 - 104ms/epoch - 21ms/step\n",
            "Epoch 516/800\n",
            "5/5 - 0s - loss: 0.4217 - accuracy: 0.8449 - val_loss: 0.5150 - val_accuracy: 0.8175 - 106ms/epoch - 21ms/step\n",
            "Epoch 517/800\n",
            "5/5 - 0s - loss: 0.4179 - accuracy: 0.8414 - val_loss: 0.5173 - val_accuracy: 0.8211 - 94ms/epoch - 19ms/step\n",
            "Epoch 518/800\n",
            "5/5 - 0s - loss: 0.4242 - accuracy: 0.8457 - val_loss: 0.5186 - val_accuracy: 0.8211 - 103ms/epoch - 21ms/step\n",
            "Epoch 519/800\n",
            "5/5 - 0s - loss: 0.4369 - accuracy: 0.8370 - val_loss: 0.5142 - val_accuracy: 0.8281 - 107ms/epoch - 21ms/step\n",
            "Epoch 520/800\n",
            "5/5 - 0s - loss: 0.4227 - accuracy: 0.8387 - val_loss: 0.5138 - val_accuracy: 0.8246 - 100ms/epoch - 20ms/step\n",
            "Epoch 521/800\n",
            "5/5 - 0s - loss: 0.4308 - accuracy: 0.8352 - val_loss: 0.5164 - val_accuracy: 0.8175 - 96ms/epoch - 19ms/step\n",
            "Epoch 522/800\n",
            "5/5 - 0s - loss: 0.4527 - accuracy: 0.8405 - val_loss: 0.5170 - val_accuracy: 0.8175 - 96ms/epoch - 19ms/step\n",
            "Epoch 523/800\n",
            "5/5 - 0s - loss: 0.4483 - accuracy: 0.8440 - val_loss: 0.5479 - val_accuracy: 0.8105 - 105ms/epoch - 21ms/step\n",
            "Epoch 524/800\n",
            "5/5 - 0s - loss: 0.4656 - accuracy: 0.8352 - val_loss: 0.5364 - val_accuracy: 0.8105 - 161ms/epoch - 32ms/step\n",
            "Epoch 525/800\n",
            "5/5 - 0s - loss: 0.4862 - accuracy: 0.8221 - val_loss: 0.5229 - val_accuracy: 0.8035 - 196ms/epoch - 39ms/step\n",
            "Epoch 526/800\n",
            "5/5 - 0s - loss: 0.4766 - accuracy: 0.8265 - val_loss: 0.5242 - val_accuracy: 0.8140 - 238ms/epoch - 48ms/step\n",
            "Epoch 527/800\n",
            "5/5 - 0s - loss: 0.4472 - accuracy: 0.8361 - val_loss: 0.5358 - val_accuracy: 0.8070 - 231ms/epoch - 46ms/step\n",
            "Epoch 528/800\n",
            "5/5 - 0s - loss: 0.4454 - accuracy: 0.8414 - val_loss: 0.5658 - val_accuracy: 0.7930 - 236ms/epoch - 47ms/step\n",
            "Epoch 529/800\n",
            "5/5 - 0s - loss: 0.4417 - accuracy: 0.8449 - val_loss: 0.5755 - val_accuracy: 0.7930 - 231ms/epoch - 46ms/step\n",
            "Epoch 530/800\n",
            "5/5 - 0s - loss: 0.4555 - accuracy: 0.8405 - val_loss: 0.5654 - val_accuracy: 0.7930 - 227ms/epoch - 45ms/step\n",
            "Epoch 531/800\n",
            "5/5 - 0s - loss: 0.4448 - accuracy: 0.8431 - val_loss: 0.5458 - val_accuracy: 0.7930 - 226ms/epoch - 45ms/step\n",
            "Epoch 532/800\n",
            "5/5 - 0s - loss: 0.4388 - accuracy: 0.8387 - val_loss: 0.5307 - val_accuracy: 0.7965 - 219ms/epoch - 44ms/step\n",
            "Epoch 533/800\n",
            "5/5 - 0s - loss: 0.4233 - accuracy: 0.8396 - val_loss: 0.5214 - val_accuracy: 0.8000 - 247ms/epoch - 49ms/step\n",
            "Epoch 534/800\n",
            "5/5 - 0s - loss: 0.4148 - accuracy: 0.8466 - val_loss: 0.5138 - val_accuracy: 0.8105 - 202ms/epoch - 40ms/step\n",
            "Epoch 535/800\n",
            "5/5 - 0s - loss: 0.4290 - accuracy: 0.8475 - val_loss: 0.5224 - val_accuracy: 0.8070 - 222ms/epoch - 44ms/step\n",
            "Epoch 536/800\n",
            "5/5 - 0s - loss: 0.4347 - accuracy: 0.8528 - val_loss: 0.5514 - val_accuracy: 0.8035 - 217ms/epoch - 43ms/step\n",
            "Epoch 537/800\n",
            "5/5 - 0s - loss: 0.4898 - accuracy: 0.8317 - val_loss: 0.6246 - val_accuracy: 0.7860 - 215ms/epoch - 43ms/step\n",
            "Epoch 538/800\n",
            "5/5 - 0s - loss: 0.5331 - accuracy: 0.8203 - val_loss: 0.5848 - val_accuracy: 0.8035 - 199ms/epoch - 40ms/step\n",
            "Epoch 539/800\n",
            "5/5 - 0s - loss: 0.4712 - accuracy: 0.8431 - val_loss: 0.5284 - val_accuracy: 0.8175 - 211ms/epoch - 42ms/step\n",
            "Epoch 540/800\n",
            "5/5 - 0s - loss: 0.4263 - accuracy: 0.8536 - val_loss: 0.5141 - val_accuracy: 0.8246 - 248ms/epoch - 50ms/step\n",
            "Epoch 541/800\n",
            "5/5 - 0s - loss: 0.4179 - accuracy: 0.8598 - val_loss: 0.5189 - val_accuracy: 0.8140 - 104ms/epoch - 21ms/step\n",
            "Epoch 542/800\n",
            "5/5 - 0s - loss: 0.4074 - accuracy: 0.8536 - val_loss: 0.5292 - val_accuracy: 0.8070 - 98ms/epoch - 20ms/step\n",
            "Epoch 543/800\n",
            "5/5 - 0s - loss: 0.4157 - accuracy: 0.8414 - val_loss: 0.5376 - val_accuracy: 0.8070 - 109ms/epoch - 22ms/step\n",
            "Epoch 544/800\n",
            "5/5 - 0s - loss: 0.4163 - accuracy: 0.8449 - val_loss: 0.5419 - val_accuracy: 0.8035 - 96ms/epoch - 19ms/step\n",
            "Epoch 545/800\n",
            "5/5 - 0s - loss: 0.4132 - accuracy: 0.8440 - val_loss: 0.5421 - val_accuracy: 0.8000 - 101ms/epoch - 20ms/step\n",
            "Epoch 546/800\n",
            "5/5 - 0s - loss: 0.4152 - accuracy: 0.8510 - val_loss: 0.5453 - val_accuracy: 0.7965 - 98ms/epoch - 20ms/step\n",
            "Epoch 547/800\n",
            "5/5 - 0s - loss: 0.4356 - accuracy: 0.8475 - val_loss: 0.5499 - val_accuracy: 0.8000 - 97ms/epoch - 19ms/step\n",
            "Epoch 548/800\n",
            "5/5 - 0s - loss: 0.4358 - accuracy: 0.8431 - val_loss: 0.5460 - val_accuracy: 0.8000 - 121ms/epoch - 24ms/step\n",
            "Epoch 549/800\n",
            "5/5 - 0s - loss: 0.4275 - accuracy: 0.8422 - val_loss: 0.5431 - val_accuracy: 0.8070 - 107ms/epoch - 21ms/step\n",
            "Epoch 550/800\n",
            "5/5 - 0s - loss: 0.4276 - accuracy: 0.8484 - val_loss: 0.5467 - val_accuracy: 0.8105 - 106ms/epoch - 21ms/step\n",
            "Epoch 551/800\n",
            "5/5 - 0s - loss: 0.4316 - accuracy: 0.8510 - val_loss: 0.5502 - val_accuracy: 0.8035 - 97ms/epoch - 19ms/step\n",
            "Epoch 552/800\n",
            "5/5 - 0s - loss: 0.4190 - accuracy: 0.8414 - val_loss: 0.5518 - val_accuracy: 0.8105 - 107ms/epoch - 21ms/step\n",
            "Epoch 553/800\n",
            "5/5 - 0s - loss: 0.4205 - accuracy: 0.8466 - val_loss: 0.5492 - val_accuracy: 0.8105 - 100ms/epoch - 20ms/step\n",
            "Epoch 554/800\n",
            "5/5 - 0s - loss: 0.4309 - accuracy: 0.8449 - val_loss: 0.5462 - val_accuracy: 0.8070 - 109ms/epoch - 22ms/step\n",
            "Epoch 555/800\n",
            "5/5 - 0s - loss: 0.4037 - accuracy: 0.8440 - val_loss: 0.5413 - val_accuracy: 0.8070 - 102ms/epoch - 20ms/step\n",
            "Epoch 556/800\n",
            "5/5 - 0s - loss: 0.4144 - accuracy: 0.8493 - val_loss: 0.5368 - val_accuracy: 0.8000 - 108ms/epoch - 22ms/step\n",
            "Epoch 557/800\n",
            "5/5 - 0s - loss: 0.4138 - accuracy: 0.8431 - val_loss: 0.5320 - val_accuracy: 0.8070 - 113ms/epoch - 23ms/step\n",
            "Epoch 558/800\n",
            "5/5 - 0s - loss: 0.4158 - accuracy: 0.8466 - val_loss: 0.5286 - val_accuracy: 0.8070 - 97ms/epoch - 19ms/step\n",
            "Epoch 559/800\n",
            "5/5 - 0s - loss: 0.4153 - accuracy: 0.8493 - val_loss: 0.5272 - val_accuracy: 0.8070 - 108ms/epoch - 22ms/step\n",
            "Epoch 560/800\n",
            "5/5 - 0s - loss: 0.4071 - accuracy: 0.8528 - val_loss: 0.5199 - val_accuracy: 0.8175 - 115ms/epoch - 23ms/step\n",
            "Epoch 561/800\n",
            "5/5 - 0s - loss: 0.4142 - accuracy: 0.8510 - val_loss: 0.5256 - val_accuracy: 0.8000 - 108ms/epoch - 22ms/step\n",
            "Epoch 562/800\n",
            "5/5 - 0s - loss: 0.4625 - accuracy: 0.8317 - val_loss: 0.5335 - val_accuracy: 0.7895 - 111ms/epoch - 22ms/step\n",
            "Epoch 563/800\n",
            "5/5 - 0s - loss: 0.4786 - accuracy: 0.8361 - val_loss: 0.5275 - val_accuracy: 0.7930 - 119ms/epoch - 24ms/step\n",
            "Epoch 564/800\n",
            "5/5 - 0s - loss: 0.4522 - accuracy: 0.8370 - val_loss: 0.5153 - val_accuracy: 0.8070 - 120ms/epoch - 24ms/step\n",
            "Epoch 565/800\n",
            "5/5 - 0s - loss: 0.4468 - accuracy: 0.8352 - val_loss: 0.5246 - val_accuracy: 0.7965 - 112ms/epoch - 22ms/step\n",
            "Epoch 566/800\n",
            "5/5 - 0s - loss: 0.4641 - accuracy: 0.8300 - val_loss: 0.5130 - val_accuracy: 0.8140 - 144ms/epoch - 29ms/step\n",
            "Epoch 567/800\n",
            "5/5 - 0s - loss: 0.4243 - accuracy: 0.8528 - val_loss: 0.5082 - val_accuracy: 0.8175 - 99ms/epoch - 20ms/step\n",
            "Epoch 568/800\n",
            "5/5 - 0s - loss: 0.4228 - accuracy: 0.8449 - val_loss: 0.5173 - val_accuracy: 0.8070 - 96ms/epoch - 19ms/step\n",
            "Epoch 569/800\n",
            "5/5 - 0s - loss: 0.4151 - accuracy: 0.8501 - val_loss: 0.5368 - val_accuracy: 0.8000 - 104ms/epoch - 21ms/step\n",
            "Epoch 570/800\n",
            "5/5 - 0s - loss: 0.4458 - accuracy: 0.8379 - val_loss: 0.6004 - val_accuracy: 0.8000 - 101ms/epoch - 20ms/step\n",
            "Epoch 571/800\n",
            "5/5 - 0s - loss: 0.4769 - accuracy: 0.8405 - val_loss: 0.6152 - val_accuracy: 0.7965 - 98ms/epoch - 20ms/step\n",
            "Epoch 572/800\n",
            "5/5 - 0s - loss: 0.4892 - accuracy: 0.8361 - val_loss: 0.5968 - val_accuracy: 0.7930 - 127ms/epoch - 25ms/step\n",
            "Epoch 573/800\n",
            "5/5 - 0s - loss: 0.4523 - accuracy: 0.8414 - val_loss: 0.5582 - val_accuracy: 0.7965 - 96ms/epoch - 19ms/step\n",
            "Epoch 574/800\n",
            "5/5 - 0s - loss: 0.4371 - accuracy: 0.8361 - val_loss: 0.5382 - val_accuracy: 0.8035 - 106ms/epoch - 21ms/step\n",
            "Epoch 575/800\n",
            "5/5 - 0s - loss: 0.4536 - accuracy: 0.8344 - val_loss: 0.5312 - val_accuracy: 0.8000 - 117ms/epoch - 23ms/step\n",
            "Epoch 576/800\n",
            "5/5 - 0s - loss: 0.4492 - accuracy: 0.8379 - val_loss: 0.5237 - val_accuracy: 0.8035 - 122ms/epoch - 24ms/step\n",
            "Epoch 577/800\n",
            "5/5 - 0s - loss: 0.4381 - accuracy: 0.8370 - val_loss: 0.5175 - val_accuracy: 0.8105 - 108ms/epoch - 22ms/step\n",
            "Epoch 578/800\n",
            "5/5 - 0s - loss: 0.4186 - accuracy: 0.8536 - val_loss: 0.5216 - val_accuracy: 0.8000 - 111ms/epoch - 22ms/step\n",
            "Epoch 579/800\n",
            "5/5 - 0s - loss: 0.4260 - accuracy: 0.8484 - val_loss: 0.5181 - val_accuracy: 0.7930 - 100ms/epoch - 20ms/step\n",
            "Epoch 580/800\n",
            "5/5 - 0s - loss: 0.4236 - accuracy: 0.8545 - val_loss: 0.5056 - val_accuracy: 0.8175 - 99ms/epoch - 20ms/step\n",
            "Epoch 581/800\n",
            "5/5 - 0s - loss: 0.4226 - accuracy: 0.8563 - val_loss: 0.5045 - val_accuracy: 0.8105 - 95ms/epoch - 19ms/step\n",
            "Epoch 582/800\n",
            "5/5 - 0s - loss: 0.4193 - accuracy: 0.8457 - val_loss: 0.5081 - val_accuracy: 0.8070 - 105ms/epoch - 21ms/step\n",
            "Epoch 583/800\n",
            "5/5 - 0s - loss: 0.4144 - accuracy: 0.8545 - val_loss: 0.5160 - val_accuracy: 0.8000 - 103ms/epoch - 21ms/step\n",
            "Epoch 584/800\n",
            "5/5 - 0s - loss: 0.4142 - accuracy: 0.8519 - val_loss: 0.5157 - val_accuracy: 0.8035 - 106ms/epoch - 21ms/step\n",
            "Epoch 585/800\n",
            "5/5 - 0s - loss: 0.4122 - accuracy: 0.8554 - val_loss: 0.5058 - val_accuracy: 0.8140 - 102ms/epoch - 20ms/step\n",
            "Epoch 586/800\n",
            "5/5 - 0s - loss: 0.4322 - accuracy: 0.8387 - val_loss: 0.5037 - val_accuracy: 0.8246 - 99ms/epoch - 20ms/step\n",
            "Epoch 587/800\n",
            "5/5 - 0s - loss: 0.4314 - accuracy: 0.8493 - val_loss: 0.5423 - val_accuracy: 0.8105 - 113ms/epoch - 23ms/step\n",
            "Epoch 588/800\n",
            "5/5 - 0s - loss: 0.4631 - accuracy: 0.8344 - val_loss: 0.5865 - val_accuracy: 0.7930 - 98ms/epoch - 20ms/step\n",
            "Epoch 589/800\n",
            "5/5 - 0s - loss: 0.4932 - accuracy: 0.8282 - val_loss: 0.6095 - val_accuracy: 0.8000 - 112ms/epoch - 22ms/step\n",
            "Epoch 590/800\n",
            "5/5 - 0s - loss: 0.4703 - accuracy: 0.8405 - val_loss: 0.5802 - val_accuracy: 0.8000 - 126ms/epoch - 25ms/step\n",
            "Epoch 591/800\n",
            "5/5 - 0s - loss: 0.4531 - accuracy: 0.8422 - val_loss: 0.5811 - val_accuracy: 0.7965 - 95ms/epoch - 19ms/step\n",
            "Epoch 592/800\n",
            "5/5 - 0s - loss: 0.4465 - accuracy: 0.8387 - val_loss: 0.5665 - val_accuracy: 0.7965 - 108ms/epoch - 22ms/step\n",
            "Epoch 593/800\n",
            "5/5 - 0s - loss: 0.4325 - accuracy: 0.8493 - val_loss: 0.5443 - val_accuracy: 0.8211 - 107ms/epoch - 21ms/step\n",
            "Epoch 594/800\n",
            "5/5 - 0s - loss: 0.4868 - accuracy: 0.8256 - val_loss: 0.5665 - val_accuracy: 0.8035 - 107ms/epoch - 21ms/step\n",
            "Epoch 595/800\n",
            "5/5 - 0s - loss: 0.5125 - accuracy: 0.8168 - val_loss: 0.5236 - val_accuracy: 0.8140 - 109ms/epoch - 22ms/step\n",
            "Epoch 596/800\n",
            "5/5 - 0s - loss: 0.4283 - accuracy: 0.8440 - val_loss: 0.5371 - val_accuracy: 0.7965 - 104ms/epoch - 21ms/step\n",
            "Epoch 597/800\n",
            "5/5 - 0s - loss: 0.4425 - accuracy: 0.8457 - val_loss: 0.5633 - val_accuracy: 0.8035 - 97ms/epoch - 19ms/step\n",
            "Epoch 598/800\n",
            "5/5 - 0s - loss: 0.4749 - accuracy: 0.8370 - val_loss: 0.5684 - val_accuracy: 0.8035 - 101ms/epoch - 20ms/step\n",
            "Epoch 599/800\n",
            "5/5 - 0s - loss: 0.4633 - accuracy: 0.8431 - val_loss: 0.5473 - val_accuracy: 0.8035 - 102ms/epoch - 20ms/step\n",
            "Epoch 600/800\n",
            "5/5 - 0s - loss: 0.4456 - accuracy: 0.8379 - val_loss: 0.5143 - val_accuracy: 0.8105 - 106ms/epoch - 21ms/step\n",
            "Epoch 601/800\n",
            "5/5 - 0s - loss: 0.4340 - accuracy: 0.8405 - val_loss: 0.5240 - val_accuracy: 0.8000 - 96ms/epoch - 19ms/step\n",
            "Epoch 602/800\n",
            "5/5 - 0s - loss: 0.4498 - accuracy: 0.8484 - val_loss: 0.5330 - val_accuracy: 0.8035 - 113ms/epoch - 23ms/step\n",
            "Epoch 603/800\n",
            "5/5 - 0s - loss: 0.4473 - accuracy: 0.8457 - val_loss: 0.5318 - val_accuracy: 0.8070 - 98ms/epoch - 20ms/step\n",
            "Epoch 604/800\n",
            "5/5 - 0s - loss: 0.4463 - accuracy: 0.8379 - val_loss: 0.5351 - val_accuracy: 0.8000 - 112ms/epoch - 22ms/step\n",
            "Epoch 605/800\n",
            "5/5 - 0s - loss: 0.4324 - accuracy: 0.8475 - val_loss: 0.5378 - val_accuracy: 0.8000 - 100ms/epoch - 20ms/step\n",
            "Epoch 606/800\n",
            "5/5 - 0s - loss: 0.4467 - accuracy: 0.8440 - val_loss: 0.5386 - val_accuracy: 0.8000 - 106ms/epoch - 21ms/step\n",
            "Epoch 607/800\n",
            "5/5 - 0s - loss: 0.4413 - accuracy: 0.8370 - val_loss: 0.5373 - val_accuracy: 0.8000 - 102ms/epoch - 20ms/step\n",
            "Epoch 608/800\n",
            "5/5 - 0s - loss: 0.4299 - accuracy: 0.8484 - val_loss: 0.5272 - val_accuracy: 0.8035 - 105ms/epoch - 21ms/step\n",
            "Epoch 609/800\n",
            "5/5 - 0s - loss: 0.4280 - accuracy: 0.8501 - val_loss: 0.5118 - val_accuracy: 0.8000 - 107ms/epoch - 21ms/step\n",
            "Epoch 610/800\n",
            "5/5 - 0s - loss: 0.4416 - accuracy: 0.8475 - val_loss: 0.5195 - val_accuracy: 0.8000 - 112ms/epoch - 22ms/step\n",
            "Epoch 611/800\n",
            "5/5 - 0s - loss: 0.4569 - accuracy: 0.8396 - val_loss: 0.5116 - val_accuracy: 0.8000 - 110ms/epoch - 22ms/step\n",
            "Epoch 612/800\n",
            "5/5 - 0s - loss: 0.4417 - accuracy: 0.8440 - val_loss: 0.5037 - val_accuracy: 0.7965 - 111ms/epoch - 22ms/step\n",
            "Epoch 613/800\n",
            "5/5 - 0s - loss: 0.4167 - accuracy: 0.8563 - val_loss: 0.5051 - val_accuracy: 0.8140 - 114ms/epoch - 23ms/step\n",
            "Epoch 614/800\n",
            "5/5 - 0s - loss: 0.4353 - accuracy: 0.8484 - val_loss: 0.5113 - val_accuracy: 0.8211 - 106ms/epoch - 21ms/step\n",
            "Epoch 615/800\n",
            "5/5 - 0s - loss: 0.4184 - accuracy: 0.8528 - val_loss: 0.4999 - val_accuracy: 0.8175 - 290ms/epoch - 58ms/step\n",
            "Epoch 616/800\n",
            "5/5 - 0s - loss: 0.4264 - accuracy: 0.8475 - val_loss: 0.5094 - val_accuracy: 0.8211 - 104ms/epoch - 21ms/step\n",
            "Epoch 617/800\n",
            "5/5 - 0s - loss: 0.4486 - accuracy: 0.8414 - val_loss: 0.5005 - val_accuracy: 0.8105 - 97ms/epoch - 19ms/step\n",
            "Epoch 618/800\n",
            "5/5 - 0s - loss: 0.4274 - accuracy: 0.8501 - val_loss: 0.5008 - val_accuracy: 0.8175 - 115ms/epoch - 23ms/step\n",
            "Epoch 619/800\n",
            "5/5 - 0s - loss: 0.4115 - accuracy: 0.8510 - val_loss: 0.5130 - val_accuracy: 0.8211 - 116ms/epoch - 23ms/step\n",
            "Epoch 620/800\n",
            "5/5 - 0s - loss: 0.4126 - accuracy: 0.8475 - val_loss: 0.5197 - val_accuracy: 0.8175 - 98ms/epoch - 20ms/step\n",
            "Epoch 621/800\n",
            "5/5 - 0s - loss: 0.4020 - accuracy: 0.8606 - val_loss: 0.5094 - val_accuracy: 0.8246 - 115ms/epoch - 23ms/step\n",
            "Epoch 622/800\n",
            "5/5 - 0s - loss: 0.4160 - accuracy: 0.8510 - val_loss: 0.5074 - val_accuracy: 0.8246 - 98ms/epoch - 20ms/step\n",
            "Epoch 623/800\n",
            "5/5 - 0s - loss: 0.4204 - accuracy: 0.8580 - val_loss: 0.5056 - val_accuracy: 0.8246 - 123ms/epoch - 25ms/step\n",
            "Epoch 624/800\n",
            "5/5 - 0s - loss: 0.4001 - accuracy: 0.8642 - val_loss: 0.5044 - val_accuracy: 0.8246 - 111ms/epoch - 22ms/step\n",
            "Epoch 625/800\n",
            "5/5 - 0s - loss: 0.3992 - accuracy: 0.8633 - val_loss: 0.5040 - val_accuracy: 0.8211 - 100ms/epoch - 20ms/step\n",
            "Epoch 626/800\n",
            "5/5 - 0s - loss: 0.4084 - accuracy: 0.8571 - val_loss: 0.5022 - val_accuracy: 0.8246 - 120ms/epoch - 24ms/step\n",
            "Epoch 627/800\n",
            "5/5 - 0s - loss: 0.4182 - accuracy: 0.8571 - val_loss: 0.5024 - val_accuracy: 0.8211 - 209ms/epoch - 42ms/step\n",
            "Epoch 628/800\n",
            "5/5 - 0s - loss: 0.4090 - accuracy: 0.8563 - val_loss: 0.5036 - val_accuracy: 0.8175 - 197ms/epoch - 39ms/step\n",
            "Epoch 629/800\n",
            "5/5 - 0s - loss: 0.4074 - accuracy: 0.8571 - val_loss: 0.5155 - val_accuracy: 0.8105 - 223ms/epoch - 45ms/step\n",
            "Epoch 630/800\n",
            "5/5 - 0s - loss: 0.3994 - accuracy: 0.8536 - val_loss: 0.5135 - val_accuracy: 0.8211 - 214ms/epoch - 43ms/step\n",
            "Epoch 631/800\n",
            "5/5 - 0s - loss: 0.4289 - accuracy: 0.8466 - val_loss: 0.5889 - val_accuracy: 0.8035 - 183ms/epoch - 37ms/step\n",
            "Epoch 632/800\n",
            "5/5 - 0s - loss: 0.5075 - accuracy: 0.8028 - val_loss: 0.5665 - val_accuracy: 0.8070 - 209ms/epoch - 42ms/step\n",
            "Epoch 633/800\n",
            "5/5 - 0s - loss: 0.4536 - accuracy: 0.8431 - val_loss: 0.5127 - val_accuracy: 0.8246 - 213ms/epoch - 43ms/step\n",
            "Epoch 634/800\n",
            "5/5 - 0s - loss: 0.4242 - accuracy: 0.8484 - val_loss: 0.5261 - val_accuracy: 0.8140 - 203ms/epoch - 41ms/step\n",
            "Epoch 635/800\n",
            "5/5 - 0s - loss: 0.4181 - accuracy: 0.8493 - val_loss: 0.5464 - val_accuracy: 0.8211 - 222ms/epoch - 44ms/step\n",
            "Epoch 636/800\n",
            "5/5 - 0s - loss: 0.4255 - accuracy: 0.8493 - val_loss: 0.5592 - val_accuracy: 0.8211 - 222ms/epoch - 44ms/step\n",
            "Epoch 637/800\n",
            "5/5 - 0s - loss: 0.4362 - accuracy: 0.8484 - val_loss: 0.5876 - val_accuracy: 0.8175 - 254ms/epoch - 51ms/step\n",
            "Epoch 638/800\n",
            "5/5 - 0s - loss: 0.4412 - accuracy: 0.8484 - val_loss: 0.5707 - val_accuracy: 0.8175 - 223ms/epoch - 45ms/step\n",
            "Epoch 639/800\n",
            "5/5 - 0s - loss: 0.4235 - accuracy: 0.8431 - val_loss: 0.5115 - val_accuracy: 0.8175 - 247ms/epoch - 49ms/step\n",
            "Epoch 640/800\n",
            "5/5 - 0s - loss: 0.4246 - accuracy: 0.8484 - val_loss: 0.5006 - val_accuracy: 0.8175 - 243ms/epoch - 49ms/step\n",
            "Epoch 641/800\n",
            "5/5 - 0s - loss: 0.4636 - accuracy: 0.8457 - val_loss: 0.5071 - val_accuracy: 0.8211 - 214ms/epoch - 43ms/step\n",
            "Epoch 642/800\n",
            "5/5 - 0s - loss: 0.4555 - accuracy: 0.8422 - val_loss: 0.5016 - val_accuracy: 0.8211 - 216ms/epoch - 43ms/step\n",
            "Epoch 643/800\n",
            "5/5 - 0s - loss: 0.4498 - accuracy: 0.8405 - val_loss: 0.4942 - val_accuracy: 0.8140 - 240ms/epoch - 48ms/step\n",
            "Epoch 644/800\n",
            "5/5 - 0s - loss: 0.4326 - accuracy: 0.8493 - val_loss: 0.4923 - val_accuracy: 0.8070 - 135ms/epoch - 27ms/step\n",
            "Epoch 645/800\n",
            "5/5 - 0s - loss: 0.4318 - accuracy: 0.8519 - val_loss: 0.4998 - val_accuracy: 0.8140 - 108ms/epoch - 22ms/step\n",
            "Epoch 646/800\n",
            "5/5 - 0s - loss: 0.4305 - accuracy: 0.8475 - val_loss: 0.5018 - val_accuracy: 0.8140 - 102ms/epoch - 20ms/step\n",
            "Epoch 647/800\n",
            "5/5 - 0s - loss: 0.4201 - accuracy: 0.8510 - val_loss: 0.5004 - val_accuracy: 0.8175 - 116ms/epoch - 23ms/step\n",
            "Epoch 648/800\n",
            "5/5 - 0s - loss: 0.4149 - accuracy: 0.8493 - val_loss: 0.5137 - val_accuracy: 0.7965 - 126ms/epoch - 25ms/step\n",
            "Epoch 649/800\n",
            "5/5 - 0s - loss: 0.4182 - accuracy: 0.8440 - val_loss: 0.5286 - val_accuracy: 0.7965 - 127ms/epoch - 25ms/step\n",
            "Epoch 650/800\n",
            "5/5 - 0s - loss: 0.4246 - accuracy: 0.8493 - val_loss: 0.5351 - val_accuracy: 0.7930 - 119ms/epoch - 24ms/step\n",
            "Epoch 651/800\n",
            "5/5 - 0s - loss: 0.4308 - accuracy: 0.8379 - val_loss: 0.5337 - val_accuracy: 0.7930 - 123ms/epoch - 25ms/step\n",
            "Epoch 652/800\n",
            "5/5 - 0s - loss: 0.4302 - accuracy: 0.8457 - val_loss: 0.5274 - val_accuracy: 0.7930 - 99ms/epoch - 20ms/step\n",
            "Epoch 653/800\n",
            "5/5 - 0s - loss: 0.4111 - accuracy: 0.8571 - val_loss: 0.5197 - val_accuracy: 0.7965 - 103ms/epoch - 21ms/step\n",
            "Epoch 654/800\n",
            "5/5 - 0s - loss: 0.4167 - accuracy: 0.8484 - val_loss: 0.5133 - val_accuracy: 0.8000 - 98ms/epoch - 20ms/step\n",
            "Epoch 655/800\n",
            "5/5 - 0s - loss: 0.4102 - accuracy: 0.8536 - val_loss: 0.5107 - val_accuracy: 0.8070 - 120ms/epoch - 24ms/step\n",
            "Epoch 656/800\n",
            "5/5 - 0s - loss: 0.4036 - accuracy: 0.8580 - val_loss: 0.5142 - val_accuracy: 0.8140 - 118ms/epoch - 24ms/step\n",
            "Epoch 657/800\n",
            "5/5 - 0s - loss: 0.4060 - accuracy: 0.8501 - val_loss: 0.5161 - val_accuracy: 0.8140 - 117ms/epoch - 23ms/step\n",
            "Epoch 658/800\n",
            "5/5 - 0s - loss: 0.4012 - accuracy: 0.8528 - val_loss: 0.5178 - val_accuracy: 0.8105 - 97ms/epoch - 19ms/step\n",
            "Epoch 659/800\n",
            "5/5 - 0s - loss: 0.4112 - accuracy: 0.8519 - val_loss: 0.5221 - val_accuracy: 0.8000 - 103ms/epoch - 21ms/step\n",
            "Epoch 660/800\n",
            "5/5 - 0s - loss: 0.4049 - accuracy: 0.8571 - val_loss: 0.5255 - val_accuracy: 0.8000 - 148ms/epoch - 30ms/step\n",
            "Epoch 661/800\n",
            "5/5 - 0s - loss: 0.4060 - accuracy: 0.8519 - val_loss: 0.5257 - val_accuracy: 0.8000 - 172ms/epoch - 34ms/step\n",
            "Epoch 662/800\n",
            "5/5 - 0s - loss: 0.4102 - accuracy: 0.8510 - val_loss: 0.5224 - val_accuracy: 0.8000 - 126ms/epoch - 25ms/step\n",
            "Epoch 663/800\n",
            "5/5 - 0s - loss: 0.4068 - accuracy: 0.8580 - val_loss: 0.5189 - val_accuracy: 0.8035 - 107ms/epoch - 21ms/step\n",
            "Epoch 664/800\n",
            "5/5 - 0s - loss: 0.4003 - accuracy: 0.8554 - val_loss: 0.5119 - val_accuracy: 0.8070 - 113ms/epoch - 23ms/step\n",
            "Epoch 665/800\n",
            "5/5 - 0s - loss: 0.4038 - accuracy: 0.8563 - val_loss: 0.5062 - val_accuracy: 0.8140 - 110ms/epoch - 22ms/step\n",
            "Epoch 666/800\n",
            "5/5 - 0s - loss: 0.4450 - accuracy: 0.8414 - val_loss: 0.5201 - val_accuracy: 0.8105 - 125ms/epoch - 25ms/step\n",
            "Epoch 667/800\n",
            "5/5 - 0s - loss: 0.4747 - accuracy: 0.8273 - val_loss: 0.4994 - val_accuracy: 0.8140 - 126ms/epoch - 25ms/step\n",
            "Epoch 668/800\n",
            "5/5 - 0s - loss: 0.4300 - accuracy: 0.8493 - val_loss: 0.4931 - val_accuracy: 0.8316 - 106ms/epoch - 21ms/step\n",
            "Epoch 669/800\n",
            "5/5 - 0s - loss: 0.4090 - accuracy: 0.8606 - val_loss: 0.5030 - val_accuracy: 0.8281 - 125ms/epoch - 25ms/step\n",
            "Epoch 670/800\n",
            "5/5 - 0s - loss: 0.4094 - accuracy: 0.8519 - val_loss: 0.5149 - val_accuracy: 0.8175 - 120ms/epoch - 24ms/step\n",
            "Epoch 671/800\n",
            "5/5 - 0s - loss: 0.4146 - accuracy: 0.8466 - val_loss: 0.5199 - val_accuracy: 0.8035 - 102ms/epoch - 20ms/step\n",
            "Epoch 672/800\n",
            "5/5 - 0s - loss: 0.3947 - accuracy: 0.8642 - val_loss: 0.5260 - val_accuracy: 0.7930 - 144ms/epoch - 29ms/step\n",
            "Epoch 673/800\n",
            "5/5 - 0s - loss: 0.4152 - accuracy: 0.8475 - val_loss: 0.5208 - val_accuracy: 0.8070 - 101ms/epoch - 20ms/step\n",
            "Epoch 674/800\n",
            "5/5 - 0s - loss: 0.4135 - accuracy: 0.8528 - val_loss: 0.5188 - val_accuracy: 0.8175 - 107ms/epoch - 21ms/step\n",
            "Epoch 675/800\n",
            "5/5 - 0s - loss: 0.4499 - accuracy: 0.8370 - val_loss: 0.5201 - val_accuracy: 0.8175 - 113ms/epoch - 23ms/step\n",
            "Epoch 676/800\n",
            "5/5 - 0s - loss: 0.4546 - accuracy: 0.8440 - val_loss: 0.5190 - val_accuracy: 0.8105 - 132ms/epoch - 26ms/step\n",
            "Epoch 677/800\n",
            "5/5 - 0s - loss: 0.4307 - accuracy: 0.8484 - val_loss: 0.5489 - val_accuracy: 0.8035 - 120ms/epoch - 24ms/step\n",
            "Epoch 678/800\n",
            "5/5 - 0s - loss: 0.4496 - accuracy: 0.8422 - val_loss: 0.5714 - val_accuracy: 0.8000 - 114ms/epoch - 23ms/step\n",
            "Epoch 679/800\n",
            "5/5 - 0s - loss: 0.4525 - accuracy: 0.8387 - val_loss: 0.5738 - val_accuracy: 0.7965 - 109ms/epoch - 22ms/step\n",
            "Epoch 680/800\n",
            "5/5 - 0s - loss: 0.4487 - accuracy: 0.8387 - val_loss: 0.5443 - val_accuracy: 0.8000 - 140ms/epoch - 28ms/step\n",
            "Epoch 681/800\n",
            "5/5 - 0s - loss: 0.4255 - accuracy: 0.8457 - val_loss: 0.4987 - val_accuracy: 0.8000 - 110ms/epoch - 22ms/step\n",
            "Epoch 682/800\n",
            "5/5 - 0s - loss: 0.4347 - accuracy: 0.8484 - val_loss: 0.4961 - val_accuracy: 0.7930 - 141ms/epoch - 28ms/step\n",
            "Epoch 683/800\n",
            "5/5 - 0s - loss: 0.4320 - accuracy: 0.8457 - val_loss: 0.4999 - val_accuracy: 0.8035 - 107ms/epoch - 21ms/step\n",
            "Epoch 684/800\n",
            "5/5 - 0s - loss: 0.4251 - accuracy: 0.8449 - val_loss: 0.5089 - val_accuracy: 0.8105 - 106ms/epoch - 21ms/step\n",
            "Epoch 685/800\n",
            "5/5 - 0s - loss: 0.4210 - accuracy: 0.8457 - val_loss: 0.5167 - val_accuracy: 0.8105 - 109ms/epoch - 22ms/step\n",
            "Epoch 686/800\n",
            "5/5 - 0s - loss: 0.4303 - accuracy: 0.8379 - val_loss: 0.5233 - val_accuracy: 0.8070 - 104ms/epoch - 21ms/step\n",
            "Epoch 687/800\n",
            "5/5 - 0s - loss: 0.4241 - accuracy: 0.8414 - val_loss: 0.5333 - val_accuracy: 0.8035 - 120ms/epoch - 24ms/step\n",
            "Epoch 688/800\n",
            "5/5 - 0s - loss: 0.4455 - accuracy: 0.8317 - val_loss: 0.6080 - val_accuracy: 0.7719 - 102ms/epoch - 20ms/step\n",
            "Epoch 689/800\n",
            "5/5 - 0s - loss: 0.4742 - accuracy: 0.8273 - val_loss: 0.5925 - val_accuracy: 0.7860 - 108ms/epoch - 22ms/step\n",
            "Epoch 690/800\n",
            "5/5 - 0s - loss: 0.4481 - accuracy: 0.8291 - val_loss: 0.5366 - val_accuracy: 0.8070 - 112ms/epoch - 22ms/step\n",
            "Epoch 691/800\n",
            "5/5 - 0s - loss: 0.4134 - accuracy: 0.8440 - val_loss: 0.5123 - val_accuracy: 0.8070 - 105ms/epoch - 21ms/step\n",
            "Epoch 692/800\n",
            "5/5 - 0s - loss: 0.4162 - accuracy: 0.8493 - val_loss: 0.5104 - val_accuracy: 0.8035 - 113ms/epoch - 23ms/step\n",
            "Epoch 693/800\n",
            "5/5 - 0s - loss: 0.4141 - accuracy: 0.8466 - val_loss: 0.5120 - val_accuracy: 0.8000 - 103ms/epoch - 21ms/step\n",
            "Epoch 694/800\n",
            "5/5 - 0s - loss: 0.4088 - accuracy: 0.8484 - val_loss: 0.5161 - val_accuracy: 0.8070 - 101ms/epoch - 20ms/step\n",
            "Epoch 695/800\n",
            "5/5 - 0s - loss: 0.4178 - accuracy: 0.8519 - val_loss: 0.5884 - val_accuracy: 0.7860 - 108ms/epoch - 22ms/step\n",
            "Epoch 696/800\n",
            "5/5 - 0s - loss: 0.4645 - accuracy: 0.8282 - val_loss: 0.5967 - val_accuracy: 0.7825 - 101ms/epoch - 20ms/step\n",
            "Epoch 697/800\n",
            "5/5 - 0s - loss: 0.4518 - accuracy: 0.8361 - val_loss: 0.5330 - val_accuracy: 0.8105 - 106ms/epoch - 21ms/step\n",
            "Epoch 698/800\n",
            "5/5 - 0s - loss: 0.4120 - accuracy: 0.8501 - val_loss: 0.4997 - val_accuracy: 0.8070 - 111ms/epoch - 22ms/step\n",
            "Epoch 699/800\n",
            "5/5 - 0s - loss: 0.4106 - accuracy: 0.8563 - val_loss: 0.4934 - val_accuracy: 0.8211 - 110ms/epoch - 22ms/step\n",
            "Epoch 700/800\n",
            "5/5 - 0s - loss: 0.4343 - accuracy: 0.8528 - val_loss: 0.5013 - val_accuracy: 0.8175 - 99ms/epoch - 20ms/step\n",
            "Epoch 701/800\n",
            "5/5 - 0s - loss: 0.4492 - accuracy: 0.8396 - val_loss: 0.5023 - val_accuracy: 0.8140 - 115ms/epoch - 23ms/step\n",
            "Epoch 702/800\n",
            "5/5 - 0s - loss: 0.4348 - accuracy: 0.8449 - val_loss: 0.4990 - val_accuracy: 0.8246 - 101ms/epoch - 20ms/step\n",
            "Epoch 703/800\n",
            "5/5 - 0s - loss: 0.4322 - accuracy: 0.8580 - val_loss: 0.4954 - val_accuracy: 0.8175 - 118ms/epoch - 24ms/step\n",
            "Epoch 704/800\n",
            "5/5 - 0s - loss: 0.4195 - accuracy: 0.8519 - val_loss: 0.4984 - val_accuracy: 0.8140 - 118ms/epoch - 24ms/step\n",
            "Epoch 705/800\n",
            "5/5 - 0s - loss: 0.4053 - accuracy: 0.8615 - val_loss: 0.4933 - val_accuracy: 0.8140 - 98ms/epoch - 20ms/step\n",
            "Epoch 706/800\n",
            "5/5 - 0s - loss: 0.4232 - accuracy: 0.8414 - val_loss: 0.4957 - val_accuracy: 0.8175 - 103ms/epoch - 21ms/step\n",
            "Epoch 707/800\n",
            "5/5 - 0s - loss: 0.4035 - accuracy: 0.8606 - val_loss: 0.5161 - val_accuracy: 0.8140 - 110ms/epoch - 22ms/step\n",
            "Epoch 708/800\n",
            "5/5 - 0s - loss: 0.4459 - accuracy: 0.8414 - val_loss: 0.6283 - val_accuracy: 0.7754 - 130ms/epoch - 26ms/step\n",
            "Epoch 709/800\n",
            "5/5 - 0s - loss: 0.4881 - accuracy: 0.8273 - val_loss: 0.5864 - val_accuracy: 0.7895 - 116ms/epoch - 23ms/step\n",
            "Epoch 710/800\n",
            "5/5 - 0s - loss: 0.4417 - accuracy: 0.8493 - val_loss: 0.5046 - val_accuracy: 0.8316 - 108ms/epoch - 22ms/step\n",
            "Epoch 711/800\n",
            "5/5 - 0s - loss: 0.4190 - accuracy: 0.8449 - val_loss: 0.4964 - val_accuracy: 0.8211 - 104ms/epoch - 21ms/step\n",
            "Epoch 712/800\n",
            "5/5 - 0s - loss: 0.4307 - accuracy: 0.8422 - val_loss: 0.5023 - val_accuracy: 0.8246 - 107ms/epoch - 21ms/step\n",
            "Epoch 713/800\n",
            "5/5 - 0s - loss: 0.4117 - accuracy: 0.8606 - val_loss: 0.5144 - val_accuracy: 0.8211 - 112ms/epoch - 22ms/step\n",
            "Epoch 714/800\n",
            "5/5 - 0s - loss: 0.4116 - accuracy: 0.8484 - val_loss: 0.5267 - val_accuracy: 0.8070 - 103ms/epoch - 21ms/step\n",
            "Epoch 715/800\n",
            "5/5 - 0s - loss: 0.4041 - accuracy: 0.8554 - val_loss: 0.5355 - val_accuracy: 0.8035 - 133ms/epoch - 27ms/step\n",
            "Epoch 716/800\n",
            "5/5 - 0s - loss: 0.4028 - accuracy: 0.8606 - val_loss: 0.5414 - val_accuracy: 0.8070 - 118ms/epoch - 24ms/step\n",
            "Epoch 717/800\n",
            "5/5 - 0s - loss: 0.4170 - accuracy: 0.8484 - val_loss: 0.5438 - val_accuracy: 0.8035 - 102ms/epoch - 20ms/step\n",
            "Epoch 718/800\n",
            "5/5 - 0s - loss: 0.4065 - accuracy: 0.8563 - val_loss: 0.5395 - val_accuracy: 0.8070 - 104ms/epoch - 21ms/step\n",
            "Epoch 719/800\n",
            "5/5 - 0s - loss: 0.4105 - accuracy: 0.8536 - val_loss: 0.5306 - val_accuracy: 0.8105 - 98ms/epoch - 20ms/step\n",
            "Epoch 720/800\n",
            "5/5 - 0s - loss: 0.4096 - accuracy: 0.8519 - val_loss: 0.5232 - val_accuracy: 0.8211 - 117ms/epoch - 23ms/step\n",
            "Epoch 721/800\n",
            "5/5 - 0s - loss: 0.3995 - accuracy: 0.8580 - val_loss: 0.5203 - val_accuracy: 0.8211 - 122ms/epoch - 24ms/step\n",
            "Epoch 722/800\n",
            "5/5 - 0s - loss: 0.3932 - accuracy: 0.8606 - val_loss: 0.5407 - val_accuracy: 0.8211 - 99ms/epoch - 20ms/step\n",
            "Epoch 723/800\n",
            "5/5 - 0s - loss: 0.4147 - accuracy: 0.8519 - val_loss: 0.5518 - val_accuracy: 0.8211 - 101ms/epoch - 20ms/step\n",
            "Epoch 724/800\n",
            "5/5 - 0s - loss: 0.4221 - accuracy: 0.8536 - val_loss: 0.5514 - val_accuracy: 0.8211 - 123ms/epoch - 25ms/step\n",
            "Epoch 725/800\n",
            "5/5 - 0s - loss: 0.4075 - accuracy: 0.8545 - val_loss: 0.5455 - val_accuracy: 0.8140 - 128ms/epoch - 26ms/step\n",
            "Epoch 726/800\n",
            "5/5 - 0s - loss: 0.4038 - accuracy: 0.8536 - val_loss: 0.5309 - val_accuracy: 0.8105 - 210ms/epoch - 42ms/step\n",
            "Epoch 727/800\n",
            "5/5 - 0s - loss: 0.4036 - accuracy: 0.8501 - val_loss: 0.5198 - val_accuracy: 0.8211 - 215ms/epoch - 43ms/step\n",
            "Epoch 728/800\n",
            "5/5 - 0s - loss: 0.4014 - accuracy: 0.8545 - val_loss: 0.5232 - val_accuracy: 0.8140 - 230ms/epoch - 46ms/step\n",
            "Epoch 729/800\n",
            "5/5 - 0s - loss: 0.4001 - accuracy: 0.8606 - val_loss: 0.5347 - val_accuracy: 0.8175 - 227ms/epoch - 45ms/step\n",
            "Epoch 730/800\n",
            "5/5 - 0s - loss: 0.4130 - accuracy: 0.8519 - val_loss: 0.6341 - val_accuracy: 0.7754 - 202ms/epoch - 40ms/step\n",
            "Epoch 731/800\n",
            "5/5 - 0s - loss: 0.4762 - accuracy: 0.8344 - val_loss: 0.5931 - val_accuracy: 0.7965 - 231ms/epoch - 46ms/step\n",
            "Epoch 732/800\n",
            "5/5 - 0s - loss: 0.4461 - accuracy: 0.8370 - val_loss: 0.5288 - val_accuracy: 0.8140 - 223ms/epoch - 45ms/step\n",
            "Epoch 733/800\n",
            "5/5 - 0s - loss: 0.3953 - accuracy: 0.8545 - val_loss: 0.5071 - val_accuracy: 0.8386 - 201ms/epoch - 40ms/step\n",
            "Epoch 734/800\n",
            "5/5 - 0s - loss: 0.3978 - accuracy: 0.8563 - val_loss: 0.5043 - val_accuracy: 0.8281 - 240ms/epoch - 48ms/step\n",
            "Epoch 735/800\n",
            "5/5 - 0s - loss: 0.4158 - accuracy: 0.8475 - val_loss: 0.5063 - val_accuracy: 0.8175 - 232ms/epoch - 46ms/step\n",
            "Epoch 736/800\n",
            "5/5 - 0s - loss: 0.4419 - accuracy: 0.8449 - val_loss: 0.4961 - val_accuracy: 0.8281 - 193ms/epoch - 39ms/step\n",
            "Epoch 737/800\n",
            "5/5 - 0s - loss: 0.4082 - accuracy: 0.8493 - val_loss: 0.5269 - val_accuracy: 0.8070 - 202ms/epoch - 40ms/step\n",
            "Epoch 738/800\n",
            "5/5 - 0s - loss: 0.4267 - accuracy: 0.8457 - val_loss: 0.5584 - val_accuracy: 0.7965 - 231ms/epoch - 46ms/step\n",
            "Epoch 739/800\n",
            "5/5 - 0s - loss: 0.4546 - accuracy: 0.8335 - val_loss: 0.6041 - val_accuracy: 0.7860 - 220ms/epoch - 44ms/step\n",
            "Epoch 740/800\n",
            "5/5 - 0s - loss: 0.4875 - accuracy: 0.8238 - val_loss: 0.5783 - val_accuracy: 0.7895 - 223ms/epoch - 45ms/step\n",
            "Epoch 741/800\n",
            "5/5 - 0s - loss: 0.4639 - accuracy: 0.8335 - val_loss: 0.5290 - val_accuracy: 0.7965 - 210ms/epoch - 42ms/step\n",
            "Epoch 742/800\n",
            "5/5 - 0s - loss: 0.4273 - accuracy: 0.8422 - val_loss: 0.5057 - val_accuracy: 0.8140 - 161ms/epoch - 32ms/step\n",
            "Epoch 743/800\n",
            "5/5 - 0s - loss: 0.4239 - accuracy: 0.8484 - val_loss: 0.5303 - val_accuracy: 0.8000 - 118ms/epoch - 24ms/step\n",
            "Epoch 744/800\n",
            "5/5 - 0s - loss: 0.4532 - accuracy: 0.8536 - val_loss: 0.5482 - val_accuracy: 0.8035 - 105ms/epoch - 21ms/step\n",
            "Epoch 745/800\n",
            "5/5 - 0s - loss: 0.4658 - accuracy: 0.8405 - val_loss: 0.5531 - val_accuracy: 0.8105 - 120ms/epoch - 24ms/step\n",
            "Epoch 746/800\n",
            "5/5 - 0s - loss: 0.4747 - accuracy: 0.8387 - val_loss: 0.5434 - val_accuracy: 0.8070 - 99ms/epoch - 20ms/step\n",
            "Epoch 747/800\n",
            "5/5 - 0s - loss: 0.4496 - accuracy: 0.8405 - val_loss: 0.5357 - val_accuracy: 0.7965 - 103ms/epoch - 21ms/step\n",
            "Epoch 748/800\n",
            "5/5 - 0s - loss: 0.4281 - accuracy: 0.8431 - val_loss: 0.5341 - val_accuracy: 0.8000 - 100ms/epoch - 20ms/step\n",
            "Epoch 749/800\n",
            "5/5 - 0s - loss: 0.4184 - accuracy: 0.8510 - val_loss: 0.5340 - val_accuracy: 0.8070 - 101ms/epoch - 20ms/step\n",
            "Epoch 750/800\n",
            "5/5 - 0s - loss: 0.4182 - accuracy: 0.8466 - val_loss: 0.5251 - val_accuracy: 0.8070 - 100ms/epoch - 20ms/step\n",
            "Epoch 751/800\n",
            "5/5 - 0s - loss: 0.4228 - accuracy: 0.8440 - val_loss: 0.4998 - val_accuracy: 0.8105 - 105ms/epoch - 21ms/step\n",
            "Epoch 752/800\n",
            "5/5 - 0s - loss: 0.4086 - accuracy: 0.8536 - val_loss: 0.4953 - val_accuracy: 0.8175 - 110ms/epoch - 22ms/step\n",
            "Epoch 753/800\n",
            "5/5 - 0s - loss: 0.4043 - accuracy: 0.8536 - val_loss: 0.5011 - val_accuracy: 0.8140 - 112ms/epoch - 22ms/step\n",
            "Epoch 754/800\n",
            "5/5 - 0s - loss: 0.4047 - accuracy: 0.8606 - val_loss: 0.5052 - val_accuracy: 0.8105 - 139ms/epoch - 28ms/step\n",
            "Epoch 755/800\n",
            "5/5 - 0s - loss: 0.3955 - accuracy: 0.8580 - val_loss: 0.5134 - val_accuracy: 0.8000 - 115ms/epoch - 23ms/step\n",
            "Epoch 756/800\n",
            "5/5 - 0s - loss: 0.4129 - accuracy: 0.8519 - val_loss: 0.5287 - val_accuracy: 0.7965 - 102ms/epoch - 20ms/step\n",
            "Epoch 757/800\n",
            "5/5 - 0s - loss: 0.3974 - accuracy: 0.8615 - val_loss: 0.5251 - val_accuracy: 0.8000 - 101ms/epoch - 20ms/step\n",
            "Epoch 758/800\n",
            "5/5 - 0s - loss: 0.4062 - accuracy: 0.8510 - val_loss: 0.5221 - val_accuracy: 0.8070 - 111ms/epoch - 22ms/step\n",
            "Epoch 759/800\n",
            "5/5 - 0s - loss: 0.3986 - accuracy: 0.8598 - val_loss: 0.5124 - val_accuracy: 0.8070 - 103ms/epoch - 21ms/step\n",
            "Epoch 760/800\n",
            "5/5 - 0s - loss: 0.4022 - accuracy: 0.8545 - val_loss: 0.4956 - val_accuracy: 0.8246 - 107ms/epoch - 21ms/step\n",
            "Epoch 761/800\n",
            "5/5 - 0s - loss: 0.4152 - accuracy: 0.8493 - val_loss: 0.4971 - val_accuracy: 0.8246 - 96ms/epoch - 19ms/step\n",
            "Epoch 762/800\n",
            "5/5 - 0s - loss: 0.4184 - accuracy: 0.8440 - val_loss: 0.4959 - val_accuracy: 0.8246 - 127ms/epoch - 25ms/step\n",
            "Epoch 763/800\n",
            "5/5 - 0s - loss: 0.4034 - accuracy: 0.8545 - val_loss: 0.4967 - val_accuracy: 0.8140 - 122ms/epoch - 24ms/step\n",
            "Epoch 764/800\n",
            "5/5 - 0s - loss: 0.4063 - accuracy: 0.8519 - val_loss: 0.5042 - val_accuracy: 0.8070 - 101ms/epoch - 20ms/step\n",
            "Epoch 765/800\n",
            "5/5 - 0s - loss: 0.4038 - accuracy: 0.8554 - val_loss: 0.5128 - val_accuracy: 0.8070 - 118ms/epoch - 24ms/step\n",
            "Epoch 766/800\n",
            "5/5 - 0s - loss: 0.3969 - accuracy: 0.8589 - val_loss: 0.5184 - val_accuracy: 0.8070 - 104ms/epoch - 21ms/step\n",
            "Epoch 767/800\n",
            "5/5 - 0s - loss: 0.3974 - accuracy: 0.8545 - val_loss: 0.5211 - val_accuracy: 0.8000 - 120ms/epoch - 24ms/step\n",
            "Epoch 768/800\n",
            "5/5 - 0s - loss: 0.3940 - accuracy: 0.8528 - val_loss: 0.5207 - val_accuracy: 0.8000 - 103ms/epoch - 21ms/step\n",
            "Epoch 769/800\n",
            "5/5 - 0s - loss: 0.3970 - accuracy: 0.8589 - val_loss: 0.5176 - val_accuracy: 0.8070 - 103ms/epoch - 21ms/step\n",
            "Epoch 770/800\n",
            "5/5 - 0s - loss: 0.3927 - accuracy: 0.8580 - val_loss: 0.5081 - val_accuracy: 0.8140 - 107ms/epoch - 21ms/step\n",
            "Epoch 771/800\n",
            "5/5 - 0s - loss: 0.3973 - accuracy: 0.8554 - val_loss: 0.5277 - val_accuracy: 0.8105 - 121ms/epoch - 24ms/step\n",
            "Epoch 772/800\n",
            "5/5 - 0s - loss: 0.5054 - accuracy: 0.8230 - val_loss: 0.5632 - val_accuracy: 0.8000 - 123ms/epoch - 25ms/step\n",
            "Epoch 773/800\n",
            "5/5 - 0s - loss: 0.5220 - accuracy: 0.8160 - val_loss: 0.5086 - val_accuracy: 0.8175 - 101ms/epoch - 20ms/step\n",
            "Epoch 774/800\n",
            "5/5 - 0s - loss: 0.4093 - accuracy: 0.8475 - val_loss: 0.5032 - val_accuracy: 0.8211 - 102ms/epoch - 20ms/step\n",
            "Epoch 775/800\n",
            "5/5 - 0s - loss: 0.4135 - accuracy: 0.8501 - val_loss: 0.5308 - val_accuracy: 0.8175 - 109ms/epoch - 22ms/step\n",
            "Epoch 776/800\n",
            "5/5 - 0s - loss: 0.4062 - accuracy: 0.8475 - val_loss: 0.5511 - val_accuracy: 0.8140 - 101ms/epoch - 20ms/step\n",
            "Epoch 777/800\n",
            "5/5 - 0s - loss: 0.4116 - accuracy: 0.8545 - val_loss: 0.5583 - val_accuracy: 0.8070 - 106ms/epoch - 21ms/step\n",
            "Epoch 778/800\n",
            "5/5 - 0s - loss: 0.4077 - accuracy: 0.8545 - val_loss: 0.5542 - val_accuracy: 0.8070 - 102ms/epoch - 20ms/step\n",
            "Epoch 779/800\n",
            "5/5 - 0s - loss: 0.4158 - accuracy: 0.8510 - val_loss: 0.5444 - val_accuracy: 0.8035 - 106ms/epoch - 21ms/step\n",
            "Epoch 780/800\n",
            "5/5 - 0s - loss: 0.4079 - accuracy: 0.8501 - val_loss: 0.5284 - val_accuracy: 0.8105 - 132ms/epoch - 26ms/step\n",
            "Epoch 781/800\n",
            "5/5 - 0s - loss: 0.3953 - accuracy: 0.8571 - val_loss: 0.5073 - val_accuracy: 0.8246 - 108ms/epoch - 22ms/step\n",
            "Epoch 782/800\n",
            "5/5 - 0s - loss: 0.3929 - accuracy: 0.8536 - val_loss: 0.5031 - val_accuracy: 0.8281 - 108ms/epoch - 22ms/step\n",
            "Epoch 783/800\n",
            "5/5 - 0s - loss: 0.3963 - accuracy: 0.8685 - val_loss: 0.5056 - val_accuracy: 0.8316 - 109ms/epoch - 22ms/step\n",
            "Epoch 784/800\n",
            "5/5 - 0s - loss: 0.4022 - accuracy: 0.8571 - val_loss: 0.5119 - val_accuracy: 0.8351 - 122ms/epoch - 24ms/step\n",
            "Epoch 785/800\n",
            "5/5 - 0s - loss: 0.3781 - accuracy: 0.8624 - val_loss: 0.5149 - val_accuracy: 0.8316 - 100ms/epoch - 20ms/step\n",
            "Epoch 786/800\n",
            "5/5 - 0s - loss: 0.3973 - accuracy: 0.8589 - val_loss: 0.5119 - val_accuracy: 0.8316 - 113ms/epoch - 23ms/step\n",
            "Epoch 787/800\n",
            "5/5 - 0s - loss: 0.3911 - accuracy: 0.8528 - val_loss: 0.5138 - val_accuracy: 0.8316 - 108ms/epoch - 22ms/step\n",
            "Epoch 788/800\n",
            "5/5 - 0s - loss: 0.3945 - accuracy: 0.8519 - val_loss: 0.5175 - val_accuracy: 0.8316 - 105ms/epoch - 21ms/step\n",
            "Epoch 789/800\n",
            "5/5 - 0s - loss: 0.3897 - accuracy: 0.8606 - val_loss: 0.5149 - val_accuracy: 0.8246 - 119ms/epoch - 24ms/step\n",
            "Epoch 790/800\n",
            "5/5 - 0s - loss: 0.4076 - accuracy: 0.8545 - val_loss: 0.5133 - val_accuracy: 0.8140 - 98ms/epoch - 20ms/step\n",
            "Epoch 791/800\n",
            "5/5 - 0s - loss: 0.4323 - accuracy: 0.8484 - val_loss: 0.5122 - val_accuracy: 0.8175 - 104ms/epoch - 21ms/step\n",
            "Epoch 792/800\n",
            "5/5 - 0s - loss: 0.4026 - accuracy: 0.8571 - val_loss: 0.5463 - val_accuracy: 0.8035 - 103ms/epoch - 21ms/step\n",
            "Epoch 793/800\n",
            "5/5 - 0s - loss: 0.4220 - accuracy: 0.8501 - val_loss: 0.5762 - val_accuracy: 0.8070 - 108ms/epoch - 22ms/step\n",
            "Epoch 794/800\n",
            "5/5 - 0s - loss: 0.4340 - accuracy: 0.8501 - val_loss: 0.6061 - val_accuracy: 0.7930 - 117ms/epoch - 23ms/step\n",
            "Epoch 795/800\n",
            "5/5 - 0s - loss: 0.4219 - accuracy: 0.8598 - val_loss: 0.5401 - val_accuracy: 0.8105 - 102ms/epoch - 20ms/step\n",
            "Epoch 796/800\n",
            "5/5 - 0s - loss: 0.4116 - accuracy: 0.8519 - val_loss: 0.5116 - val_accuracy: 0.8105 - 116ms/epoch - 23ms/step\n",
            "Epoch 797/800\n",
            "5/5 - 0s - loss: 0.4041 - accuracy: 0.8510 - val_loss: 0.5060 - val_accuracy: 0.8105 - 108ms/epoch - 22ms/step\n",
            "Epoch 798/800\n",
            "5/5 - 0s - loss: 0.4053 - accuracy: 0.8554 - val_loss: 0.5084 - val_accuracy: 0.8105 - 122ms/epoch - 24ms/step\n",
            "Epoch 799/800\n",
            "5/5 - 0s - loss: 0.4051 - accuracy: 0.8615 - val_loss: 0.5159 - val_accuracy: 0.8105 - 101ms/epoch - 20ms/step\n",
            "Epoch 800/800\n",
            "5/5 - 0s - loss: 0.4054 - accuracy: 0.8563 - val_loss: 0.5220 - val_accuracy: 0.8105 - 124ms/epoch - 25ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WJLmpdZx1nm",
        "outputId": "b499351d-e8ce-4b3a-e5bb-e62aba8dea67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9/9 [==============================] - 0s 4ms/step - loss: 0.5220 - accuracy: 0.8105\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.522039532661438, 0.8105263113975525]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Zlaky5lqTZH",
        "outputId": "d7079db2-3b62-4e18-eba9-fe8f744b2cb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9/9 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
              "       0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jOruksPXsaA",
        "outputId": "0b4d92f1-7b98-4f11-e402-bca26bf27c5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 2., 2., 1., 1., 0.,\n",
              "       1., 1., 2., 1., 1., 1., 2., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
              "       1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 2., 2., 2., 2., 1., 1.,\n",
              "       1., 1., 2., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 2., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 2., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 2., 2., 1., 1., 1., 1., 1., 1., 1., 2., 1., 0., 0., 1., 1.,\n",
              "       1., 1., 1., 1., 2., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
              "       2., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 2., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 2., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 2., 2., 1.,\n",
              "       1., 0., 1., 1., 1., 1., 2., 1., 1., 1., 2., 2., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "accuracy_score(y_test, y_pred), f1_score(y_test, y_pred, average='weighted')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goZvC0zXn7wX",
        "outputId": "0018af34-54ce-4d38-ffa4-fe3d34aa5597"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8105263157894737, 0.7553088477660271)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_test, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zZWhkDfoMBu",
        "outputId": "7f9da889-2865-4674-daad-79fdfc650f2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  8,  24,   0],\n",
              "       [  3, 222,   1],\n",
              "       [  0,  26,   1]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conf = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "buy_acc = conf[0][0]/sum(conf[0])\n",
        "sell_acc = conf[2][2]/sum(conf[2])\n",
        "\n",
        "print('매수(buy) 정확도: ', buy_acc)\n",
        "print('매도(sell) 정확도: ', sell_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG1Zl4DdWwpe",
        "outputId": "da79a5b1-f0f0-4689-ae73-07069ca25ed6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "매수(buy) 정확도:  0.25\n",
            "매도(sell) 정확도:  0.037037037037037035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CNN"
      ],
      "metadata": {
        "id": "PFfTnmozO8cX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Conv1D(filters=128, kernel_size=3, activation='relu', input_shape=(3, 26)))\n",
        "model.add(Conv1D(filters=64, kernel_size=2, activation = 'relu'))\n",
        "model.add(MaxPooling1D(pool_size=1))\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "P8nuB9vYO-TI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "outputId": "a3921ecc-5f32-45e0-d2fe-5ed8b4a6b8ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-7b129d4a003d>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m26\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/trackable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[1;32m   1971\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1972\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1973\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1975\u001b[0m   \u001b[0;31m# Record the current Python stack trace as the creating stacktrace of this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"conv1d_1\" (type Conv1D).\n\nNegative dimension size caused by subtracting 2 from 1 for '{{node conv1d_1/Conv1D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv1d_1/Conv1D/ExpandDims, conv1d_1/Conv1D/ExpandDims_1)' with input shapes: [?,1,1,128], [1,2,128,64].\n\nCall arguments received by layer \"conv1d_1\" (type Conv1D):\n  • inputs=tf.Tensor(shape=(None, 1, 128), dtype=float32)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "w_UmByOAViGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from keras import callbacks\n",
        "\n",
        "epochs=500\n",
        "verbosity = 2\n",
        "dirx ='/content/drive/MyDrive'\n",
        "os.chdir(dirx)\n",
        "h5 = 'network.h5'\n",
        "checkpoint = callbacks.ModelCheckpoint(h5,\n",
        "                                       monitor='val_loss',\n",
        "                                       verbose=0,\n",
        "                                       save_best_only=True,\n",
        "                                       save_weights_only=True,\n",
        "                                       mode='auto',\n",
        "                                       period=1)\n",
        "callback = [checkpoint]\n",
        "json = 'network.json'\n",
        "model_json = model.to_json()\n",
        "with open(json, 'w') as json_file:\n",
        "  json_file.write(model_json)\n",
        "history = model.fit(X_train,\n",
        "                    y_train,\n",
        "                    epochs = epochs,\n",
        "                    batch_size =len(X_train)//4,\n",
        "                    validation_data = (X_test,y_test),\n",
        "                    verbose = verbosity,\n",
        "                    callbacks=callback)"
      ],
      "metadata": {
        "id": "hE6ttHWvaY8n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4d70d11-3e61-45bb-d02e-3674e3e55a79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "5/5 - 4s - loss: 1.0062 - accuracy: 0.5148 - val_loss: 0.7091 - val_accuracy: 0.8028 - 4s/epoch - 753ms/step\n",
            "Epoch 2/500\n",
            "5/5 - 0s - loss: 0.6673 - accuracy: 0.8190 - val_loss: 0.7043 - val_accuracy: 0.8028 - 82ms/epoch - 16ms/step\n",
            "Epoch 3/500\n",
            "5/5 - 0s - loss: 0.7389 - accuracy: 0.8190 - val_loss: 0.6877 - val_accuracy: 0.8028 - 95ms/epoch - 19ms/step\n",
            "Epoch 4/500\n",
            "5/5 - 0s - loss: 0.6709 - accuracy: 0.8190 - val_loss: 0.6452 - val_accuracy: 0.8028 - 112ms/epoch - 22ms/step\n",
            "Epoch 5/500\n",
            "5/5 - 0s - loss: 0.6156 - accuracy: 0.8190 - val_loss: 0.6780 - val_accuracy: 0.8028 - 62ms/epoch - 12ms/step\n",
            "Epoch 6/500\n",
            "5/5 - 0s - loss: 0.6243 - accuracy: 0.8190 - val_loss: 0.6833 - val_accuracy: 0.8028 - 63ms/epoch - 13ms/step\n",
            "Epoch 7/500\n",
            "5/5 - 0s - loss: 0.6185 - accuracy: 0.8190 - val_loss: 0.6542 - val_accuracy: 0.8028 - 84ms/epoch - 17ms/step\n",
            "Epoch 8/500\n",
            "5/5 - 0s - loss: 0.6200 - accuracy: 0.8190 - val_loss: 0.6552 - val_accuracy: 0.8028 - 78ms/epoch - 16ms/step\n",
            "Epoch 9/500\n",
            "5/5 - 0s - loss: 0.6377 - accuracy: 0.8190 - val_loss: 0.6466 - val_accuracy: 0.8028 - 63ms/epoch - 13ms/step\n",
            "Epoch 10/500\n",
            "5/5 - 0s - loss: 0.6207 - accuracy: 0.8190 - val_loss: 0.6680 - val_accuracy: 0.8028 - 75ms/epoch - 15ms/step\n",
            "Epoch 11/500\n",
            "5/5 - 0s - loss: 0.6171 - accuracy: 0.8190 - val_loss: 0.6630 - val_accuracy: 0.8028 - 77ms/epoch - 15ms/step\n",
            "Epoch 12/500\n",
            "5/5 - 0s - loss: 0.6138 - accuracy: 0.8190 - val_loss: 0.6522 - val_accuracy: 0.8028 - 59ms/epoch - 12ms/step\n",
            "Epoch 13/500\n",
            "5/5 - 0s - loss: 0.6272 - accuracy: 0.8190 - val_loss: 0.6444 - val_accuracy: 0.8028 - 82ms/epoch - 16ms/step\n",
            "Epoch 14/500\n",
            "5/5 - 0s - loss: 0.6284 - accuracy: 0.8190 - val_loss: 0.6472 - val_accuracy: 0.8028 - 57ms/epoch - 11ms/step\n",
            "Epoch 15/500\n",
            "5/5 - 0s - loss: 0.6248 - accuracy: 0.8190 - val_loss: 0.6431 - val_accuracy: 0.8028 - 103ms/epoch - 21ms/step\n",
            "Epoch 16/500\n",
            "5/5 - 0s - loss: 0.6123 - accuracy: 0.8190 - val_loss: 0.6702 - val_accuracy: 0.8028 - 57ms/epoch - 11ms/step\n",
            "Epoch 17/500\n",
            "5/5 - 0s - loss: 0.6168 - accuracy: 0.8190 - val_loss: 0.6469 - val_accuracy: 0.8028 - 75ms/epoch - 15ms/step\n",
            "Epoch 18/500\n",
            "5/5 - 0s - loss: 0.6074 - accuracy: 0.8190 - val_loss: 0.6351 - val_accuracy: 0.8028 - 85ms/epoch - 17ms/step\n",
            "Epoch 19/500\n",
            "5/5 - 0s - loss: 0.6102 - accuracy: 0.8190 - val_loss: 0.6376 - val_accuracy: 0.8028 - 74ms/epoch - 15ms/step\n",
            "Epoch 20/500\n",
            "5/5 - 0s - loss: 0.5991 - accuracy: 0.8190 - val_loss: 0.6393 - val_accuracy: 0.8028 - 91ms/epoch - 18ms/step\n",
            "Epoch 21/500\n",
            "5/5 - 0s - loss: 0.6084 - accuracy: 0.8190 - val_loss: 0.6408 - val_accuracy: 0.8028 - 73ms/epoch - 15ms/step\n",
            "Epoch 22/500\n",
            "5/5 - 0s - loss: 0.6069 - accuracy: 0.8190 - val_loss: 0.6431 - val_accuracy: 0.8028 - 72ms/epoch - 14ms/step\n",
            "Epoch 23/500\n",
            "5/5 - 0s - loss: 0.6022 - accuracy: 0.8190 - val_loss: 0.6580 - val_accuracy: 0.8028 - 75ms/epoch - 15ms/step\n",
            "Epoch 24/500\n",
            "5/5 - 0s - loss: 0.6069 - accuracy: 0.8190 - val_loss: 0.6512 - val_accuracy: 0.8028 - 62ms/epoch - 12ms/step\n",
            "Epoch 25/500\n",
            "5/5 - 0s - loss: 0.6052 - accuracy: 0.8190 - val_loss: 0.6887 - val_accuracy: 0.8028 - 61ms/epoch - 12ms/step\n",
            "Epoch 26/500\n",
            "5/5 - 0s - loss: 0.6306 - accuracy: 0.8190 - val_loss: 0.6938 - val_accuracy: 0.8028 - 79ms/epoch - 16ms/step\n",
            "Epoch 27/500\n",
            "5/5 - 0s - loss: 0.6126 - accuracy: 0.8190 - val_loss: 0.6393 - val_accuracy: 0.8028 - 64ms/epoch - 13ms/step\n",
            "Epoch 28/500\n",
            "5/5 - 0s - loss: 0.6061 - accuracy: 0.8190 - val_loss: 0.6394 - val_accuracy: 0.8028 - 63ms/epoch - 13ms/step\n",
            "Epoch 29/500\n",
            "5/5 - 0s - loss: 0.6182 - accuracy: 0.8190 - val_loss: 0.6393 - val_accuracy: 0.8028 - 59ms/epoch - 12ms/step\n",
            "Epoch 30/500\n",
            "5/5 - 0s - loss: 0.6107 - accuracy: 0.8190 - val_loss: 0.6548 - val_accuracy: 0.8028 - 60ms/epoch - 12ms/step\n",
            "Epoch 31/500\n",
            "5/5 - 0s - loss: 0.6054 - accuracy: 0.8190 - val_loss: 0.6824 - val_accuracy: 0.8028 - 79ms/epoch - 16ms/step\n",
            "Epoch 32/500\n",
            "5/5 - 0s - loss: 0.6373 - accuracy: 0.8190 - val_loss: 0.6726 - val_accuracy: 0.8028 - 80ms/epoch - 16ms/step\n",
            "Epoch 33/500\n",
            "5/5 - 0s - loss: 0.6140 - accuracy: 0.8190 - val_loss: 0.6350 - val_accuracy: 0.8028 - 107ms/epoch - 21ms/step\n",
            "Epoch 34/500\n",
            "5/5 - 0s - loss: 0.6191 - accuracy: 0.8190 - val_loss: 0.6487 - val_accuracy: 0.8028 - 76ms/epoch - 15ms/step\n",
            "Epoch 35/500\n",
            "5/5 - 0s - loss: 0.6469 - accuracy: 0.8190 - val_loss: 0.6375 - val_accuracy: 0.8028 - 58ms/epoch - 12ms/step\n",
            "Epoch 36/500\n",
            "5/5 - 0s - loss: 0.6032 - accuracy: 0.8190 - val_loss: 0.6561 - val_accuracy: 0.8028 - 63ms/epoch - 13ms/step\n",
            "Epoch 37/500\n",
            "5/5 - 0s - loss: 0.6101 - accuracy: 0.8190 - val_loss: 0.6596 - val_accuracy: 0.8028 - 61ms/epoch - 12ms/step\n",
            "Epoch 38/500\n",
            "5/5 - 0s - loss: 0.5964 - accuracy: 0.8190 - val_loss: 0.6496 - val_accuracy: 0.8028 - 58ms/epoch - 12ms/step\n",
            "Epoch 39/500\n",
            "5/5 - 0s - loss: 0.5864 - accuracy: 0.8190 - val_loss: 0.6476 - val_accuracy: 0.8028 - 72ms/epoch - 14ms/step\n",
            "Epoch 40/500\n",
            "5/5 - 0s - loss: 0.6094 - accuracy: 0.8190 - val_loss: 0.6549 - val_accuracy: 0.8028 - 75ms/epoch - 15ms/step\n",
            "Epoch 41/500\n",
            "5/5 - 0s - loss: 0.6143 - accuracy: 0.8190 - val_loss: 0.6451 - val_accuracy: 0.8028 - 74ms/epoch - 15ms/step\n",
            "Epoch 42/500\n",
            "5/5 - 0s - loss: 0.6087 - accuracy: 0.8190 - val_loss: 0.6428 - val_accuracy: 0.8028 - 59ms/epoch - 12ms/step\n",
            "Epoch 43/500\n",
            "5/5 - 0s - loss: 0.6073 - accuracy: 0.8190 - val_loss: 0.6463 - val_accuracy: 0.8028 - 58ms/epoch - 12ms/step\n",
            "Epoch 44/500\n",
            "5/5 - 0s - loss: 0.6098 - accuracy: 0.8190 - val_loss: 0.6544 - val_accuracy: 0.8028 - 74ms/epoch - 15ms/step\n",
            "Epoch 45/500\n",
            "5/5 - 0s - loss: 0.6007 - accuracy: 0.8190 - val_loss: 0.6409 - val_accuracy: 0.8028 - 66ms/epoch - 13ms/step\n",
            "Epoch 46/500\n",
            "5/5 - 0s - loss: 0.6177 - accuracy: 0.8190 - val_loss: 0.6453 - val_accuracy: 0.8028 - 60ms/epoch - 12ms/step\n",
            "Epoch 47/500\n",
            "5/5 - 0s - loss: 0.6039 - accuracy: 0.8190 - val_loss: 0.6388 - val_accuracy: 0.8028 - 76ms/epoch - 15ms/step\n",
            "Epoch 48/500\n",
            "5/5 - 0s - loss: 0.6031 - accuracy: 0.8190 - val_loss: 0.6395 - val_accuracy: 0.8028 - 79ms/epoch - 16ms/step\n",
            "Epoch 49/500\n",
            "5/5 - 0s - loss: 0.5993 - accuracy: 0.8190 - val_loss: 0.6418 - val_accuracy: 0.8028 - 59ms/epoch - 12ms/step\n",
            "Epoch 50/500\n",
            "5/5 - 0s - loss: 0.6081 - accuracy: 0.8190 - val_loss: 0.6415 - val_accuracy: 0.8028 - 59ms/epoch - 12ms/step\n",
            "Epoch 51/500\n",
            "5/5 - 0s - loss: 0.6026 - accuracy: 0.8190 - val_loss: 0.6681 - val_accuracy: 0.8028 - 56ms/epoch - 11ms/step\n",
            "Epoch 52/500\n",
            "5/5 - 0s - loss: 0.6044 - accuracy: 0.8190 - val_loss: 0.6545 - val_accuracy: 0.8028 - 62ms/epoch - 12ms/step\n",
            "Epoch 53/500\n",
            "5/5 - 0s - loss: 0.6126 - accuracy: 0.8190 - val_loss: 0.6629 - val_accuracy: 0.8028 - 75ms/epoch - 15ms/step\n",
            "Epoch 54/500\n",
            "5/5 - 0s - loss: 0.6167 - accuracy: 0.8190 - val_loss: 0.6557 - val_accuracy: 0.8028 - 62ms/epoch - 12ms/step\n",
            "Epoch 55/500\n",
            "5/5 - 0s - loss: 0.6066 - accuracy: 0.8190 - val_loss: 0.7117 - val_accuracy: 0.8028 - 81ms/epoch - 16ms/step\n",
            "Epoch 56/500\n",
            "5/5 - 0s - loss: 0.6167 - accuracy: 0.8190 - val_loss: 0.6875 - val_accuracy: 0.8028 - 63ms/epoch - 13ms/step\n",
            "Epoch 57/500\n",
            "5/5 - 0s - loss: 0.6090 - accuracy: 0.8190 - val_loss: 0.6533 - val_accuracy: 0.8028 - 79ms/epoch - 16ms/step\n",
            "Epoch 58/500\n",
            "5/5 - 0s - loss: 0.6308 - accuracy: 0.8190 - val_loss: 0.6526 - val_accuracy: 0.8028 - 77ms/epoch - 15ms/step\n",
            "Epoch 59/500\n",
            "5/5 - 0s - loss: 0.6090 - accuracy: 0.8190 - val_loss: 0.7142 - val_accuracy: 0.8028 - 75ms/epoch - 15ms/step\n",
            "Epoch 60/500\n",
            "5/5 - 0s - loss: 0.6244 - accuracy: 0.8190 - val_loss: 0.7364 - val_accuracy: 0.8028 - 69ms/epoch - 14ms/step\n",
            "Epoch 61/500\n",
            "5/5 - 0s - loss: 0.6172 - accuracy: 0.8190 - val_loss: 0.6754 - val_accuracy: 0.8028 - 61ms/epoch - 12ms/step\n",
            "Epoch 62/500\n",
            "5/5 - 0s - loss: 0.5929 - accuracy: 0.8190 - val_loss: 0.6713 - val_accuracy: 0.8028 - 63ms/epoch - 13ms/step\n",
            "Epoch 63/500\n",
            "5/5 - 0s - loss: 0.6273 - accuracy: 0.8190 - val_loss: 0.6675 - val_accuracy: 0.8028 - 55ms/epoch - 11ms/step\n",
            "Epoch 64/500\n",
            "5/5 - 0s - loss: 0.6138 - accuracy: 0.8190 - val_loss: 0.6616 - val_accuracy: 0.8028 - 58ms/epoch - 12ms/step\n",
            "Epoch 65/500\n",
            "5/5 - 0s - loss: 0.6005 - accuracy: 0.8190 - val_loss: 0.6630 - val_accuracy: 0.8028 - 60ms/epoch - 12ms/step\n",
            "Epoch 66/500\n",
            "5/5 - 0s - loss: 0.5909 - accuracy: 0.8190 - val_loss: 0.6580 - val_accuracy: 0.8028 - 61ms/epoch - 12ms/step\n",
            "Epoch 67/500\n",
            "5/5 - 0s - loss: 0.5993 - accuracy: 0.8190 - val_loss: 0.6640 - val_accuracy: 0.8028 - 76ms/epoch - 15ms/step\n",
            "Epoch 68/500\n",
            "5/5 - 0s - loss: 0.5899 - accuracy: 0.8190 - val_loss: 0.6531 - val_accuracy: 0.8028 - 63ms/epoch - 13ms/step\n",
            "Epoch 69/500\n",
            "5/5 - 0s - loss: 0.6011 - accuracy: 0.8190 - val_loss: 0.6485 - val_accuracy: 0.8028 - 75ms/epoch - 15ms/step\n",
            "Epoch 70/500\n",
            "5/5 - 0s - loss: 0.5940 - accuracy: 0.8190 - val_loss: 0.6745 - val_accuracy: 0.8028 - 75ms/epoch - 15ms/step\n",
            "Epoch 71/500\n",
            "5/5 - 0s - loss: 0.6139 - accuracy: 0.8190 - val_loss: 0.7192 - val_accuracy: 0.8028 - 71ms/epoch - 14ms/step\n",
            "Epoch 72/500\n",
            "5/5 - 0s - loss: 0.6250 - accuracy: 0.8190 - val_loss: 0.6701 - val_accuracy: 0.8028 - 80ms/epoch - 16ms/step\n",
            "Epoch 73/500\n",
            "5/5 - 0s - loss: 0.5969 - accuracy: 0.8190 - val_loss: 0.6404 - val_accuracy: 0.8028 - 75ms/epoch - 15ms/step\n",
            "Epoch 74/500\n",
            "5/5 - 0s - loss: 0.6305 - accuracy: 0.8190 - val_loss: 0.6402 - val_accuracy: 0.8028 - 68ms/epoch - 14ms/step\n",
            "Epoch 75/500\n",
            "5/5 - 0s - loss: 0.6145 - accuracy: 0.8190 - val_loss: 0.6442 - val_accuracy: 0.8028 - 57ms/epoch - 11ms/step\n",
            "Epoch 76/500\n",
            "5/5 - 0s - loss: 0.5974 - accuracy: 0.8190 - val_loss: 0.6484 - val_accuracy: 0.8028 - 65ms/epoch - 13ms/step\n",
            "Epoch 77/500\n",
            "5/5 - 0s - loss: 0.5874 - accuracy: 0.8190 - val_loss: 0.6405 - val_accuracy: 0.8028 - 74ms/epoch - 15ms/step\n",
            "Epoch 78/500\n",
            "5/5 - 0s - loss: 0.5916 - accuracy: 0.8190 - val_loss: 0.6397 - val_accuracy: 0.8028 - 58ms/epoch - 12ms/step\n",
            "Epoch 79/500\n",
            "5/5 - 0s - loss: 0.5913 - accuracy: 0.8190 - val_loss: 0.6388 - val_accuracy: 0.8028 - 74ms/epoch - 15ms/step\n",
            "Epoch 80/500\n",
            "5/5 - 0s - loss: 0.5919 - accuracy: 0.8190 - val_loss: 0.6510 - val_accuracy: 0.8028 - 77ms/epoch - 15ms/step\n",
            "Epoch 81/500\n",
            "5/5 - 0s - loss: 0.5987 - accuracy: 0.8190 - val_loss: 0.6638 - val_accuracy: 0.8028 - 81ms/epoch - 16ms/step\n",
            "Epoch 82/500\n",
            "5/5 - 0s - loss: 0.5985 - accuracy: 0.8190 - val_loss: 0.6396 - val_accuracy: 0.8028 - 92ms/epoch - 18ms/step\n",
            "Epoch 83/500\n",
            "5/5 - 0s - loss: 0.6062 - accuracy: 0.8190 - val_loss: 0.6414 - val_accuracy: 0.8028 - 59ms/epoch - 12ms/step\n",
            "Epoch 84/500\n",
            "5/5 - 0s - loss: 0.6140 - accuracy: 0.8190 - val_loss: 0.6361 - val_accuracy: 0.8028 - 78ms/epoch - 16ms/step\n",
            "Epoch 85/500\n",
            "5/5 - 0s - loss: 0.5886 - accuracy: 0.8190 - val_loss: 0.6416 - val_accuracy: 0.8028 - 61ms/epoch - 12ms/step\n",
            "Epoch 86/500\n",
            "5/5 - 0s - loss: 0.5808 - accuracy: 0.8190 - val_loss: 0.6444 - val_accuracy: 0.8028 - 85ms/epoch - 17ms/step\n",
            "Epoch 87/500\n",
            "5/5 - 0s - loss: 0.5978 - accuracy: 0.8190 - val_loss: 0.6445 - val_accuracy: 0.8028 - 83ms/epoch - 17ms/step\n",
            "Epoch 88/500\n",
            "5/5 - 0s - loss: 0.5891 - accuracy: 0.8190 - val_loss: 0.6452 - val_accuracy: 0.8028 - 60ms/epoch - 12ms/step\n",
            "Epoch 89/500\n",
            "5/5 - 0s - loss: 0.5933 - accuracy: 0.8190 - val_loss: 0.6475 - val_accuracy: 0.8028 - 61ms/epoch - 12ms/step\n",
            "Epoch 90/500\n",
            "5/5 - 0s - loss: 0.5801 - accuracy: 0.8190 - val_loss: 0.6436 - val_accuracy: 0.8028 - 83ms/epoch - 17ms/step\n",
            "Epoch 91/500\n",
            "5/5 - 0s - loss: 0.5875 - accuracy: 0.8190 - val_loss: 0.6545 - val_accuracy: 0.8028 - 60ms/epoch - 12ms/step\n",
            "Epoch 92/500\n",
            "5/5 - 0s - loss: 0.5866 - accuracy: 0.8190 - val_loss: 0.6539 - val_accuracy: 0.8028 - 77ms/epoch - 15ms/step\n",
            "Epoch 93/500\n",
            "5/5 - 0s - loss: 0.5765 - accuracy: 0.8190 - val_loss: 0.6500 - val_accuracy: 0.8028 - 60ms/epoch - 12ms/step\n",
            "Epoch 94/500\n",
            "5/5 - 0s - loss: 0.5772 - accuracy: 0.8190 - val_loss: 0.6481 - val_accuracy: 0.8028 - 69ms/epoch - 14ms/step\n",
            "Epoch 95/500\n",
            "5/5 - 0s - loss: 0.5889 - accuracy: 0.8190 - val_loss: 0.6536 - val_accuracy: 0.8028 - 57ms/epoch - 11ms/step\n",
            "Epoch 96/500\n",
            "5/5 - 0s - loss: 0.5960 - accuracy: 0.8190 - val_loss: 0.6530 - val_accuracy: 0.8028 - 64ms/epoch - 13ms/step\n",
            "Epoch 97/500\n",
            "5/5 - 0s - loss: 0.6004 - accuracy: 0.8190 - val_loss: 0.6462 - val_accuracy: 0.8028 - 74ms/epoch - 15ms/step\n",
            "Epoch 98/500\n",
            "5/5 - 0s - loss: 0.5797 - accuracy: 0.8190 - val_loss: 0.6468 - val_accuracy: 0.8028 - 62ms/epoch - 12ms/step\n",
            "Epoch 99/500\n",
            "5/5 - 0s - loss: 0.5733 - accuracy: 0.8190 - val_loss: 0.6495 - val_accuracy: 0.8028 - 63ms/epoch - 13ms/step\n",
            "Epoch 100/500\n",
            "5/5 - 0s - loss: 0.5827 - accuracy: 0.8190 - val_loss: 0.6551 - val_accuracy: 0.8028 - 67ms/epoch - 13ms/step\n",
            "Epoch 101/500\n",
            "5/5 - 0s - loss: 0.5780 - accuracy: 0.8190 - val_loss: 0.6509 - val_accuracy: 0.8028 - 86ms/epoch - 17ms/step\n",
            "Epoch 102/500\n",
            "5/5 - 0s - loss: 0.5793 - accuracy: 0.8190 - val_loss: 0.6529 - val_accuracy: 0.8028 - 67ms/epoch - 13ms/step\n",
            "Epoch 103/500\n",
            "5/5 - 0s - loss: 0.5862 - accuracy: 0.8190 - val_loss: 0.6516 - val_accuracy: 0.8028 - 63ms/epoch - 13ms/step\n",
            "Epoch 104/500\n",
            "5/5 - 0s - loss: 0.5714 - accuracy: 0.8190 - val_loss: 0.6601 - val_accuracy: 0.8028 - 71ms/epoch - 14ms/step\n",
            "Epoch 105/500\n",
            "5/5 - 0s - loss: 0.5820 - accuracy: 0.8190 - val_loss: 0.6663 - val_accuracy: 0.8028 - 78ms/epoch - 16ms/step\n",
            "Epoch 106/500\n",
            "5/5 - 0s - loss: 0.5690 - accuracy: 0.8190 - val_loss: 0.6638 - val_accuracy: 0.8028 - 56ms/epoch - 11ms/step\n",
            "Epoch 107/500\n",
            "5/5 - 0s - loss: 0.5718 - accuracy: 0.8190 - val_loss: 0.6674 - val_accuracy: 0.8028 - 59ms/epoch - 12ms/step\n",
            "Epoch 108/500\n",
            "5/5 - 0s - loss: 0.5840 - accuracy: 0.8190 - val_loss: 0.6593 - val_accuracy: 0.8028 - 63ms/epoch - 13ms/step\n",
            "Epoch 109/500\n",
            "5/5 - 0s - loss: 0.5727 - accuracy: 0.8190 - val_loss: 0.6556 - val_accuracy: 0.8028 - 81ms/epoch - 16ms/step\n",
            "Epoch 110/500\n",
            "5/5 - 0s - loss: 0.5866 - accuracy: 0.8190 - val_loss: 0.6886 - val_accuracy: 0.8028 - 61ms/epoch - 12ms/step\n",
            "Epoch 111/500\n",
            "5/5 - 0s - loss: 0.6201 - accuracy: 0.8190 - val_loss: 0.6735 - val_accuracy: 0.8028 - 70ms/epoch - 14ms/step\n",
            "Epoch 112/500\n",
            "5/5 - 0s - loss: 0.6041 - accuracy: 0.8190 - val_loss: 0.7132 - val_accuracy: 0.8028 - 66ms/epoch - 13ms/step\n",
            "Epoch 113/500\n",
            "5/5 - 0s - loss: 0.6241 - accuracy: 0.8190 - val_loss: 0.6826 - val_accuracy: 0.8028 - 79ms/epoch - 16ms/step\n",
            "Epoch 114/500\n",
            "5/5 - 0s - loss: 0.6006 - accuracy: 0.8190 - val_loss: 0.6531 - val_accuracy: 0.8028 - 58ms/epoch - 12ms/step\n",
            "Epoch 115/500\n",
            "5/5 - 0s - loss: 0.5761 - accuracy: 0.8190 - val_loss: 0.6418 - val_accuracy: 0.8028 - 70ms/epoch - 14ms/step\n",
            "Epoch 116/500\n",
            "5/5 - 0s - loss: 0.5681 - accuracy: 0.8190 - val_loss: 0.6440 - val_accuracy: 0.8028 - 77ms/epoch - 15ms/step\n",
            "Epoch 117/500\n",
            "5/5 - 0s - loss: 0.5798 - accuracy: 0.8190 - val_loss: 0.6513 - val_accuracy: 0.8028 - 59ms/epoch - 12ms/step\n",
            "Epoch 118/500\n",
            "5/5 - 0s - loss: 0.5899 - accuracy: 0.8190 - val_loss: 0.6477 - val_accuracy: 0.8028 - 61ms/epoch - 12ms/step\n",
            "Epoch 119/500\n",
            "5/5 - 0s - loss: 0.5714 - accuracy: 0.8190 - val_loss: 0.6455 - val_accuracy: 0.8028 - 77ms/epoch - 15ms/step\n",
            "Epoch 120/500\n",
            "5/5 - 0s - loss: 0.5751 - accuracy: 0.8190 - val_loss: 0.6494 - val_accuracy: 0.8028 - 72ms/epoch - 14ms/step\n",
            "Epoch 121/500\n",
            "5/5 - 0s - loss: 0.5825 - accuracy: 0.8190 - val_loss: 0.6566 - val_accuracy: 0.8028 - 72ms/epoch - 14ms/step\n",
            "Epoch 122/500\n",
            "5/5 - 0s - loss: 0.5679 - accuracy: 0.8190 - val_loss: 0.6630 - val_accuracy: 0.8028 - 78ms/epoch - 16ms/step\n",
            "Epoch 123/500\n",
            "5/5 - 0s - loss: 0.5790 - accuracy: 0.8190 - val_loss: 0.6632 - val_accuracy: 0.8028 - 66ms/epoch - 13ms/step\n",
            "Epoch 124/500\n",
            "5/5 - 0s - loss: 0.5774 - accuracy: 0.8190 - val_loss: 0.6574 - val_accuracy: 0.8028 - 69ms/epoch - 14ms/step\n",
            "Epoch 125/500\n",
            "5/5 - 0s - loss: 0.5756 - accuracy: 0.8190 - val_loss: 0.6692 - val_accuracy: 0.8028 - 60ms/epoch - 12ms/step\n",
            "Epoch 126/500\n",
            "5/5 - 0s - loss: 0.5929 - accuracy: 0.8190 - val_loss: 0.6683 - val_accuracy: 0.8028 - 80ms/epoch - 16ms/step\n",
            "Epoch 127/500\n",
            "5/5 - 0s - loss: 0.5822 - accuracy: 0.8190 - val_loss: 0.6820 - val_accuracy: 0.8028 - 94ms/epoch - 19ms/step\n",
            "Epoch 128/500\n",
            "5/5 - 0s - loss: 0.6019 - accuracy: 0.8190 - val_loss: 0.6723 - val_accuracy: 0.8028 - 119ms/epoch - 24ms/step\n",
            "Epoch 129/500\n",
            "5/5 - 0s - loss: 0.5813 - accuracy: 0.8190 - val_loss: 0.6852 - val_accuracy: 0.8028 - 107ms/epoch - 21ms/step\n",
            "Epoch 130/500\n",
            "5/5 - 0s - loss: 0.6007 - accuracy: 0.8190 - val_loss: 0.6962 - val_accuracy: 0.8028 - 97ms/epoch - 19ms/step\n",
            "Epoch 131/500\n",
            "5/5 - 0s - loss: 0.5940 - accuracy: 0.8190 - val_loss: 0.6956 - val_accuracy: 0.8028 - 99ms/epoch - 20ms/step\n",
            "Epoch 132/500\n",
            "5/5 - 0s - loss: 0.5905 - accuracy: 0.8190 - val_loss: 0.7278 - val_accuracy: 0.8028 - 94ms/epoch - 19ms/step\n",
            "Epoch 133/500\n",
            "5/5 - 0s - loss: 0.6252 - accuracy: 0.8190 - val_loss: 0.6972 - val_accuracy: 0.8028 - 98ms/epoch - 20ms/step\n",
            "Epoch 134/500\n",
            "5/5 - 0s - loss: 0.5899 - accuracy: 0.8190 - val_loss: 0.6564 - val_accuracy: 0.8028 - 101ms/epoch - 20ms/step\n",
            "Epoch 135/500\n",
            "5/5 - 0s - loss: 0.5688 - accuracy: 0.8190 - val_loss: 0.6421 - val_accuracy: 0.8028 - 97ms/epoch - 19ms/step\n",
            "Epoch 136/500\n",
            "5/5 - 0s - loss: 0.5757 - accuracy: 0.8190 - val_loss: 0.6453 - val_accuracy: 0.8028 - 100ms/epoch - 20ms/step\n",
            "Epoch 137/500\n",
            "5/5 - 0s - loss: 0.5885 - accuracy: 0.8190 - val_loss: 0.6476 - val_accuracy: 0.8028 - 91ms/epoch - 18ms/step\n",
            "Epoch 138/500\n",
            "5/5 - 0s - loss: 0.5812 - accuracy: 0.8190 - val_loss: 0.6512 - val_accuracy: 0.8028 - 91ms/epoch - 18ms/step\n",
            "Epoch 139/500\n",
            "5/5 - 0s - loss: 0.5799 - accuracy: 0.8190 - val_loss: 0.6621 - val_accuracy: 0.8028 - 95ms/epoch - 19ms/step\n",
            "Epoch 140/500\n",
            "5/5 - 0s - loss: 0.5744 - accuracy: 0.8190 - val_loss: 0.6708 - val_accuracy: 0.8028 - 94ms/epoch - 19ms/step\n",
            "Epoch 141/500\n",
            "5/5 - 0s - loss: 0.5717 - accuracy: 0.8190 - val_loss: 0.6697 - val_accuracy: 0.8028 - 74ms/epoch - 15ms/step\n",
            "Epoch 142/500\n",
            "5/5 - 0s - loss: 0.5767 - accuracy: 0.8190 - val_loss: 0.6542 - val_accuracy: 0.8028 - 99ms/epoch - 20ms/step\n",
            "Epoch 143/500\n",
            "5/5 - 0s - loss: 0.5945 - accuracy: 0.8190 - val_loss: 0.6388 - val_accuracy: 0.8028 - 93ms/epoch - 19ms/step\n",
            "Epoch 144/500\n",
            "5/5 - 0s - loss: 0.5749 - accuracy: 0.8190 - val_loss: 0.6636 - val_accuracy: 0.8028 - 88ms/epoch - 18ms/step\n",
            "Epoch 145/500\n",
            "5/5 - 0s - loss: 0.6040 - accuracy: 0.8190 - val_loss: 0.6880 - val_accuracy: 0.8028 - 99ms/epoch - 20ms/step\n",
            "Epoch 146/500\n",
            "5/5 - 0s - loss: 0.6019 - accuracy: 0.8190 - val_loss: 0.6633 - val_accuracy: 0.8028 - 99ms/epoch - 20ms/step\n",
            "Epoch 147/500\n",
            "5/5 - 0s - loss: 0.5805 - accuracy: 0.8190 - val_loss: 0.6445 - val_accuracy: 0.8028 - 85ms/epoch - 17ms/step\n",
            "Epoch 148/500\n",
            "5/5 - 0s - loss: 0.5679 - accuracy: 0.8190 - val_loss: 0.6388 - val_accuracy: 0.8028 - 96ms/epoch - 19ms/step\n",
            "Epoch 149/500\n",
            "5/5 - 0s - loss: 0.5775 - accuracy: 0.8198 - val_loss: 0.6362 - val_accuracy: 0.8028 - 101ms/epoch - 20ms/step\n",
            "Epoch 150/500\n",
            "5/5 - 0s - loss: 0.5792 - accuracy: 0.8190 - val_loss: 0.6358 - val_accuracy: 0.8028 - 89ms/epoch - 18ms/step\n",
            "Epoch 151/500\n",
            "5/5 - 0s - loss: 0.5810 - accuracy: 0.8198 - val_loss: 0.6376 - val_accuracy: 0.8028 - 99ms/epoch - 20ms/step\n",
            "Epoch 152/500\n",
            "5/5 - 0s - loss: 0.5749 - accuracy: 0.8190 - val_loss: 0.6455 - val_accuracy: 0.8028 - 81ms/epoch - 16ms/step\n",
            "Epoch 153/500\n",
            "5/5 - 0s - loss: 0.5752 - accuracy: 0.8206 - val_loss: 0.6398 - val_accuracy: 0.8028 - 99ms/epoch - 20ms/step\n",
            "Epoch 154/500\n",
            "5/5 - 0s - loss: 0.5714 - accuracy: 0.8206 - val_loss: 0.6452 - val_accuracy: 0.8028 - 91ms/epoch - 18ms/step\n",
            "Epoch 155/500\n",
            "5/5 - 0s - loss: 0.5710 - accuracy: 0.8206 - val_loss: 0.6390 - val_accuracy: 0.8028 - 99ms/epoch - 20ms/step\n",
            "Epoch 156/500\n",
            "5/5 - 0s - loss: 0.5639 - accuracy: 0.8190 - val_loss: 0.6554 - val_accuracy: 0.8028 - 94ms/epoch - 19ms/step\n",
            "Epoch 157/500\n",
            "5/5 - 0s - loss: 0.5831 - accuracy: 0.8190 - val_loss: 0.6533 - val_accuracy: 0.8028 - 90ms/epoch - 18ms/step\n",
            "Epoch 158/500\n",
            "5/5 - 0s - loss: 0.5734 - accuracy: 0.8190 - val_loss: 0.6348 - val_accuracy: 0.8028 - 291ms/epoch - 58ms/step\n",
            "Epoch 159/500\n",
            "5/5 - 0s - loss: 0.5785 - accuracy: 0.8190 - val_loss: 0.6354 - val_accuracy: 0.8028 - 113ms/epoch - 23ms/step\n",
            "Epoch 160/500\n",
            "5/5 - 0s - loss: 0.5730 - accuracy: 0.8190 - val_loss: 0.6446 - val_accuracy: 0.8028 - 78ms/epoch - 16ms/step\n",
            "Epoch 161/500\n",
            "5/5 - 0s - loss: 0.5889 - accuracy: 0.8190 - val_loss: 0.6615 - val_accuracy: 0.8028 - 71ms/epoch - 14ms/step\n",
            "Epoch 162/500\n",
            "5/5 - 0s - loss: 0.5880 - accuracy: 0.8190 - val_loss: 0.6563 - val_accuracy: 0.8028 - 73ms/epoch - 15ms/step\n",
            "Epoch 163/500\n",
            "5/5 - 0s - loss: 0.5774 - accuracy: 0.8190 - val_loss: 0.6490 - val_accuracy: 0.8028 - 57ms/epoch - 11ms/step\n",
            "Epoch 164/500\n",
            "5/5 - 0s - loss: 0.5675 - accuracy: 0.8190 - val_loss: 0.6530 - val_accuracy: 0.8028 - 57ms/epoch - 11ms/step\n",
            "Epoch 165/500\n",
            "5/5 - 0s - loss: 0.5728 - accuracy: 0.8190 - val_loss: 0.6938 - val_accuracy: 0.8028 - 59ms/epoch - 12ms/step\n",
            "Epoch 166/500\n",
            "5/5 - 0s - loss: 0.5858 - accuracy: 0.8190 - val_loss: 0.6730 - val_accuracy: 0.8028 - 57ms/epoch - 11ms/step\n",
            "Epoch 167/500\n",
            "5/5 - 0s - loss: 0.5592 - accuracy: 0.8190 - val_loss: 0.6420 - val_accuracy: 0.8028 - 57ms/epoch - 11ms/step\n",
            "Epoch 168/500\n",
            "5/5 - 0s - loss: 0.5625 - accuracy: 0.8190 - val_loss: 0.6321 - val_accuracy: 0.8028 - 90ms/epoch - 18ms/step\n",
            "Epoch 169/500\n",
            "5/5 - 0s - loss: 0.5886 - accuracy: 0.8190 - val_loss: 0.6322 - val_accuracy: 0.8028 - 81ms/epoch - 16ms/step\n",
            "Epoch 170/500\n",
            "5/5 - 0s - loss: 0.5749 - accuracy: 0.8190 - val_loss: 0.6323 - val_accuracy: 0.8028 - 75ms/epoch - 15ms/step\n",
            "Epoch 171/500\n",
            "5/5 - 0s - loss: 0.5873 - accuracy: 0.8198 - val_loss: 0.6356 - val_accuracy: 0.8028 - 58ms/epoch - 12ms/step\n",
            "Epoch 172/500\n",
            "5/5 - 0s - loss: 0.5907 - accuracy: 0.8198 - val_loss: 0.6319 - val_accuracy: 0.8028 - 97ms/epoch - 19ms/step\n",
            "Epoch 173/500\n",
            "5/5 - 0s - loss: 0.5808 - accuracy: 0.8198 - val_loss: 0.6434 - val_accuracy: 0.8028 - 59ms/epoch - 12ms/step\n",
            "Epoch 174/500\n",
            "5/5 - 0s - loss: 0.5782 - accuracy: 0.8190 - val_loss: 0.6504 - val_accuracy: 0.8028 - 76ms/epoch - 15ms/step\n",
            "Epoch 175/500\n",
            "5/5 - 0s - loss: 0.5692 - accuracy: 0.8190 - val_loss: 0.6637 - val_accuracy: 0.8028 - 74ms/epoch - 15ms/step\n",
            "Epoch 176/500\n",
            "5/5 - 0s - loss: 0.5721 - accuracy: 0.8190 - val_loss: 0.6723 - val_accuracy: 0.8028 - 75ms/epoch - 15ms/step\n",
            "Epoch 177/500\n",
            "5/5 - 0s - loss: 0.5865 - accuracy: 0.8206 - val_loss: 0.7064 - val_accuracy: 0.8028 - 60ms/epoch - 12ms/step\n",
            "Epoch 178/500\n",
            "5/5 - 0s - loss: 0.5979 - accuracy: 0.8198 - val_loss: 0.6851 - val_accuracy: 0.8028 - 58ms/epoch - 12ms/step\n",
            "Epoch 179/500\n",
            "5/5 - 0s - loss: 0.5778 - accuracy: 0.8198 - val_loss: 0.6583 - val_accuracy: 0.8028 - 57ms/epoch - 11ms/step\n",
            "Epoch 180/500\n",
            "5/5 - 0s - loss: 0.5608 - accuracy: 0.8190 - val_loss: 0.6605 - val_accuracy: 0.8028 - 56ms/epoch - 11ms/step\n",
            "Epoch 181/500\n",
            "5/5 - 0s - loss: 0.5871 - accuracy: 0.8190 - val_loss: 0.6643 - val_accuracy: 0.8028 - 75ms/epoch - 15ms/step\n",
            "Epoch 182/500\n",
            "5/5 - 0s - loss: 0.5736 - accuracy: 0.8190 - val_loss: 0.6545 - val_accuracy: 0.8028 - 56ms/epoch - 11ms/step\n",
            "Epoch 183/500\n",
            "5/5 - 0s - loss: 0.5596 - accuracy: 0.8198 - val_loss: 0.6516 - val_accuracy: 0.8028 - 72ms/epoch - 14ms/step\n",
            "Epoch 184/500\n",
            "5/5 - 0s - loss: 0.5581 - accuracy: 0.8214 - val_loss: 0.6588 - val_accuracy: 0.8028 - 73ms/epoch - 15ms/step\n",
            "Epoch 185/500\n",
            "5/5 - 0s - loss: 0.5545 - accuracy: 0.8198 - val_loss: 0.6600 - val_accuracy: 0.8028 - 72ms/epoch - 14ms/step\n",
            "Epoch 186/500\n",
            "5/5 - 0s - loss: 0.5658 - accuracy: 0.8183 - val_loss: 0.6545 - val_accuracy: 0.8028 - 61ms/epoch - 12ms/step\n",
            "Epoch 187/500\n",
            "5/5 - 0s - loss: 0.5501 - accuracy: 0.8214 - val_loss: 0.6538 - val_accuracy: 0.8028 - 73ms/epoch - 15ms/step\n",
            "Epoch 188/500\n",
            "5/5 - 0s - loss: 0.5446 - accuracy: 0.8222 - val_loss: 0.6622 - val_accuracy: 0.8028 - 60ms/epoch - 12ms/step\n",
            "Epoch 189/500\n",
            "5/5 - 0s - loss: 0.5621 - accuracy: 0.8198 - val_loss: 0.6695 - val_accuracy: 0.8028 - 58ms/epoch - 12ms/step\n",
            "Epoch 190/500\n",
            "5/5 - 0s - loss: 0.5588 - accuracy: 0.8214 - val_loss: 0.6805 - val_accuracy: 0.8028 - 75ms/epoch - 15ms/step\n",
            "Epoch 191/500\n",
            "5/5 - 0s - loss: 0.5648 - accuracy: 0.8175 - val_loss: 0.7420 - val_accuracy: 0.8028 - 57ms/epoch - 11ms/step\n",
            "Epoch 192/500\n",
            "5/5 - 0s - loss: 0.5629 - accuracy: 0.8214 - val_loss: 0.6522 - val_accuracy: 0.8028 - 54ms/epoch - 11ms/step\n",
            "Epoch 193/500\n",
            "5/5 - 0s - loss: 0.5530 - accuracy: 0.8214 - val_loss: 0.6515 - val_accuracy: 0.8028 - 75ms/epoch - 15ms/step\n",
            "Epoch 194/500\n",
            "5/5 - 0s - loss: 0.5714 - accuracy: 0.8198 - val_loss: 0.6657 - val_accuracy: 0.8028 - 62ms/epoch - 12ms/step\n",
            "Epoch 195/500\n",
            "5/5 - 0s - loss: 0.5694 - accuracy: 0.8222 - val_loss: 0.6760 - val_accuracy: 0.8028 - 75ms/epoch - 15ms/step\n",
            "Epoch 196/500\n",
            "5/5 - 0s - loss: 0.5461 - accuracy: 0.8214 - val_loss: 0.6791 - val_accuracy: 0.8028 - 73ms/epoch - 15ms/step\n",
            "Epoch 197/500\n",
            "5/5 - 0s - loss: 0.5465 - accuracy: 0.8206 - val_loss: 0.6642 - val_accuracy: 0.8028 - 88ms/epoch - 18ms/step\n",
            "Epoch 198/500\n",
            "5/5 - 0s - loss: 0.5458 - accuracy: 0.8245 - val_loss: 0.6398 - val_accuracy: 0.8028 - 77ms/epoch - 15ms/step\n",
            "Epoch 199/500\n",
            "5/5 - 0s - loss: 0.5475 - accuracy: 0.8222 - val_loss: 0.6389 - val_accuracy: 0.8028 - 57ms/epoch - 11ms/step\n",
            "Epoch 200/500\n",
            "5/5 - 0s - loss: 0.5471 - accuracy: 0.8229 - val_loss: 0.6496 - val_accuracy: 0.8028 - 75ms/epoch - 15ms/step\n",
            "Epoch 201/500\n",
            "5/5 - 0s - loss: 0.5438 - accuracy: 0.8276 - val_loss: 0.6715 - val_accuracy: 0.8028 - 72ms/epoch - 14ms/step\n",
            "Epoch 202/500\n",
            "5/5 - 0s - loss: 0.5396 - accuracy: 0.8229 - val_loss: 0.6962 - val_accuracy: 0.8028 - 71ms/epoch - 14ms/step\n",
            "Epoch 203/500\n",
            "5/5 - 0s - loss: 0.5517 - accuracy: 0.8206 - val_loss: 0.6863 - val_accuracy: 0.8169 - 82ms/epoch - 16ms/step\n",
            "Epoch 204/500\n",
            "5/5 - 0s - loss: 0.5509 - accuracy: 0.8214 - val_loss: 0.6397 - val_accuracy: 0.8028 - 58ms/epoch - 12ms/step\n",
            "Epoch 205/500\n",
            "5/5 - 0s - loss: 0.5752 - accuracy: 0.8237 - val_loss: 0.8025 - val_accuracy: 0.6761 - 60ms/epoch - 12ms/step\n",
            "Epoch 206/500\n",
            "5/5 - 0s - loss: 0.6484 - accuracy: 0.7473 - val_loss: 0.7962 - val_accuracy: 0.7535 - 59ms/epoch - 12ms/step\n",
            "Epoch 207/500\n",
            "5/5 - 0s - loss: 0.5671 - accuracy: 0.8128 - val_loss: 0.6537 - val_accuracy: 0.8028 - 76ms/epoch - 15ms/step\n",
            "Epoch 208/500\n",
            "5/5 - 0s - loss: 0.5532 - accuracy: 0.8206 - val_loss: 0.6252 - val_accuracy: 0.8028 - 103ms/epoch - 21ms/step\n",
            "Epoch 209/500\n",
            "5/5 - 0s - loss: 0.5655 - accuracy: 0.8190 - val_loss: 0.6382 - val_accuracy: 0.8028 - 58ms/epoch - 12ms/step\n",
            "Epoch 210/500\n",
            "5/5 - 0s - loss: 0.5823 - accuracy: 0.8190 - val_loss: 0.6430 - val_accuracy: 0.8028 - 90ms/epoch - 18ms/step\n",
            "Epoch 211/500\n",
            "5/5 - 0s - loss: 0.5797 - accuracy: 0.8190 - val_loss: 0.6391 - val_accuracy: 0.8028 - 57ms/epoch - 11ms/step\n",
            "Epoch 212/500\n",
            "5/5 - 0s - loss: 0.5646 - accuracy: 0.8190 - val_loss: 0.6392 - val_accuracy: 0.8028 - 53ms/epoch - 11ms/step\n",
            "Epoch 213/500\n",
            "5/5 - 0s - loss: 0.5595 - accuracy: 0.8198 - val_loss: 0.6470 - val_accuracy: 0.8028 - 71ms/epoch - 14ms/step\n",
            "Epoch 214/500\n",
            "5/5 - 0s - loss: 0.5515 - accuracy: 0.8190 - val_loss: 0.6550 - val_accuracy: 0.8028 - 87ms/epoch - 17ms/step\n",
            "Epoch 215/500\n",
            "5/5 - 0s - loss: 0.5496 - accuracy: 0.8198 - val_loss: 0.6562 - val_accuracy: 0.8028 - 57ms/epoch - 11ms/step\n",
            "Epoch 216/500\n",
            "5/5 - 0s - loss: 0.5585 - accuracy: 0.8190 - val_loss: 0.6518 - val_accuracy: 0.8028 - 61ms/epoch - 12ms/step\n",
            "Epoch 217/500\n",
            "5/5 - 0s - loss: 0.5508 - accuracy: 0.8214 - val_loss: 0.6478 - val_accuracy: 0.8028 - 59ms/epoch - 12ms/step\n",
            "Epoch 218/500\n",
            "5/5 - 0s - loss: 0.5440 - accuracy: 0.8222 - val_loss: 0.6498 - val_accuracy: 0.8028 - 59ms/epoch - 12ms/step\n",
            "Epoch 219/500\n",
            "5/5 - 0s - loss: 0.5476 - accuracy: 0.8190 - val_loss: 0.6488 - val_accuracy: 0.8028 - 75ms/epoch - 15ms/step\n",
            "Epoch 220/500\n",
            "5/5 - 0s - loss: 0.5361 - accuracy: 0.8237 - val_loss: 0.6543 - val_accuracy: 0.8028 - 74ms/epoch - 15ms/step\n",
            "Epoch 221/500\n",
            "5/5 - 0s - loss: 0.5435 - accuracy: 0.8206 - val_loss: 0.7283 - val_accuracy: 0.8028 - 75ms/epoch - 15ms/step\n",
            "Epoch 222/500\n",
            "5/5 - 0s - loss: 0.6313 - accuracy: 0.8136 - val_loss: 0.7737 - val_accuracy: 0.8028 - 72ms/epoch - 14ms/step\n",
            "Epoch 223/500\n",
            "5/5 - 0s - loss: 0.6073 - accuracy: 0.8214 - val_loss: 0.6829 - val_accuracy: 0.8028 - 83ms/epoch - 17ms/step\n",
            "Epoch 224/500\n",
            "5/5 - 0s - loss: 0.5532 - accuracy: 0.8198 - val_loss: 0.6502 - val_accuracy: 0.8028 - 77ms/epoch - 15ms/step\n",
            "Epoch 225/500\n",
            "5/5 - 0s - loss: 0.5436 - accuracy: 0.8206 - val_loss: 0.6437 - val_accuracy: 0.8028 - 54ms/epoch - 11ms/step\n",
            "Epoch 226/500\n",
            "5/5 - 0s - loss: 0.5547 - accuracy: 0.8190 - val_loss: 0.6537 - val_accuracy: 0.8028 - 69ms/epoch - 14ms/step\n",
            "Epoch 227/500\n",
            "5/5 - 0s - loss: 0.5893 - accuracy: 0.8190 - val_loss: 0.7123 - val_accuracy: 0.8028 - 76ms/epoch - 15ms/step\n",
            "Epoch 228/500\n",
            "5/5 - 0s - loss: 0.5758 - accuracy: 0.8222 - val_loss: 0.7017 - val_accuracy: 0.8028 - 57ms/epoch - 11ms/step\n",
            "Epoch 229/500\n",
            "5/5 - 0s - loss: 0.5509 - accuracy: 0.8245 - val_loss: 0.6516 - val_accuracy: 0.8028 - 73ms/epoch - 15ms/step\n",
            "Epoch 230/500\n",
            "5/5 - 0s - loss: 0.5504 - accuracy: 0.8198 - val_loss: 0.6133 - val_accuracy: 0.8028 - 104ms/epoch - 21ms/step\n",
            "Epoch 231/500\n",
            "5/5 - 0s - loss: 0.5607 - accuracy: 0.8206 - val_loss: 0.6111 - val_accuracy: 0.8028 - 82ms/epoch - 16ms/step\n",
            "Epoch 232/500\n",
            "5/5 - 0s - loss: 0.5490 - accuracy: 0.8222 - val_loss: 0.6243 - val_accuracy: 0.8028 - 72ms/epoch - 14ms/step\n",
            "Epoch 233/500\n",
            "5/5 - 0s - loss: 0.5434 - accuracy: 0.8253 - val_loss: 0.6595 - val_accuracy: 0.8028 - 67ms/epoch - 13ms/step\n",
            "Epoch 234/500\n",
            "5/5 - 0s - loss: 0.5400 - accuracy: 0.8261 - val_loss: 0.6707 - val_accuracy: 0.8028 - 58ms/epoch - 12ms/step\n",
            "Epoch 235/500\n",
            "5/5 - 0s - loss: 0.5557 - accuracy: 0.8222 - val_loss: 0.6809 - val_accuracy: 0.8028 - 77ms/epoch - 15ms/step\n",
            "Epoch 236/500\n",
            "5/5 - 0s - loss: 0.5355 - accuracy: 0.8214 - val_loss: 0.6461 - val_accuracy: 0.8028 - 65ms/epoch - 13ms/step\n",
            "Epoch 237/500\n",
            "5/5 - 0s - loss: 0.5298 - accuracy: 0.8245 - val_loss: 0.6394 - val_accuracy: 0.8028 - 62ms/epoch - 12ms/step\n",
            "Epoch 238/500\n",
            "5/5 - 0s - loss: 0.5415 - accuracy: 0.8229 - val_loss: 0.6320 - val_accuracy: 0.8028 - 71ms/epoch - 14ms/step\n",
            "Epoch 239/500\n",
            "5/5 - 0s - loss: 0.5394 - accuracy: 0.8206 - val_loss: 0.6095 - val_accuracy: 0.8028 - 93ms/epoch - 19ms/step\n",
            "Epoch 240/500\n",
            "5/5 - 0s - loss: 0.5480 - accuracy: 0.8261 - val_loss: 0.6305 - val_accuracy: 0.8028 - 61ms/epoch - 12ms/step\n",
            "Epoch 241/500\n",
            "5/5 - 0s - loss: 0.5818 - accuracy: 0.8206 - val_loss: 0.6242 - val_accuracy: 0.8028 - 58ms/epoch - 12ms/step\n",
            "Epoch 242/500\n",
            "5/5 - 0s - loss: 0.5569 - accuracy: 0.8229 - val_loss: 0.6464 - val_accuracy: 0.8028 - 55ms/epoch - 11ms/step\n",
            "Epoch 243/500\n",
            "5/5 - 0s - loss: 0.5438 - accuracy: 0.8222 - val_loss: 0.6084 - val_accuracy: 0.8028 - 78ms/epoch - 16ms/step\n",
            "Epoch 244/500\n",
            "5/5 - 0s - loss: 0.5705 - accuracy: 0.8206 - val_loss: 0.6094 - val_accuracy: 0.8028 - 62ms/epoch - 12ms/step\n",
            "Epoch 245/500\n",
            "5/5 - 0s - loss: 0.5584 - accuracy: 0.8222 - val_loss: 0.6398 - val_accuracy: 0.8028 - 59ms/epoch - 12ms/step\n",
            "Epoch 246/500\n",
            "5/5 - 0s - loss: 0.5511 - accuracy: 0.8222 - val_loss: 0.6540 - val_accuracy: 0.8028 - 70ms/epoch - 14ms/step\n",
            "Epoch 247/500\n",
            "5/5 - 0s - loss: 0.5407 - accuracy: 0.8214 - val_loss: 0.6424 - val_accuracy: 0.8028 - 72ms/epoch - 14ms/step\n",
            "Epoch 248/500\n",
            "5/5 - 0s - loss: 0.5433 - accuracy: 0.8206 - val_loss: 0.6465 - val_accuracy: 0.8028 - 73ms/epoch - 15ms/step\n",
            "Epoch 249/500\n",
            "5/5 - 0s - loss: 0.5350 - accuracy: 0.8222 - val_loss: 0.6436 - val_accuracy: 0.8028 - 73ms/epoch - 15ms/step\n",
            "Epoch 250/500\n",
            "5/5 - 0s - loss: 0.5449 - accuracy: 0.8183 - val_loss: 0.6320 - val_accuracy: 0.8028 - 85ms/epoch - 17ms/step\n",
            "Epoch 251/500\n",
            "5/5 - 0s - loss: 0.5407 - accuracy: 0.8198 - val_loss: 0.6257 - val_accuracy: 0.8028 - 72ms/epoch - 14ms/step\n",
            "Epoch 252/500\n",
            "5/5 - 0s - loss: 0.5208 - accuracy: 0.8198 - val_loss: 0.6559 - val_accuracy: 0.8099 - 71ms/epoch - 14ms/step\n",
            "Epoch 253/500\n",
            "5/5 - 0s - loss: 0.5345 - accuracy: 0.8229 - val_loss: 0.6253 - val_accuracy: 0.8028 - 80ms/epoch - 16ms/step\n",
            "Epoch 254/500\n",
            "5/5 - 0s - loss: 0.5354 - accuracy: 0.8229 - val_loss: 0.6127 - val_accuracy: 0.8028 - 73ms/epoch - 15ms/step\n",
            "Epoch 255/500\n",
            "5/5 - 0s - loss: 0.5285 - accuracy: 0.8206 - val_loss: 0.6647 - val_accuracy: 0.8028 - 77ms/epoch - 15ms/step\n",
            "Epoch 256/500\n",
            "5/5 - 0s - loss: 0.5464 - accuracy: 0.8222 - val_loss: 0.7212 - val_accuracy: 0.8028 - 57ms/epoch - 11ms/step\n",
            "Epoch 257/500\n",
            "5/5 - 0s - loss: 0.5642 - accuracy: 0.8245 - val_loss: 0.6599 - val_accuracy: 0.8028 - 58ms/epoch - 12ms/step\n",
            "Epoch 258/500\n",
            "5/5 - 0s - loss: 0.5475 - accuracy: 0.8222 - val_loss: 0.6219 - val_accuracy: 0.8028 - 73ms/epoch - 15ms/step\n",
            "Epoch 259/500\n",
            "5/5 - 0s - loss: 0.5493 - accuracy: 0.8214 - val_loss: 0.6102 - val_accuracy: 0.8028 - 72ms/epoch - 14ms/step\n",
            "Epoch 260/500\n",
            "5/5 - 0s - loss: 0.5389 - accuracy: 0.8237 - val_loss: 0.6081 - val_accuracy: 0.8028 - 82ms/epoch - 16ms/step\n",
            "Epoch 261/500\n",
            "5/5 - 0s - loss: 0.5230 - accuracy: 0.8245 - val_loss: 0.6250 - val_accuracy: 0.8099 - 76ms/epoch - 15ms/step\n",
            "Epoch 262/500\n",
            "5/5 - 0s - loss: 0.5199 - accuracy: 0.8245 - val_loss: 0.6131 - val_accuracy: 0.8099 - 59ms/epoch - 12ms/step\n",
            "Epoch 263/500\n",
            "5/5 - 0s - loss: 0.5167 - accuracy: 0.8346 - val_loss: 0.6019 - val_accuracy: 0.8099 - 115ms/epoch - 23ms/step\n",
            "Epoch 264/500\n",
            "5/5 - 0s - loss: 0.5277 - accuracy: 0.8292 - val_loss: 0.6237 - val_accuracy: 0.8169 - 76ms/epoch - 15ms/step\n",
            "Epoch 265/500\n",
            "5/5 - 0s - loss: 0.5342 - accuracy: 0.8261 - val_loss: 0.6049 - val_accuracy: 0.8028 - 73ms/epoch - 15ms/step\n",
            "Epoch 266/500\n",
            "5/5 - 0s - loss: 0.5335 - accuracy: 0.8261 - val_loss: 0.6260 - val_accuracy: 0.8028 - 77ms/epoch - 15ms/step\n",
            "Epoch 267/500\n",
            "5/5 - 0s - loss: 0.5309 - accuracy: 0.8237 - val_loss: 0.6216 - val_accuracy: 0.8028 - 76ms/epoch - 15ms/step\n",
            "Epoch 268/500\n",
            "5/5 - 0s - loss: 0.5316 - accuracy: 0.8222 - val_loss: 0.6083 - val_accuracy: 0.8028 - 72ms/epoch - 14ms/step\n",
            "Epoch 269/500\n",
            "5/5 - 0s - loss: 0.5390 - accuracy: 0.8237 - val_loss: 0.6209 - val_accuracy: 0.8028 - 61ms/epoch - 12ms/step\n",
            "Epoch 270/500\n",
            "5/5 - 0s - loss: 0.5339 - accuracy: 0.8261 - val_loss: 0.6781 - val_accuracy: 0.8169 - 66ms/epoch - 13ms/step\n",
            "Epoch 271/500\n",
            "5/5 - 0s - loss: 0.5414 - accuracy: 0.8222 - val_loss: 0.6221 - val_accuracy: 0.8028 - 72ms/epoch - 14ms/step\n",
            "Epoch 272/500\n",
            "5/5 - 0s - loss: 0.5335 - accuracy: 0.8276 - val_loss: 0.6176 - val_accuracy: 0.8028 - 75ms/epoch - 15ms/step\n",
            "Epoch 273/500\n",
            "5/5 - 0s - loss: 0.5876 - accuracy: 0.8222 - val_loss: 0.6134 - val_accuracy: 0.8028 - 57ms/epoch - 11ms/step\n",
            "Epoch 274/500\n",
            "5/5 - 0s - loss: 0.5537 - accuracy: 0.8245 - val_loss: 0.6416 - val_accuracy: 0.8028 - 58ms/epoch - 12ms/step\n",
            "Epoch 275/500\n",
            "5/5 - 0s - loss: 0.5501 - accuracy: 0.8253 - val_loss: 0.6676 - val_accuracy: 0.8028 - 61ms/epoch - 12ms/step\n",
            "Epoch 276/500\n",
            "5/5 - 0s - loss: 0.5412 - accuracy: 0.8245 - val_loss: 0.6356 - val_accuracy: 0.8028 - 82ms/epoch - 16ms/step\n",
            "Epoch 277/500\n",
            "5/5 - 0s - loss: 0.5373 - accuracy: 0.8245 - val_loss: 0.6209 - val_accuracy: 0.8028 - 54ms/epoch - 11ms/step\n",
            "Epoch 278/500\n",
            "5/5 - 0s - loss: 0.5421 - accuracy: 0.8253 - val_loss: 0.6146 - val_accuracy: 0.8028 - 71ms/epoch - 14ms/step\n",
            "Epoch 279/500\n",
            "5/5 - 0s - loss: 0.5381 - accuracy: 0.8268 - val_loss: 0.6193 - val_accuracy: 0.8099 - 58ms/epoch - 12ms/step\n",
            "Epoch 280/500\n",
            "5/5 - 0s - loss: 0.5259 - accuracy: 0.8237 - val_loss: 0.6761 - val_accuracy: 0.8099 - 82ms/epoch - 16ms/step\n",
            "Epoch 281/500\n",
            "5/5 - 0s - loss: 0.5375 - accuracy: 0.8245 - val_loss: 0.6523 - val_accuracy: 0.8099 - 58ms/epoch - 12ms/step\n",
            "Epoch 282/500\n",
            "5/5 - 0s - loss: 0.5202 - accuracy: 0.8268 - val_loss: 0.6249 - val_accuracy: 0.8028 - 75ms/epoch - 15ms/step\n",
            "Epoch 283/500\n",
            "5/5 - 0s - loss: 0.5216 - accuracy: 0.8268 - val_loss: 0.6233 - val_accuracy: 0.8028 - 63ms/epoch - 13ms/step\n",
            "Epoch 284/500\n",
            "5/5 - 0s - loss: 0.5242 - accuracy: 0.8253 - val_loss: 0.6189 - val_accuracy: 0.8028 - 75ms/epoch - 15ms/step\n",
            "Epoch 285/500\n",
            "5/5 - 0s - loss: 0.5192 - accuracy: 0.8222 - val_loss: 0.6017 - val_accuracy: 0.8028 - 82ms/epoch - 16ms/step\n",
            "Epoch 286/500\n",
            "5/5 - 0s - loss: 0.5287 - accuracy: 0.8261 - val_loss: 0.6205 - val_accuracy: 0.8028 - 58ms/epoch - 12ms/step\n",
            "Epoch 287/500\n",
            "5/5 - 0s - loss: 0.5565 - accuracy: 0.8268 - val_loss: 0.6232 - val_accuracy: 0.8028 - 79ms/epoch - 16ms/step\n",
            "Epoch 288/500\n",
            "5/5 - 0s - loss: 0.5453 - accuracy: 0.8300 - val_loss: 0.6023 - val_accuracy: 0.8099 - 60ms/epoch - 12ms/step\n",
            "Epoch 289/500\n",
            "5/5 - 0s - loss: 0.5166 - accuracy: 0.8253 - val_loss: 0.6209 - val_accuracy: 0.8169 - 60ms/epoch - 12ms/step\n",
            "Epoch 290/500\n",
            "5/5 - 0s - loss: 0.5294 - accuracy: 0.8315 - val_loss: 0.6683 - val_accuracy: 0.7817 - 62ms/epoch - 12ms/step\n",
            "Epoch 291/500\n",
            "5/5 - 0s - loss: 0.5219 - accuracy: 0.8253 - val_loss: 0.5993 - val_accuracy: 0.8099 - 79ms/epoch - 16ms/step\n",
            "Epoch 292/500\n",
            "5/5 - 0s - loss: 0.5389 - accuracy: 0.8300 - val_loss: 0.6102 - val_accuracy: 0.8028 - 66ms/epoch - 13ms/step\n",
            "Epoch 293/500\n",
            "5/5 - 0s - loss: 0.5531 - accuracy: 0.8229 - val_loss: 0.6073 - val_accuracy: 0.8028 - 96ms/epoch - 19ms/step\n",
            "Epoch 294/500\n",
            "5/5 - 0s - loss: 0.5424 - accuracy: 0.8276 - val_loss: 0.6035 - val_accuracy: 0.8028 - 81ms/epoch - 16ms/step\n",
            "Epoch 295/500\n",
            "5/5 - 0s - loss: 0.5248 - accuracy: 0.8237 - val_loss: 0.6126 - val_accuracy: 0.8028 - 89ms/epoch - 18ms/step\n",
            "Epoch 296/500\n",
            "5/5 - 0s - loss: 0.5097 - accuracy: 0.8284 - val_loss: 0.6354 - val_accuracy: 0.8028 - 88ms/epoch - 18ms/step\n",
            "Epoch 297/500\n",
            "5/5 - 0s - loss: 0.5110 - accuracy: 0.8253 - val_loss: 0.6350 - val_accuracy: 0.8028 - 100ms/epoch - 20ms/step\n",
            "Epoch 298/500\n",
            "5/5 - 0s - loss: 0.5088 - accuracy: 0.8307 - val_loss: 0.6266 - val_accuracy: 0.8028 - 105ms/epoch - 21ms/step\n",
            "Epoch 299/500\n",
            "5/5 - 0s - loss: 0.5181 - accuracy: 0.8261 - val_loss: 0.6225 - val_accuracy: 0.8028 - 80ms/epoch - 16ms/step\n",
            "Epoch 300/500\n",
            "5/5 - 0s - loss: 0.5099 - accuracy: 0.8331 - val_loss: 0.6197 - val_accuracy: 0.8028 - 92ms/epoch - 18ms/step\n",
            "Epoch 301/500\n",
            "5/5 - 0s - loss: 0.5084 - accuracy: 0.8292 - val_loss: 0.6405 - val_accuracy: 0.8099 - 104ms/epoch - 21ms/step\n",
            "Epoch 302/500\n",
            "5/5 - 0s - loss: 0.5319 - accuracy: 0.8276 - val_loss: 0.6865 - val_accuracy: 0.7887 - 80ms/epoch - 16ms/step\n",
            "Epoch 303/500\n",
            "5/5 - 0s - loss: 0.5068 - accuracy: 0.8315 - val_loss: 0.6072 - val_accuracy: 0.8099 - 96ms/epoch - 19ms/step\n",
            "Epoch 304/500\n",
            "5/5 - 0s - loss: 0.5242 - accuracy: 0.8261 - val_loss: 0.6080 - val_accuracy: 0.8028 - 85ms/epoch - 17ms/step\n",
            "Epoch 305/500\n",
            "5/5 - 0s - loss: 0.5154 - accuracy: 0.8245 - val_loss: 0.6338 - val_accuracy: 0.8099 - 97ms/epoch - 19ms/step\n",
            "Epoch 306/500\n",
            "5/5 - 0s - loss: 0.5167 - accuracy: 0.8237 - val_loss: 0.6242 - val_accuracy: 0.8099 - 97ms/epoch - 19ms/step\n",
            "Epoch 307/500\n",
            "5/5 - 0s - loss: 0.5095 - accuracy: 0.8245 - val_loss: 0.6158 - val_accuracy: 0.8028 - 86ms/epoch - 17ms/step\n",
            "Epoch 308/500\n",
            "5/5 - 0s - loss: 0.5024 - accuracy: 0.8261 - val_loss: 0.6334 - val_accuracy: 0.8028 - 108ms/epoch - 22ms/step\n",
            "Epoch 309/500\n",
            "5/5 - 0s - loss: 0.5140 - accuracy: 0.8253 - val_loss: 0.6461 - val_accuracy: 0.8028 - 97ms/epoch - 19ms/step\n",
            "Epoch 310/500\n",
            "5/5 - 0s - loss: 0.5077 - accuracy: 0.8253 - val_loss: 0.6437 - val_accuracy: 0.8028 - 98ms/epoch - 20ms/step\n",
            "Epoch 311/500\n",
            "5/5 - 0s - loss: 0.5136 - accuracy: 0.8268 - val_loss: 0.6362 - val_accuracy: 0.8028 - 108ms/epoch - 22ms/step\n",
            "Epoch 312/500\n",
            "5/5 - 0s - loss: 0.4975 - accuracy: 0.8315 - val_loss: 0.6325 - val_accuracy: 0.8028 - 93ms/epoch - 19ms/step\n",
            "Epoch 313/500\n",
            "5/5 - 0s - loss: 0.5108 - accuracy: 0.8253 - val_loss: 0.6336 - val_accuracy: 0.8028 - 90ms/epoch - 18ms/step\n",
            "Epoch 314/500\n",
            "5/5 - 0s - loss: 0.4916 - accuracy: 0.8315 - val_loss: 0.6363 - val_accuracy: 0.8099 - 107ms/epoch - 21ms/step\n",
            "Epoch 315/500\n",
            "5/5 - 0s - loss: 0.4903 - accuracy: 0.8346 - val_loss: 0.6445 - val_accuracy: 0.8099 - 103ms/epoch - 21ms/step\n",
            "Epoch 316/500\n",
            "5/5 - 0s - loss: 0.5063 - accuracy: 0.8190 - val_loss: 0.6261 - val_accuracy: 0.8099 - 103ms/epoch - 21ms/step\n",
            "Epoch 317/500\n",
            "5/5 - 0s - loss: 0.5114 - accuracy: 0.8276 - val_loss: 0.6333 - val_accuracy: 0.8099 - 93ms/epoch - 19ms/step\n",
            "Epoch 318/500\n",
            "5/5 - 0s - loss: 0.5201 - accuracy: 0.8276 - val_loss: 0.6286 - val_accuracy: 0.8099 - 90ms/epoch - 18ms/step\n",
            "Epoch 319/500\n",
            "5/5 - 0s - loss: 0.4914 - accuracy: 0.8339 - val_loss: 0.6267 - val_accuracy: 0.8099 - 81ms/epoch - 16ms/step\n",
            "Epoch 320/500\n",
            "5/5 - 0s - loss: 0.4965 - accuracy: 0.8370 - val_loss: 0.6089 - val_accuracy: 0.8169 - 90ms/epoch - 18ms/step\n",
            "Epoch 321/500\n",
            "5/5 - 0s - loss: 0.4945 - accuracy: 0.8315 - val_loss: 0.6138 - val_accuracy: 0.8169 - 89ms/epoch - 18ms/step\n",
            "Epoch 322/500\n",
            "5/5 - 0s - loss: 0.5064 - accuracy: 0.8253 - val_loss: 0.6092 - val_accuracy: 0.8099 - 117ms/epoch - 23ms/step\n",
            "Epoch 323/500\n",
            "5/5 - 0s - loss: 0.5175 - accuracy: 0.8276 - val_loss: 0.6305 - val_accuracy: 0.8099 - 99ms/epoch - 20ms/step\n",
            "Epoch 324/500\n",
            "5/5 - 0s - loss: 0.5051 - accuracy: 0.8292 - val_loss: 0.6288 - val_accuracy: 0.8099 - 102ms/epoch - 20ms/step\n",
            "Epoch 325/500\n",
            "5/5 - 0s - loss: 0.5000 - accuracy: 0.8292 - val_loss: 0.6321 - val_accuracy: 0.8169 - 83ms/epoch - 17ms/step\n",
            "Epoch 326/500\n",
            "5/5 - 0s - loss: 0.4896 - accuracy: 0.8370 - val_loss: 0.6322 - val_accuracy: 0.8169 - 97ms/epoch - 19ms/step\n",
            "Epoch 327/500\n",
            "5/5 - 0s - loss: 0.4908 - accuracy: 0.8339 - val_loss: 0.6145 - val_accuracy: 0.8169 - 105ms/epoch - 21ms/step\n",
            "Epoch 328/500\n",
            "5/5 - 0s - loss: 0.4956 - accuracy: 0.8346 - val_loss: 0.5958 - val_accuracy: 0.8099 - 114ms/epoch - 23ms/step\n",
            "Epoch 329/500\n",
            "5/5 - 0s - loss: 0.4928 - accuracy: 0.8323 - val_loss: 0.6179 - val_accuracy: 0.8099 - 76ms/epoch - 15ms/step\n",
            "Epoch 330/500\n",
            "5/5 - 0s - loss: 0.4897 - accuracy: 0.8385 - val_loss: 0.6282 - val_accuracy: 0.8169 - 55ms/epoch - 11ms/step\n",
            "Epoch 331/500\n",
            "5/5 - 0s - loss: 0.4972 - accuracy: 0.8315 - val_loss: 0.6554 - val_accuracy: 0.8099 - 56ms/epoch - 11ms/step\n",
            "Epoch 332/500\n",
            "5/5 - 0s - loss: 0.5364 - accuracy: 0.8198 - val_loss: 0.7090 - val_accuracy: 0.8028 - 57ms/epoch - 11ms/step\n",
            "Epoch 333/500\n",
            "5/5 - 0s - loss: 0.5514 - accuracy: 0.8198 - val_loss: 0.6580 - val_accuracy: 0.8028 - 81ms/epoch - 16ms/step\n",
            "Epoch 334/500\n",
            "5/5 - 0s - loss: 0.5273 - accuracy: 0.8253 - val_loss: 0.6524 - val_accuracy: 0.8028 - 61ms/epoch - 12ms/step\n",
            "Epoch 335/500\n",
            "5/5 - 0s - loss: 0.5307 - accuracy: 0.8237 - val_loss: 0.6338 - val_accuracy: 0.8028 - 59ms/epoch - 12ms/step\n",
            "Epoch 336/500\n",
            "5/5 - 0s - loss: 0.5175 - accuracy: 0.8261 - val_loss: 0.6007 - val_accuracy: 0.8028 - 56ms/epoch - 11ms/step\n",
            "Epoch 337/500\n",
            "5/5 - 0s - loss: 0.5098 - accuracy: 0.8268 - val_loss: 0.6003 - val_accuracy: 0.8028 - 61ms/epoch - 12ms/step\n",
            "Epoch 338/500\n",
            "5/5 - 0s - loss: 0.5229 - accuracy: 0.8300 - val_loss: 0.6421 - val_accuracy: 0.8099 - 58ms/epoch - 12ms/step\n",
            "Epoch 339/500\n",
            "5/5 - 0s - loss: 0.5176 - accuracy: 0.8346 - val_loss: 0.6372 - val_accuracy: 0.8099 - 59ms/epoch - 12ms/step\n",
            "Epoch 340/500\n",
            "5/5 - 0s - loss: 0.5040 - accuracy: 0.8284 - val_loss: 0.6236 - val_accuracy: 0.8169 - 73ms/epoch - 15ms/step\n",
            "Epoch 341/500\n",
            "5/5 - 0s - loss: 0.5042 - accuracy: 0.8253 - val_loss: 0.6043 - val_accuracy: 0.8099 - 56ms/epoch - 11ms/step\n",
            "Epoch 342/500\n",
            "5/5 - 0s - loss: 0.4985 - accuracy: 0.8315 - val_loss: 0.6057 - val_accuracy: 0.8028 - 72ms/epoch - 14ms/step\n",
            "Epoch 343/500\n",
            "5/5 - 0s - loss: 0.5251 - accuracy: 0.8292 - val_loss: 0.6143 - val_accuracy: 0.8099 - 74ms/epoch - 15ms/step\n",
            "Epoch 344/500\n",
            "5/5 - 0s - loss: 0.5062 - accuracy: 0.8284 - val_loss: 0.6242 - val_accuracy: 0.8099 - 60ms/epoch - 12ms/step\n",
            "Epoch 345/500\n",
            "5/5 - 0s - loss: 0.4992 - accuracy: 0.8300 - val_loss: 0.6115 - val_accuracy: 0.8099 - 65ms/epoch - 13ms/step\n",
            "Epoch 346/500\n",
            "5/5 - 0s - loss: 0.4931 - accuracy: 0.8307 - val_loss: 0.6042 - val_accuracy: 0.8099 - 57ms/epoch - 11ms/step\n",
            "Epoch 347/500\n",
            "5/5 - 0s - loss: 0.4829 - accuracy: 0.8346 - val_loss: 0.6096 - val_accuracy: 0.8099 - 74ms/epoch - 15ms/step\n",
            "Epoch 348/500\n",
            "5/5 - 0s - loss: 0.4951 - accuracy: 0.8292 - val_loss: 0.6065 - val_accuracy: 0.8028 - 62ms/epoch - 12ms/step\n",
            "Epoch 349/500\n",
            "5/5 - 0s - loss: 0.4915 - accuracy: 0.8261 - val_loss: 0.6049 - val_accuracy: 0.8028 - 64ms/epoch - 13ms/step\n",
            "Epoch 350/500\n",
            "5/5 - 0s - loss: 0.4923 - accuracy: 0.8292 - val_loss: 0.6091 - val_accuracy: 0.8028 - 54ms/epoch - 11ms/step\n",
            "Epoch 351/500\n",
            "5/5 - 0s - loss: 0.4980 - accuracy: 0.8292 - val_loss: 0.6066 - val_accuracy: 0.8028 - 54ms/epoch - 11ms/step\n",
            "Epoch 352/500\n",
            "5/5 - 0s - loss: 0.4935 - accuracy: 0.8229 - val_loss: 0.6107 - val_accuracy: 0.8028 - 62ms/epoch - 12ms/step\n",
            "Epoch 353/500\n",
            "5/5 - 0s - loss: 0.5057 - accuracy: 0.8237 - val_loss: 0.6084 - val_accuracy: 0.8028 - 80ms/epoch - 16ms/step\n",
            "Epoch 354/500\n",
            "5/5 - 0s - loss: 0.4943 - accuracy: 0.8315 - val_loss: 0.6545 - val_accuracy: 0.8169 - 60ms/epoch - 12ms/step\n",
            "Epoch 355/500\n",
            "5/5 - 0s - loss: 0.5837 - accuracy: 0.8105 - val_loss: 0.7547 - val_accuracy: 0.8099 - 76ms/epoch - 15ms/step\n",
            "Epoch 356/500\n",
            "5/5 - 0s - loss: 0.5594 - accuracy: 0.8222 - val_loss: 0.6175 - val_accuracy: 0.8028 - 62ms/epoch - 12ms/step\n",
            "Epoch 357/500\n",
            "5/5 - 0s - loss: 0.5224 - accuracy: 0.8214 - val_loss: 0.5996 - val_accuracy: 0.8028 - 55ms/epoch - 11ms/step\n",
            "Epoch 358/500\n",
            "5/5 - 0s - loss: 0.5344 - accuracy: 0.8206 - val_loss: 0.6155 - val_accuracy: 0.8028 - 72ms/epoch - 14ms/step\n",
            "Epoch 359/500\n",
            "5/5 - 0s - loss: 0.5273 - accuracy: 0.8214 - val_loss: 0.6087 - val_accuracy: 0.8028 - 73ms/epoch - 15ms/step\n",
            "Epoch 360/500\n",
            "5/5 - 0s - loss: 0.5232 - accuracy: 0.8206 - val_loss: 0.6290 - val_accuracy: 0.8028 - 65ms/epoch - 13ms/step\n",
            "Epoch 361/500\n",
            "5/5 - 0s - loss: 0.5227 - accuracy: 0.8206 - val_loss: 0.6502 - val_accuracy: 0.8028 - 55ms/epoch - 11ms/step\n",
            "Epoch 362/500\n",
            "5/5 - 0s - loss: 0.5189 - accuracy: 0.8214 - val_loss: 0.6352 - val_accuracy: 0.8028 - 81ms/epoch - 16ms/step\n",
            "Epoch 363/500\n",
            "5/5 - 0s - loss: 0.5013 - accuracy: 0.8237 - val_loss: 0.6172 - val_accuracy: 0.8028 - 76ms/epoch - 15ms/step\n",
            "Epoch 364/500\n",
            "5/5 - 0s - loss: 0.4996 - accuracy: 0.8268 - val_loss: 0.6097 - val_accuracy: 0.8028 - 79ms/epoch - 16ms/step\n",
            "Epoch 365/500\n",
            "5/5 - 0s - loss: 0.5053 - accuracy: 0.8276 - val_loss: 0.6114 - val_accuracy: 0.8028 - 72ms/epoch - 14ms/step\n",
            "Epoch 366/500\n",
            "5/5 - 0s - loss: 0.5244 - accuracy: 0.8253 - val_loss: 0.6323 - val_accuracy: 0.8028 - 80ms/epoch - 16ms/step\n",
            "Epoch 367/500\n",
            "5/5 - 0s - loss: 0.5044 - accuracy: 0.8253 - val_loss: 0.6458 - val_accuracy: 0.8028 - 59ms/epoch - 12ms/step\n",
            "Epoch 368/500\n",
            "5/5 - 0s - loss: 0.5022 - accuracy: 0.8268 - val_loss: 0.6460 - val_accuracy: 0.8028 - 60ms/epoch - 12ms/step\n",
            "Epoch 369/500\n",
            "5/5 - 0s - loss: 0.4930 - accuracy: 0.8253 - val_loss: 0.6297 - val_accuracy: 0.8028 - 78ms/epoch - 16ms/step\n",
            "Epoch 370/500\n",
            "5/5 - 0s - loss: 0.4946 - accuracy: 0.8300 - val_loss: 0.6130 - val_accuracy: 0.8028 - 58ms/epoch - 12ms/step\n",
            "Epoch 371/500\n",
            "5/5 - 0s - loss: 0.5017 - accuracy: 0.8331 - val_loss: 0.6513 - val_accuracy: 0.8028 - 56ms/epoch - 11ms/step\n",
            "Epoch 372/500\n",
            "5/5 - 0s - loss: 0.4964 - accuracy: 0.8331 - val_loss: 0.6253 - val_accuracy: 0.8028 - 55ms/epoch - 11ms/step\n",
            "Epoch 373/500\n",
            "5/5 - 0s - loss: 0.5003 - accuracy: 0.8253 - val_loss: 0.6036 - val_accuracy: 0.8028 - 56ms/epoch - 11ms/step\n",
            "Epoch 374/500\n",
            "5/5 - 0s - loss: 0.5107 - accuracy: 0.8222 - val_loss: 0.6072 - val_accuracy: 0.8028 - 74ms/epoch - 15ms/step\n",
            "Epoch 375/500\n",
            "5/5 - 0s - loss: 0.4989 - accuracy: 0.8261 - val_loss: 0.6169 - val_accuracy: 0.8028 - 68ms/epoch - 14ms/step\n",
            "Epoch 376/500\n",
            "5/5 - 0s - loss: 0.4918 - accuracy: 0.8276 - val_loss: 0.6146 - val_accuracy: 0.8028 - 58ms/epoch - 12ms/step\n",
            "Epoch 377/500\n",
            "5/5 - 0s - loss: 0.4988 - accuracy: 0.8284 - val_loss: 0.6093 - val_accuracy: 0.8099 - 75ms/epoch - 15ms/step\n",
            "Epoch 378/500\n",
            "5/5 - 0s - loss: 0.4999 - accuracy: 0.8237 - val_loss: 0.6300 - val_accuracy: 0.7958 - 64ms/epoch - 13ms/step\n",
            "Epoch 379/500\n",
            "5/5 - 0s - loss: 0.5105 - accuracy: 0.8105 - val_loss: 0.6684 - val_accuracy: 0.7817 - 74ms/epoch - 15ms/step\n",
            "Epoch 380/500\n",
            "5/5 - 0s - loss: 0.4961 - accuracy: 0.8105 - val_loss: 0.6063 - val_accuracy: 0.8028 - 74ms/epoch - 15ms/step\n",
            "Epoch 381/500\n",
            "5/5 - 0s - loss: 0.4893 - accuracy: 0.8229 - val_loss: 0.6015 - val_accuracy: 0.8169 - 68ms/epoch - 14ms/step\n",
            "Epoch 382/500\n",
            "5/5 - 0s - loss: 0.4993 - accuracy: 0.8276 - val_loss: 0.6150 - val_accuracy: 0.8169 - 59ms/epoch - 12ms/step\n",
            "Epoch 383/500\n",
            "5/5 - 0s - loss: 0.4999 - accuracy: 0.8300 - val_loss: 0.6215 - val_accuracy: 0.8169 - 59ms/epoch - 12ms/step\n",
            "Epoch 384/500\n",
            "5/5 - 0s - loss: 0.5247 - accuracy: 0.8066 - val_loss: 0.7110 - val_accuracy: 0.7887 - 60ms/epoch - 12ms/step\n",
            "Epoch 385/500\n",
            "5/5 - 0s - loss: 0.5331 - accuracy: 0.8190 - val_loss: 0.6535 - val_accuracy: 0.8028 - 64ms/epoch - 13ms/step\n",
            "Epoch 386/500\n",
            "5/5 - 0s - loss: 0.5084 - accuracy: 0.8245 - val_loss: 0.6040 - val_accuracy: 0.8028 - 59ms/epoch - 12ms/step\n",
            "Epoch 387/500\n",
            "5/5 - 0s - loss: 0.5003 - accuracy: 0.8229 - val_loss: 0.5918 - val_accuracy: 0.8028 - 82ms/epoch - 16ms/step\n",
            "Epoch 388/500\n",
            "5/5 - 0s - loss: 0.5050 - accuracy: 0.8237 - val_loss: 0.6113 - val_accuracy: 0.8028 - 57ms/epoch - 11ms/step\n",
            "Epoch 389/500\n",
            "5/5 - 0s - loss: 0.5026 - accuracy: 0.8245 - val_loss: 0.6386 - val_accuracy: 0.8028 - 78ms/epoch - 16ms/step\n",
            "Epoch 390/500\n",
            "5/5 - 0s - loss: 0.4992 - accuracy: 0.8245 - val_loss: 0.6486 - val_accuracy: 0.8028 - 76ms/epoch - 15ms/step\n",
            "Epoch 391/500\n",
            "5/5 - 0s - loss: 0.4918 - accuracy: 0.8292 - val_loss: 0.6414 - val_accuracy: 0.8099 - 56ms/epoch - 11ms/step\n",
            "Epoch 392/500\n",
            "5/5 - 0s - loss: 0.4856 - accuracy: 0.8268 - val_loss: 0.6240 - val_accuracy: 0.8099 - 77ms/epoch - 15ms/step\n",
            "Epoch 393/500\n",
            "5/5 - 0s - loss: 0.4833 - accuracy: 0.8307 - val_loss: 0.6062 - val_accuracy: 0.8099 - 59ms/epoch - 12ms/step\n",
            "Epoch 394/500\n",
            "5/5 - 0s - loss: 0.4858 - accuracy: 0.8229 - val_loss: 0.6149 - val_accuracy: 0.8169 - 56ms/epoch - 11ms/step\n",
            "Epoch 395/500\n",
            "5/5 - 0s - loss: 0.4923 - accuracy: 0.8237 - val_loss: 0.7262 - val_accuracy: 0.7535 - 78ms/epoch - 16ms/step\n",
            "Epoch 396/500\n",
            "5/5 - 0s - loss: 0.5433 - accuracy: 0.8050 - val_loss: 0.6504 - val_accuracy: 0.8099 - 78ms/epoch - 16ms/step\n",
            "Epoch 397/500\n",
            "5/5 - 0s - loss: 0.4883 - accuracy: 0.8284 - val_loss: 0.5829 - val_accuracy: 0.8169 - 98ms/epoch - 20ms/step\n",
            "Epoch 398/500\n",
            "5/5 - 0s - loss: 0.5202 - accuracy: 0.8261 - val_loss: 0.5812 - val_accuracy: 0.7958 - 82ms/epoch - 16ms/step\n",
            "Epoch 399/500\n",
            "5/5 - 0s - loss: 0.5139 - accuracy: 0.8222 - val_loss: 0.7803 - val_accuracy: 0.7465 - 57ms/epoch - 11ms/step\n",
            "Epoch 400/500\n",
            "5/5 - 0s - loss: 0.6009 - accuracy: 0.7878 - val_loss: 0.7209 - val_accuracy: 0.8099 - 54ms/epoch - 11ms/step\n",
            "Epoch 401/500\n",
            "5/5 - 0s - loss: 0.5270 - accuracy: 0.8245 - val_loss: 0.5785 - val_accuracy: 0.8099 - 101ms/epoch - 20ms/step\n",
            "Epoch 402/500\n",
            "5/5 - 0s - loss: 0.5404 - accuracy: 0.8245 - val_loss: 0.5861 - val_accuracy: 0.8028 - 70ms/epoch - 14ms/step\n",
            "Epoch 403/500\n",
            "5/5 - 0s - loss: 0.5370 - accuracy: 0.8237 - val_loss: 0.5980 - val_accuracy: 0.8028 - 66ms/epoch - 13ms/step\n",
            "Epoch 404/500\n",
            "5/5 - 0s - loss: 0.5250 - accuracy: 0.8253 - val_loss: 0.6182 - val_accuracy: 0.8028 - 58ms/epoch - 12ms/step\n",
            "Epoch 405/500\n",
            "5/5 - 0s - loss: 0.5152 - accuracy: 0.8222 - val_loss: 0.6301 - val_accuracy: 0.8028 - 58ms/epoch - 12ms/step\n",
            "Epoch 406/500\n",
            "5/5 - 0s - loss: 0.5030 - accuracy: 0.8237 - val_loss: 0.6310 - val_accuracy: 0.8028 - 73ms/epoch - 15ms/step\n",
            "Epoch 407/500\n",
            "5/5 - 0s - loss: 0.5063 - accuracy: 0.8214 - val_loss: 0.6311 - val_accuracy: 0.8099 - 63ms/epoch - 13ms/step\n",
            "Epoch 408/500\n",
            "5/5 - 0s - loss: 0.5120 - accuracy: 0.8261 - val_loss: 0.6225 - val_accuracy: 0.8099 - 72ms/epoch - 14ms/step\n",
            "Epoch 409/500\n",
            "5/5 - 0s - loss: 0.4946 - accuracy: 0.8292 - val_loss: 0.6171 - val_accuracy: 0.8099 - 57ms/epoch - 11ms/step\n",
            "Epoch 410/500\n",
            "5/5 - 0s - loss: 0.4988 - accuracy: 0.8245 - val_loss: 0.6151 - val_accuracy: 0.8169 - 76ms/epoch - 15ms/step\n",
            "Epoch 411/500\n",
            "5/5 - 0s - loss: 0.4856 - accuracy: 0.8307 - val_loss: 0.6224 - val_accuracy: 0.8169 - 78ms/epoch - 16ms/step\n",
            "Epoch 412/500\n",
            "5/5 - 0s - loss: 0.4940 - accuracy: 0.8339 - val_loss: 0.6297 - val_accuracy: 0.8099 - 58ms/epoch - 12ms/step\n",
            "Epoch 413/500\n",
            "5/5 - 0s - loss: 0.4961 - accuracy: 0.8276 - val_loss: 0.6262 - val_accuracy: 0.8099 - 63ms/epoch - 13ms/step\n",
            "Epoch 414/500\n",
            "5/5 - 0s - loss: 0.4813 - accuracy: 0.8339 - val_loss: 0.6264 - val_accuracy: 0.8099 - 63ms/epoch - 13ms/step\n",
            "Epoch 415/500\n",
            "5/5 - 0s - loss: 0.4765 - accuracy: 0.8276 - val_loss: 0.6365 - val_accuracy: 0.8169 - 66ms/epoch - 13ms/step\n",
            "Epoch 416/500\n",
            "5/5 - 0s - loss: 0.4887 - accuracy: 0.8346 - val_loss: 0.6350 - val_accuracy: 0.8028 - 81ms/epoch - 16ms/step\n",
            "Epoch 417/500\n",
            "5/5 - 0s - loss: 0.4829 - accuracy: 0.8339 - val_loss: 0.6713 - val_accuracy: 0.8028 - 55ms/epoch - 11ms/step\n",
            "Epoch 418/500\n",
            "5/5 - 0s - loss: 0.5182 - accuracy: 0.8315 - val_loss: 0.6794 - val_accuracy: 0.8028 - 52ms/epoch - 10ms/step\n",
            "Epoch 419/500\n",
            "5/5 - 0s - loss: 0.5108 - accuracy: 0.8261 - val_loss: 0.6489 - val_accuracy: 0.8169 - 77ms/epoch - 15ms/step\n",
            "Epoch 420/500\n",
            "5/5 - 0s - loss: 0.4949 - accuracy: 0.8307 - val_loss: 0.6066 - val_accuracy: 0.8099 - 60ms/epoch - 12ms/step\n",
            "Epoch 421/500\n",
            "5/5 - 0s - loss: 0.5041 - accuracy: 0.8331 - val_loss: 0.5967 - val_accuracy: 0.8239 - 57ms/epoch - 11ms/step\n",
            "Epoch 422/500\n",
            "5/5 - 0s - loss: 0.4977 - accuracy: 0.8300 - val_loss: 0.6047 - val_accuracy: 0.8169 - 72ms/epoch - 14ms/step\n",
            "Epoch 423/500\n",
            "5/5 - 0s - loss: 0.4776 - accuracy: 0.8339 - val_loss: 0.6198 - val_accuracy: 0.8169 - 74ms/epoch - 15ms/step\n",
            "Epoch 424/500\n",
            "5/5 - 0s - loss: 0.4805 - accuracy: 0.8307 - val_loss: 0.6184 - val_accuracy: 0.8169 - 73ms/epoch - 15ms/step\n",
            "Epoch 425/500\n",
            "5/5 - 0s - loss: 0.4851 - accuracy: 0.8300 - val_loss: 0.6023 - val_accuracy: 0.7887 - 74ms/epoch - 15ms/step\n",
            "Epoch 426/500\n",
            "5/5 - 0s - loss: 0.4958 - accuracy: 0.8245 - val_loss: 0.5868 - val_accuracy: 0.7887 - 72ms/epoch - 14ms/step\n",
            "Epoch 427/500\n",
            "5/5 - 0s - loss: 0.5021 - accuracy: 0.8214 - val_loss: 0.5817 - val_accuracy: 0.8028 - 78ms/epoch - 16ms/step\n",
            "Epoch 428/500\n",
            "5/5 - 0s - loss: 0.4917 - accuracy: 0.8346 - val_loss: 0.5887 - val_accuracy: 0.8169 - 60ms/epoch - 12ms/step\n",
            "Epoch 429/500\n",
            "5/5 - 0s - loss: 0.5050 - accuracy: 0.8276 - val_loss: 0.6073 - val_accuracy: 0.8099 - 56ms/epoch - 11ms/step\n",
            "Epoch 430/500\n",
            "5/5 - 0s - loss: 0.5154 - accuracy: 0.8237 - val_loss: 0.5934 - val_accuracy: 0.8099 - 68ms/epoch - 14ms/step\n",
            "Epoch 431/500\n",
            "5/5 - 0s - loss: 0.4981 - accuracy: 0.8276 - val_loss: 0.6120 - val_accuracy: 0.8169 - 74ms/epoch - 15ms/step\n",
            "Epoch 432/500\n",
            "5/5 - 0s - loss: 0.4910 - accuracy: 0.8292 - val_loss: 0.6431 - val_accuracy: 0.8169 - 58ms/epoch - 12ms/step\n",
            "Epoch 433/500\n",
            "5/5 - 0s - loss: 0.4827 - accuracy: 0.8307 - val_loss: 0.6600 - val_accuracy: 0.8099 - 60ms/epoch - 12ms/step\n",
            "Epoch 434/500\n",
            "5/5 - 0s - loss: 0.4913 - accuracy: 0.8292 - val_loss: 0.6551 - val_accuracy: 0.8028 - 60ms/epoch - 12ms/step\n",
            "Epoch 435/500\n",
            "5/5 - 0s - loss: 0.4856 - accuracy: 0.8284 - val_loss: 0.6204 - val_accuracy: 0.8169 - 57ms/epoch - 11ms/step\n",
            "Epoch 436/500\n",
            "5/5 - 0s - loss: 0.4829 - accuracy: 0.8268 - val_loss: 0.6245 - val_accuracy: 0.8099 - 75ms/epoch - 15ms/step\n",
            "Epoch 437/500\n",
            "5/5 - 0s - loss: 0.4834 - accuracy: 0.8292 - val_loss: 0.6428 - val_accuracy: 0.8099 - 58ms/epoch - 12ms/step\n",
            "Epoch 438/500\n",
            "5/5 - 0s - loss: 0.4928 - accuracy: 0.8292 - val_loss: 0.6473 - val_accuracy: 0.8099 - 63ms/epoch - 13ms/step\n",
            "Epoch 439/500\n",
            "5/5 - 0s - loss: 0.4842 - accuracy: 0.8315 - val_loss: 0.6319 - val_accuracy: 0.8169 - 59ms/epoch - 12ms/step\n",
            "Epoch 440/500\n",
            "5/5 - 0s - loss: 0.4768 - accuracy: 0.8292 - val_loss: 0.5959 - val_accuracy: 0.8169 - 73ms/epoch - 15ms/step\n",
            "Epoch 441/500\n",
            "5/5 - 0s - loss: 0.4722 - accuracy: 0.8315 - val_loss: 0.5996 - val_accuracy: 0.8169 - 57ms/epoch - 11ms/step\n",
            "Epoch 442/500\n",
            "5/5 - 0s - loss: 0.4757 - accuracy: 0.8268 - val_loss: 0.6095 - val_accuracy: 0.8169 - 63ms/epoch - 13ms/step\n",
            "Epoch 443/500\n",
            "5/5 - 0s - loss: 0.4736 - accuracy: 0.8339 - val_loss: 0.5776 - val_accuracy: 0.8169 - 104ms/epoch - 21ms/step\n",
            "Epoch 444/500\n",
            "5/5 - 0s - loss: 0.5255 - accuracy: 0.8292 - val_loss: 0.5785 - val_accuracy: 0.8099 - 83ms/epoch - 17ms/step\n",
            "Epoch 445/500\n",
            "5/5 - 0s - loss: 0.5308 - accuracy: 0.8284 - val_loss: 0.5967 - val_accuracy: 0.8169 - 61ms/epoch - 12ms/step\n",
            "Epoch 446/500\n",
            "5/5 - 0s - loss: 0.4676 - accuracy: 0.8354 - val_loss: 0.6647 - val_accuracy: 0.8099 - 76ms/epoch - 15ms/step\n",
            "Epoch 447/500\n",
            "5/5 - 0s - loss: 0.4862 - accuracy: 0.8276 - val_loss: 0.6888 - val_accuracy: 0.8099 - 57ms/epoch - 11ms/step\n",
            "Epoch 448/500\n",
            "5/5 - 0s - loss: 0.4979 - accuracy: 0.8346 - val_loss: 0.7106 - val_accuracy: 0.8169 - 62ms/epoch - 12ms/step\n",
            "Epoch 449/500\n",
            "5/5 - 0s - loss: 0.5198 - accuracy: 0.8284 - val_loss: 0.6583 - val_accuracy: 0.7746 - 60ms/epoch - 12ms/step\n",
            "Epoch 450/500\n",
            "5/5 - 0s - loss: 0.4877 - accuracy: 0.8331 - val_loss: 0.5891 - val_accuracy: 0.8099 - 74ms/epoch - 15ms/step\n",
            "Epoch 451/500\n",
            "5/5 - 0s - loss: 0.4911 - accuracy: 0.8315 - val_loss: 0.5638 - val_accuracy: 0.8099 - 99ms/epoch - 20ms/step\n",
            "Epoch 452/500\n",
            "5/5 - 0s - loss: 0.4865 - accuracy: 0.8300 - val_loss: 0.6128 - val_accuracy: 0.8169 - 59ms/epoch - 12ms/step\n",
            "Epoch 453/500\n",
            "5/5 - 0s - loss: 0.4825 - accuracy: 0.8315 - val_loss: 0.6350 - val_accuracy: 0.8099 - 82ms/epoch - 16ms/step\n",
            "Epoch 454/500\n",
            "5/5 - 0s - loss: 0.4750 - accuracy: 0.8331 - val_loss: 0.6010 - val_accuracy: 0.8099 - 61ms/epoch - 12ms/step\n",
            "Epoch 455/500\n",
            "5/5 - 0s - loss: 0.4805 - accuracy: 0.8307 - val_loss: 0.5654 - val_accuracy: 0.8099 - 58ms/epoch - 12ms/step\n",
            "Epoch 456/500\n",
            "5/5 - 0s - loss: 0.5291 - accuracy: 0.8307 - val_loss: 0.6478 - val_accuracy: 0.8028 - 73ms/epoch - 15ms/step\n",
            "Epoch 457/500\n",
            "5/5 - 0s - loss: 0.6308 - accuracy: 0.8323 - val_loss: 0.6263 - val_accuracy: 0.8028 - 81ms/epoch - 16ms/step\n",
            "Epoch 458/500\n",
            "5/5 - 0s - loss: 0.5616 - accuracy: 0.8315 - val_loss: 0.5743 - val_accuracy: 0.8099 - 76ms/epoch - 15ms/step\n",
            "Epoch 459/500\n",
            "5/5 - 0s - loss: 0.4888 - accuracy: 0.8362 - val_loss: 0.5904 - val_accuracy: 0.8099 - 90ms/epoch - 18ms/step\n",
            "Epoch 460/500\n",
            "5/5 - 0s - loss: 0.4794 - accuracy: 0.8323 - val_loss: 0.6112 - val_accuracy: 0.8169 - 58ms/epoch - 12ms/step\n",
            "Epoch 461/500\n",
            "5/5 - 0s - loss: 0.4867 - accuracy: 0.8370 - val_loss: 0.5841 - val_accuracy: 0.8099 - 69ms/epoch - 14ms/step\n",
            "Epoch 462/500\n",
            "5/5 - 0s - loss: 0.4910 - accuracy: 0.8339 - val_loss: 0.5772 - val_accuracy: 0.8169 - 56ms/epoch - 11ms/step\n",
            "Epoch 463/500\n",
            "5/5 - 0s - loss: 0.4828 - accuracy: 0.8378 - val_loss: 0.5760 - val_accuracy: 0.8169 - 75ms/epoch - 15ms/step\n",
            "Epoch 464/500\n",
            "5/5 - 0s - loss: 0.4866 - accuracy: 0.8354 - val_loss: 0.6093 - val_accuracy: 0.7676 - 72ms/epoch - 14ms/step\n",
            "Epoch 465/500\n",
            "5/5 - 0s - loss: 0.4922 - accuracy: 0.8214 - val_loss: 0.6055 - val_accuracy: 0.7676 - 103ms/epoch - 21ms/step\n",
            "Epoch 466/500\n",
            "5/5 - 0s - loss: 0.4814 - accuracy: 0.8284 - val_loss: 0.5746 - val_accuracy: 0.8099 - 103ms/epoch - 21ms/step\n",
            "Epoch 467/500\n",
            "5/5 - 0s - loss: 0.4714 - accuracy: 0.8339 - val_loss: 0.5641 - val_accuracy: 0.8239 - 102ms/epoch - 20ms/step\n",
            "Epoch 468/500\n",
            "5/5 - 0s - loss: 0.4695 - accuracy: 0.8354 - val_loss: 0.5554 - val_accuracy: 0.8099 - 112ms/epoch - 22ms/step\n",
            "Epoch 469/500\n",
            "5/5 - 0s - loss: 0.4814 - accuracy: 0.8378 - val_loss: 0.5531 - val_accuracy: 0.8099 - 107ms/epoch - 21ms/step\n",
            "Epoch 470/500\n",
            "5/5 - 0s - loss: 0.4771 - accuracy: 0.8339 - val_loss: 0.5703 - val_accuracy: 0.8169 - 84ms/epoch - 17ms/step\n",
            "Epoch 471/500\n",
            "5/5 - 0s - loss: 0.4690 - accuracy: 0.8292 - val_loss: 0.7558 - val_accuracy: 0.7042 - 92ms/epoch - 18ms/step\n",
            "Epoch 472/500\n",
            "5/5 - 0s - loss: 0.5013 - accuracy: 0.8144 - val_loss: 0.6647 - val_accuracy: 0.7394 - 98ms/epoch - 20ms/step\n",
            "Epoch 473/500\n",
            "5/5 - 0s - loss: 0.4738 - accuracy: 0.8378 - val_loss: 0.5691 - val_accuracy: 0.8169 - 99ms/epoch - 20ms/step\n",
            "Epoch 474/500\n",
            "5/5 - 0s - loss: 0.4808 - accuracy: 0.8339 - val_loss: 0.5619 - val_accuracy: 0.8169 - 105ms/epoch - 21ms/step\n",
            "Epoch 475/500\n",
            "5/5 - 0s - loss: 0.4830 - accuracy: 0.8331 - val_loss: 0.5611 - val_accuracy: 0.8099 - 122ms/epoch - 24ms/step\n",
            "Epoch 476/500\n",
            "5/5 - 0s - loss: 0.5147 - accuracy: 0.8268 - val_loss: 0.5882 - val_accuracy: 0.8099 - 102ms/epoch - 20ms/step\n",
            "Epoch 477/500\n",
            "5/5 - 0s - loss: 0.4998 - accuracy: 0.8222 - val_loss: 0.6171 - val_accuracy: 0.8028 - 111ms/epoch - 22ms/step\n",
            "Epoch 478/500\n",
            "5/5 - 0s - loss: 0.4826 - accuracy: 0.8284 - val_loss: 0.6246 - val_accuracy: 0.8028 - 101ms/epoch - 20ms/step\n",
            "Epoch 479/500\n",
            "5/5 - 0s - loss: 0.4755 - accuracy: 0.8307 - val_loss: 0.6059 - val_accuracy: 0.8028 - 84ms/epoch - 17ms/step\n",
            "Epoch 480/500\n",
            "5/5 - 0s - loss: 0.4690 - accuracy: 0.8315 - val_loss: 0.5826 - val_accuracy: 0.8028 - 96ms/epoch - 19ms/step\n",
            "Epoch 481/500\n",
            "5/5 - 0s - loss: 0.4677 - accuracy: 0.8331 - val_loss: 0.5753 - val_accuracy: 0.8028 - 87ms/epoch - 17ms/step\n",
            "Epoch 482/500\n",
            "5/5 - 0s - loss: 0.4638 - accuracy: 0.8307 - val_loss: 0.5792 - val_accuracy: 0.8028 - 88ms/epoch - 18ms/step\n",
            "Epoch 483/500\n",
            "5/5 - 0s - loss: 0.4742 - accuracy: 0.8292 - val_loss: 0.5844 - val_accuracy: 0.8028 - 95ms/epoch - 19ms/step\n",
            "Epoch 484/500\n",
            "5/5 - 0s - loss: 0.4693 - accuracy: 0.8292 - val_loss: 0.5942 - val_accuracy: 0.8028 - 99ms/epoch - 20ms/step\n",
            "Epoch 485/500\n",
            "5/5 - 0s - loss: 0.4730 - accuracy: 0.8284 - val_loss: 0.5997 - val_accuracy: 0.8028 - 83ms/epoch - 17ms/step\n",
            "Epoch 486/500\n",
            "5/5 - 0s - loss: 0.4669 - accuracy: 0.8362 - val_loss: 0.6027 - val_accuracy: 0.8028 - 106ms/epoch - 21ms/step\n",
            "Epoch 487/500\n",
            "5/5 - 0s - loss: 0.4555 - accuracy: 0.8323 - val_loss: 0.5977 - val_accuracy: 0.8028 - 106ms/epoch - 21ms/step\n",
            "Epoch 488/500\n",
            "5/5 - 0s - loss: 0.4584 - accuracy: 0.8331 - val_loss: 0.5895 - val_accuracy: 0.8028 - 87ms/epoch - 17ms/step\n",
            "Epoch 489/500\n",
            "5/5 - 0s - loss: 0.4522 - accuracy: 0.8354 - val_loss: 0.5852 - val_accuracy: 0.8028 - 107ms/epoch - 21ms/step\n",
            "Epoch 490/500\n",
            "5/5 - 0s - loss: 0.4697 - accuracy: 0.8276 - val_loss: 0.5880 - val_accuracy: 0.8028 - 98ms/epoch - 20ms/step\n",
            "Epoch 491/500\n",
            "5/5 - 0s - loss: 0.4491 - accuracy: 0.8362 - val_loss: 0.5985 - val_accuracy: 0.8169 - 98ms/epoch - 20ms/step\n",
            "Epoch 492/500\n",
            "5/5 - 0s - loss: 0.4473 - accuracy: 0.8401 - val_loss: 0.5914 - val_accuracy: 0.8099 - 108ms/epoch - 22ms/step\n",
            "Epoch 493/500\n",
            "5/5 - 0s - loss: 0.4419 - accuracy: 0.8409 - val_loss: 0.5733 - val_accuracy: 0.8169 - 109ms/epoch - 22ms/step\n",
            "Epoch 494/500\n",
            "5/5 - 0s - loss: 0.4653 - accuracy: 0.8409 - val_loss: 0.5778 - val_accuracy: 0.8169 - 103ms/epoch - 21ms/step\n",
            "Epoch 495/500\n",
            "5/5 - 0s - loss: 0.4657 - accuracy: 0.8354 - val_loss: 0.5873 - val_accuracy: 0.8239 - 93ms/epoch - 19ms/step\n",
            "Epoch 496/500\n",
            "5/5 - 0s - loss: 0.4461 - accuracy: 0.8401 - val_loss: 0.6456 - val_accuracy: 0.7746 - 97ms/epoch - 19ms/step\n",
            "Epoch 497/500\n",
            "5/5 - 0s - loss: 0.4794 - accuracy: 0.8222 - val_loss: 0.6417 - val_accuracy: 0.7746 - 78ms/epoch - 16ms/step\n",
            "Epoch 498/500\n",
            "5/5 - 0s - loss: 0.4559 - accuracy: 0.8385 - val_loss: 0.5791 - val_accuracy: 0.8099 - 104ms/epoch - 21ms/step\n",
            "Epoch 499/500\n",
            "5/5 - 0s - loss: 0.4708 - accuracy: 0.8424 - val_loss: 0.5623 - val_accuracy: 0.8239 - 96ms/epoch - 19ms/step\n",
            "Epoch 500/500\n",
            "5/5 - 0s - loss: 0.4672 - accuracy: 0.8432 - val_loss: 0.5758 - val_accuracy: 0.8169 - 105ms/epoch - 21ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shw-qDWTgAju",
        "outputId": "4cb08e9b-0511-4aac-fcc2-be82e6707b73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 0s 11ms/step - loss: 0.5758 - accuracy: 0.8169\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.575832188129425, 0.8169013857841492]"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HKR7b_rf8Yk",
        "outputId": "54cf94f3-403d-4206-c722-de3e47a9eed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMDV06Z_V3CB",
        "outputId": "a37a6a3e-03b7-4716-d82d-424399c8264f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 2., 2., 1., 1., 0., 1., 1.,\n",
              "       2., 1., 1., 1., 2., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
              "       0., 1., 1., 1., 1., 0., 0., 0., 1., 2., 2., 2., 2., 1., 1., 1., 1.,\n",
              "       2., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 2., 2., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 0., 0., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       2., 2., 1., 1., 1., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "accuracy_score(y_test, y_pred), f1_score(y_test, y_pred, average='weighted')"
      ],
      "metadata": {
        "id": "2vQBFFufX7zn",
        "outputId": "d7cf95a5-9739-455d-93fe-461b98ef9ccf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8169014084507042, 0.7578710155670868)"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_test, y_pred)"
      ],
      "metadata": {
        "id": "NXwBj4qbX8Tj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9dbb8c1-f602-4dfb-a9aa-cdf620b6d340"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  4,   9,   0],\n",
              "       [  2, 112,   0],\n",
              "       [  0,  15,   0]])"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conf = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "buy_acc = conf[0][0]/sum(conf[0])\n",
        "sell_acc = conf[2][2]/sum(conf[2])\n",
        "\n",
        "print('매수(buy) 정확도: ', buy_acc)\n",
        "print('매도(sell) 정확도: ', sell_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ib6fgF2WACG",
        "outputId": "82523ca1-9edc-4bd9-9ccb-50f56ec9e389"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "매수(buy) 정확도:  0.3076923076923077\n",
            "매도(sell) 정확도:  0.0\n"
          ]
        }
      ]
    }
  ]
}